{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80274e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2e4a5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_directory=os.path.dirname(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4716c168",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\PythonCode\\\\document_portal\\\\notebook\\\\data'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path=os.path.join(current_directory, \"notebook\", \"data\")\n",
    "file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8f1cf5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['d:\\\\PythonCode\\\\document_portal\\\\notebook\\\\data\\\\Article.pdf',\n",
       " 'd:\\\\PythonCode\\\\document_portal\\\\notebook\\\\data\\\\CancerTumorDetectionusingGeneticMutatedDataandMachineLearningModels.pdf',\n",
       " 'd:\\\\PythonCode\\\\document_portal\\\\notebook\\\\data\\\\ijcsit2012030339.pdf',\n",
       " 'd:\\\\PythonCode\\\\document_portal\\\\notebook\\\\data\\\\Improvement_of_Speech_Emotion_Recognition_by_Deep_Convolutional_Neural_Network.pdf',\n",
       " 'd:\\\\PythonCode\\\\document_portal\\\\notebook\\\\data\\\\Improving_Speaker_Gender_Detection_by_Combining_Pitch_and_SDC__1_.pdf',\n",
       " 'd:\\\\PythonCode\\\\document_portal\\\\notebook\\\\data\\\\mohanty-2020-ijca-920840.pdf',\n",
       " 'd:\\\\PythonCode\\\\document_portal\\\\notebook\\\\data\\\\Paper_94-Sentiment_Analysis_on_Banking_Feedback_and_News_Data.pdf',\n",
       " 'd:\\\\PythonCode\\\\document_portal\\\\notebook\\\\data\\\\Prospect-of-Low-Power-Sensor-Network-Technology-in-Disaster-Management-for-Sustainable-Future.pdf',\n",
       " 'd:\\\\PythonCode\\\\document_portal\\\\notebook\\\\data\\\\sample.pdf',\n",
       " 'd:\\\\PythonCode\\\\document_portal\\\\notebook\\\\data\\\\W_An_Outlier_Detection_and_Rectification.pdf']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_files = [f for f in os.listdir(file_path) if f.endswith('.pdf')]\n",
    "pdf_path = [os.path.join(file_path, files) for files in pdf_files]\n",
    "pdf_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da8b0cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reading: d:\\PythonCode\\document_portal\\notebook\\data\\Article.pdf\n",
      " \n",
      " \n",
      " \n",
      "Journal of Information Systems and Telecommunication  \n",
      "Vol.12, No.2,  April -June 2024, 152-161 \n",
      " \n",
      "http://jist.acecr.org  \n",
      "ISSN 2322 -1437  / EISSN:2345 -2773  \n",
      " \n",
      "Whispered Speech Emotion Recognition with Gender Detection using \n",
      "BiLSTM and DCNN  \n",
      "Aniruddha Mohanty1*, Ravindranath C Cherukuri1 \n",
      " \n",
      "1.Department of Computer  science and Engineering, Christ (Deemed to be University), Bangalore, India  \n",
      " \n",
      " \n",
      "Received: 22 Aug 2023 / Revised: 04 Apr 2024 / Accepted: 21 May 2024  \n",
      " \n",
      " \n",
      "Abstract   \n",
      "Emotions are human mental states at a particular instance in time concerning one’s circumstances, mood, and relationships \n",
      "with others . Identifying emotions from the whispered speech is complicated as the conversation might be confidential. The \n",
      "representation of the speech relies on the magnitude of its information. Whispered speech is intelligible, a low -intensi ty \n",
      "signal, and varies from normal speech. Emotion identification is quite tricky from whispered speech. Both prosodic and \n",
      "spectral speech features help to identify emotions. The emotion identification in a whispered speech happens using prosodic \n",
      "speech fea tures such as zero -crossing rate  (ZCR) , pitch, and spectral features that include spectral centroid, chroma STFT, \n",
      "Mel scale spectrogram, Mel -frequency cepstral coefficient (MFCC), Shifted Delta Cepstrum (SDC), and Spectral Flux. \n",
      "There are two parts to the proposed implementation. Bidirectional Long  Short -Term Memory (BiLSTM) helps to identify \n",
      "the gender from the speech sample in the first step with SDC and pitch. The Deep Convolutional Neural Network (DCNN) \n",
      "model helps to identify the emotions in the second  step. This implementation is evaluated using the  wTIMIT data corpus \n",
      "and gives 98.54% accuracy. Emotions have a dynamic effect on genders, so this implementation performs better than \n",
      "traditional approaches. This approach helps to design online learning management systems, different applications for \n",
      "mobile devices, checking cyber -criminal activities, emotion detection for older people, automatic speaker identification and \n",
      "authentication, forensics, and surveillance . \n",
      " \n",
      "Keywords:  Whispered Speech; Emotion Re cognition; Speech Features; Data Corpus; BiLSTM;  DCNN . \n",
      " \n",
      "1- Introduction  \n",
      "Emotions are the humans’ short -lived feeling s that affects \n",
      "thinking, actions, relationships, and social interactions. \n",
      "Emotions express humans’ physiological and emotional \n",
      "states with facial expressions and body language. \n",
      "Whispered speech is a form of speec h that expresses \n",
      "emotions. Whispered speech is a type of communication \n",
      "produced with breath without any noise and excitation of \n",
      "voice. The whispered speech structure changes \n",
      "significantly because of the lack of periodic excitation in \n",
      "the voice. This result s in missing speech and reduced \n",
      "transparency in communication.  \n",
      "The difference between normal and whispered speech is \n",
      "the absence of vocal tract vibrations due to the vocal \n",
      "tract’s physiological blocking. The strength of whispered \n",
      "speech is minute and with out voice compared to phoned \n",
      "speech. The spectral and prosodic features help to detect \n",
      "the whispered speech. Prosodic features of speech vary \n",
      "over time. Spectral features of whispered speech are highly accurate over unvoiced consonants, voiced consonants, \n",
      "and formants in vowels [1]. \n",
      "The impacts on Speech Emotion Recognition ( SER) are \n",
      "due to various acoustic conditions such as compressed, \n",
      "noisy, telephonic conversation s and imitator speeches. \n",
      "Other environmental conditions,  such as stress, rhythm, \n",
      "and intonation, can affect  SER.  Whispered speech is also \n",
      "affected by similar acoustic and environmental conditions. \n",
      "SER depends on acoustic characteristics and gender in \n",
      "some scenarios. Hence gender plays a vital role in SER.  \n",
      "Nowadays, several whispered speech  samples of male and \n",
      "female voices are available. The effectiveness of the SER \n",
      "is more when the implementation is in two parts. The \n",
      "identification of the gender [2] happens initially using \n",
      "Bidirectional Long Short -term Memory (BiLSTM)  with \n",
      "speech features . The next step is detecting emotions using \n",
      "various speech features [3] and Deep Convolutional \n",
      "Neural Network (DCNN).   \n",
      "The structure of the paper includes various sections. \n",
      "Section 2 describes Human Emotions and their \n",
      "Applications. Details of the Related Work are in Section 3. \n",
      "Section 4 describes the System Model, which is the black \n",
      "box view of the implementation. Section 5 is the Model \n",
      " \n",
      "Journal of Information Systems and Telecommunication, Vol.12, No.2,  April -June 2024  \n",
      " \n",
      " \n",
      "153 \n",
      "Design that describes the details of the speech features and \n",
      "the deep learning models used in this implementation. \n",
      "Section 6 gives the Experiment and Result Analysis. \n",
      "Section 7 brief ly discuss  the Conclusion and Future Work.   \n",
      "2- Human  Emotions  and their  Applications  \n",
      "Human emotions are the mental state caused by countless \n",
      "associated views , feeling s, behavioral replies, and the \n",
      "degree of pressure and annoyance. It is often associated \n",
      "with attitude , temperament, behavior , disposition, and \n",
      "creativi ty [4]. Emotion recognition helps to detect a \n",
      "humans ’ emotional mood , which lasts hours and days. \n",
      "Speech conveys emotions such as anger, disgust, fear, \n",
      "happiness, neutrality, sadness, and surprise. The machine \n",
      "automatically detects various emotions from speech with \n",
      "the help of different algorithms. Emotion Recognition \n",
      "helps to : \n",
      " \n",
      "•  Detect customers ’ intentions based on the teleconference. \n",
      "•  Detect cybercrime.  \n",
      "•  Students ’ attention and teachers ’ content adjustment.  \n",
      "•  Disability assistance  \n",
      "•  Customer satisfaction  \n",
      "•  Stress monitoring  \n",
      "•  Social media analysis  \n",
      "•  Suspicious activity  \n",
      "•  Human -machine interaction and so on.   \n",
      "3- Related Work  \n",
      "Over time, numerous studies have detected emotions from \n",
      "whispered speech . The vario us deep learning models \n",
      "detect emotions based on the speech features extracted \n",
      "from the collected whispered speech data corpus . The SER \n",
      "for normal and whispered speech is diverse because of \n",
      "vocal excitation. Emotions vary with many factors; gender \n",
      "is among  the most influential factors  [5]. Identifying \n",
      "gender in the first step improves emotion detection from \n",
      "whispered speech. So, the related work explored is on \n",
      "gender detection and emotion recognition from whispered \n",
      "speech.  \n",
      "MFCC obtained by the Hilbert enve lope approach and \n",
      "weighted instantaneous frequencies (WIFs) obtained by \n",
      "the coherent demodulation help to detect gender in \n",
      "whispered speech  samples [6]. There is an opportunity to \n",
      "explore gender detection in noisy speech conditions using \n",
      "these approaches.  \n",
      "Autoencoder -enabled features  in the transfer learning \n",
      "framework propose to practice  phonated data to identify \n",
      "emotions from Whispered speech [7]. The feature \n",
      "extraction is from the Geneva Whispered Emotion Corpus \n",
      "(GeWEC) and Berlin Emotional Speech Databa se (EMO -\n",
      "DB) data corpus. The acoustic features such as Mel-frequency cepstral coefficient (MFCC), root mean square \n",
      "(RMS), frame energy, zero -crossing -rate (ZCR), pitch \n",
      "frequency (F0), probability of voicing autocorrelation \n",
      "function are the inputs to evaluate the framework. \n",
      "Implementing deep learning concepts on spontaneous data \n",
      "gives more accuracy than the current framework .  \n",
      "Gender is detected to target an anonymous speaker. The \n",
      "Deep Neural Network (DNN) model is used to generalize \n",
      "gender by using the MFCC speech feature. This \n",
      "implementation is applied to the wTIMIT dataset to verify \n",
      "the gender and recognize the speaker  [8]. The DNN model \n",
      "cannot generalize gender; evaluation can happen with \n",
      "other datasets.   \n",
      "MFCC and CNN, with the fully  connected network, detect \n",
      "gender and emotions like anger, disgust, fear, happiness, \n",
      "sadness, surprise, and a neutral state  [9]. The concept \n",
      "verification happens on RAVDESS, CREMA -D, SAVEE, \n",
      "and TESS datasets, having an accuracy of 92.283%, which \n",
      "is better than the tr aditional model.  \n",
      "The final prediction of the implemented model is to learn \n",
      "the mutually spatial -spectral features happens by a two -\n",
      "stream deep convolutional neural network with an iterative \n",
      "neighborhood  component analysis (INCA) and the most \n",
      "discriminatory optimal features  [10]. The concept \n",
      "verification happens on EMO -DB, SAVEE, and \n",
      "RAVDESS emotional speech corpora which perform with \n",
      "95%, 82%, and 85% accuracy rates. Real -time applications \n",
      "with natural and huge data corpora can help to extend this \n",
      "concept to identify emotions.  \n",
      "Mel Frequency Magnitude Coefficient (MFMC) and three \n",
      "spectral features, namely MFCC, log frequency power \n",
      "coefficient, and linear prediction cepstral coefficient, are \n",
      "used with the help of Support Vector Machine (SVM) \n",
      "modeling  [11]. The performance evaluation uses this \n",
      "concept  on Berlin, RAVDESS, SAVEE, EMOVO, and \n",
      "eNTERFACE data corpus . Feature selection, feature \n",
      "fusion, and multiple classification schemes improve the \n",
      "performance of MFMC.  \n",
      "The proposed MFF -SAug research in  which noise removal \n",
      "improves emotion. White Noise Injection, Pitch Tuning \n",
      "techniques, MFCC, ZCR, RMS speech features, and \n",
      "Convolutional Neural Network (CNN) modeling [12] \n",
      "detect emotions. Emotion detection during the interaction \n",
      "between people can extend the MFF -SAug approach .  \n",
      "The proposed Neural Network -based Blended Ensemble \n",
      "Learning (NNBEL) model is composed of a 1 -dimensional \n",
      "Convolution Neural Network (1D -CNN), Long Short -\n",
      "Term Memory (LSTM), and CapsuleNets. LSTM receives \n",
      "input from the Log Mel -spect rogram speech features, \n",
      "while 1D -CNN and CapsuleNets receive input from the \n",
      "MFCC.  Each model’s output is fed to Multi -Layer \n",
      "Perceptron (MLP) and predicts the final emotion s [13]. \n",
      "This model shows 95.3% and 94% accuracy on \n",
      "RAVDESS and IEMOCAP datasets, respectively.  \n",
      " \n",
      "Mohanty, & Cherukuri, Whispered Speech Emotion  Recognition with Gender Detection using BiLSTM and DCNN  \n",
      " \n",
      " \n",
      " \n",
      "154 \n",
      "TrustSER [14] implemented a general framework to \n",
      "determine the SER system's trustworthiness using deep \n",
      "learning techniques. Trustworthiness evaluates privacy \n",
      "(gender in formation, speaker demographics), safety, \n",
      "fairness, sustainability, and emotions (sad, angry, happy, \n",
      "neutral). The architecture of the TrustSET framework uses \n",
      "CNN encoder and Transformer encoder models. \n",
      "Trustworthy profiles under the Federated Learning \n",
      "scenario might improve privacy, fairness, and safety.  \n",
      "A hybrid meta -heuristic ensemble -based classification [15] \n",
      "helps to detect speech emotions. Raw speech samples are \n",
      "filtered using the Butterworth filter; then, spectrogram \n",
      "speech features are extracted from each frame to create a \n",
      "hybrid feature vector. The ensemble -based classification is \n",
      "applied to hybrid feature vectors to classify em otions. The \n",
      "ensemble -based classification contains a Recurrent Neural \n",
      "Network (RNN), a Deep Belief Network (DBN), and an \n",
      "Artificial Neural Network (ANN).  \n",
      "The above -related work shows that it is crucial to identify \n",
      "gender to segregate emotions in a speech. Male and female \n",
      "emotions are different based on situations. Whispered \n",
      "speech is an essential concept of emotion recognition. \n",
      "There is an opportunity to experiment further based on \n",
      "other datasets of whispered speech. So, this is a motivation \n",
      "to work on emot ion recognition in whispered speech, as \n",
      "this is a less explored area of research .  \n",
      "4- System Model  \n",
      "The analysis of e motion recognition from the whispered \n",
      "speech segregates into two parts. One is gender detection, \n",
      "and the second is emotion detection as shown in Fig 1.  \n",
      " \n",
      " \n",
      "Fig. 1 Emotion recognition from whispered speech  \n",
      " \n",
      "As part of gender detection, pre -processing of the speech \n",
      "samples is performed, and then pitch and SDC are \n",
      "extracted as part of prosodic and spectral features, \n",
      "respectively. Both the features are fused to get a single \n",
      "feature set with the help of multifeature fusion. PCA helps \n",
      "to reduce the dimensions of extracted features and is used \n",
      "as input to Bidirectional Long short -term memory \n",
      "(BiLSTM) to classify genders , as shown in Fig 2.  \n",
      " \n",
      " \n",
      "Fig .2 Gender detection from whispered speech  \n",
      " \n",
      "Following  Gender detection, the augmentation of speech \n",
      "data helps to get more realistic data. Then the speech features such as Chromatogram, Zero -Crossing Rate, \n",
      "Spectral Central, MFCC, Spectral Flux , and Mel \n",
      "Spectrog rams are extracted. All the extracted features \n",
      "combine to get a single feature using multifeature fusion. \n",
      "Then the dimensions are reduced to get the optimal data \n",
      "points. The data points are inserted into the Deep \n",
      "Convolutional Neural Network (DCNN) to identify the \n",
      "emotions , as shown in Fig 3.  \n",
      "5- System  Design  \n",
      "The implementation of system design happens in two parts. \n",
      "The first part identifies the male and female gender from \n",
      "the whispered speech samples. Then the determination of \n",
      "different emotions like sadn ess, happiness, fear, anger, \n",
      "surprise, disgust, and neutral are detected in gender -\n",
      "segregated speech samples.  \n",
      " \n",
      "Fig. 3 Emotion Recognition from Male and Female whispered speech.  \n",
      "5-1- Whispered Speech Data Corpus  \n",
      "wTIMIT [16] is a whispered voice data corpus ha ving 450 \n",
      "phonetically balanced sentences with 29 speakers. There \n",
      "are 11,324 utterances with a normal voice , which  can be \n",
      "used for normal and whispered training. This data is \n",
      "available in two parts - train and test divisions. The \n",
      "samples contain an 8 kHz sampling rate with a high -\n",
      "quality  and low-pass filter.  \n",
      "5-2-    Data Augmentation  \n",
      "Data augmentation [17 ] is the method of getting more \n",
      "realistic data from the existing data, which helps to add \n",
      "more training data to the model, reduce overfitting, and \n",
      "increase the model’s generalization ability. Below are the \n",
      "steps to generate synthetic data.  \n",
      " Shifting Time : Shifting time is a simple concept \n",
      "where the audio shifts to the left or right with a \n",
      "haphazard second. If the audio shifts to the left \n",
      "\n",
      " \n",
      "Journal of Information Systems and Telecommunication, Vol.12, No.2,  April -June 2024  \n",
      " \n",
      " \n",
      "155 \n",
      "with z seconds, then the first z seconds are \n",
      "marked as silence. Similarly, if the audio shifts to \n",
      "the right with z second s, the last z seconds are \n",
      "marked as silence.  \n",
      " Changing Pitch: Pitch change is adjusting the \n",
      "pitch randomly without upsetting the speediness \n",
      "of the audio file.  \n",
      " Changing Speed: Speed audio changes by \n",
      "stretching the time series data by a fixed rate.  \n",
      "5-3- Pre-processing  \n",
      "After the data collection and data augmentation, pre -\n",
      "processing [3] is the initial  phase  in training SER. \n",
      "Framing, Windowing, Voice activity detection, \n",
      "Normalization, and Noise reduction are pre -\n",
      "processing steps.   \n",
      "Framing:  Speech signals are quasip eriodic and vary \n",
      "over time. Hence, information and related emotions \n",
      "also fluctuate over time. The segregation of the \n",
      "signals happens in a shorter period to make the \n",
      "speech signal invariant. Twenty milliseconds to thirty \n",
      "milliseconds helps to make the speec h signal \n",
      "invariant, and five milliseconds of overlap of the \n",
      "frames avoid data leakage between the frames .  \n",
      "Windowing:  Windowing on each frame helps to diminish \n",
      "data leakage during Fast Fourier Transformation (FFT) \n",
      "after framing. The Hamming window allows  this step \n",
      "where the window size is N for the frames     , used in \n",
      "equation (1).  \n",
      " \n",
      "                  (   \n",
      "   )       (1) \n",
      "            \n",
      " \n",
      "Voice activity detection (VAD):  An utterance has three \n",
      "portions of speech activities : voiced, unvoiced, and silent. \n",
      "Zero Crossing Rate (ZCR) speech feature helps to detect \n",
      "VAD. ZCR represents the frequency of signal transitions \n",
      "between positive and negative values within a specific \n",
      "frame . Due to high energy, the ZCR value is low for \n",
      "voiced speech and high for unvoiced s peech because of \n",
      "low energy.  \n",
      "Normalization:  Normalization helps to reduce  speaker \n",
      "and recording inconsistency without affecting the features’ \n",
      "strength and enhancing the features’ generalization \n",
      "capability. The Z-normalization method is used more and \n",
      "represented as  \n",
      "       \n",
      "      (2) \n",
      "Where   is the speech signal,   and   are the mean and \n",
      "standard deviation of the data, respectively.  \n",
      "Noise Reduction:  The environmental noise captured while \n",
      "recording a speech signal  affects the recognition rate . \n",
      "Minimum mean square error (MMSE) and log -spectral \n",
      "amplitude MMSE (LogMMSE) reduce the noise from the speech signals. Noise reduction helps to get more accuracy \n",
      "in SER evaluation.  \n",
      "5-4- Feature Extraction  \n",
      "Specific emotions are present in prosodic and spectral \n",
      "features of speech. The extraction of prosodic and spectral \n",
      "features is a vital characteristic of emotion recognition \n",
      "after pre -processing the speech signals.  \n",
      "5-4-1-Prosodic Features  \n",
      "Prosodic features deal with the audio qualities of a speech \n",
      "when connected speeches use sounds as input. The \n",
      "production of the speech deals with the amount of energy, \n",
      "frequency, period, loudness, pitch, and duration. Speech \n",
      "signal com munication depends on intonation, stress, and \n",
      "rhythm, which prosodic features can detect. The prosodic \n",
      "features used in the implementation are : \n",
      "Zero -Crossing Rate (ZCR):  ZCR [17] measures the \n",
      "frequency of signal transitions between positive and \n",
      "negative values in every audio frame  and defined as  \n",
      "    \n",
      "    ∑       [ ]         [      \n",
      "    (3) \n",
      "Where        is the sign function . \n",
      "    [     ]  {        \n",
      "       \n",
      "Fundamental Frequency (Pitch):  Fundamental \n",
      "Frequency (F0) [18] is the minimum freque ncy of the \n",
      "periodic waveform. F0 is the significant parameter to \n",
      "differentiate male speech from female speech. Pitch also \n",
      "determines the voiced and unvoiced portion of the speech \n",
      "signal. This analysis uses pitch parameters like pitch mean \n",
      "value and pitch r ange.  \n",
      "The pitch range determines the number of octaves a speech  \n",
      "sample  can cover, from the lowest to the highest. F0 value \n",
      "varies from 85Hz to 180 HZ for the voiced speech of adult \n",
      "males.  \n",
      " \n",
      " \n",
      "Fig. 4 Representation of Pitch data  \n",
      " \n",
      "Similarly, the F0 value for adult females goes from 165Hz \n",
      "to 255Hz, as shown in Fig 4.  \n",
      "\n",
      " \n",
      "Mohanty, & Cherukuri, Whispered Speech Emotion  Recognition with Gender Detection using BiLSTM and DCNN  \n",
      " \n",
      " \n",
      " \n",
      "156 \n",
      "5-4-2-Spectral features  \n",
      "The representation of spectral features happens by \n",
      "converting the time -domain speech signal into the \n",
      "frequency domain with the help of the Fast Fourier \n",
      "Transform (FFT) which  benefits  to represent the \n",
      "characteristics of the Human Vocal Tract. The  spectral \n",
      "features are  \n",
      "Spectral Centroid (SC):  The Spectral Centroid [19] is the \n",
      "most used speech feature where the positioning of the \n",
      "―center of mass‖ of the audio signal spectr um defines the \n",
      "weight of the spectrum. The center of mass measures the \n",
      "weighted average of the frequency component located in \n",
      "the audio signal, defined as  \n",
      " \n",
      "      ∑            \n",
      "   \n",
      "∑       \n",
      "       (4) \n",
      "Where      represents the magnitude of bin numb er   and \n",
      "      represents the central frequency of the bin.  \n",
      "Chroma STFT:  Chroma STFT [20] is obtained using FFT \n",
      "on speech samples and the resulting spectrums a re a \n",
      "chromatogram in a vertical axis. This feature captures the \n",
      "harmonic feature of the speech s ignals.  \n",
      "Mel-scale Spectrogram:  Mel-scale Spectrogram [ 21] is a \n",
      "spectrogram in which frequencies convert to the Mel scale, \n",
      "which helps to differentiate the range of frequencies. Mel -\n",
      "spectrogram helps to understand emotions in a better way \n",
      "as humans can perceive sound on a logarithmic scale.  \n",
      "Mel Frequency Cepstral Coefficient (MFCC ): MFCC \n",
      "[17] i s SER’s  standard feature extraction technique. The \n",
      "vocal cord s, tongue, and teeth filter the sound and make it \n",
      "unique for each speaker in the Human Speech production \n",
      "system. Mel scale represents MFCC, where the frequency \n",
      "bands are equally spaced and close to the Human Auditory \n",
      "System’s response, as shown in Fig 5.  \n",
      " \n",
      "Fig 5 MFCC speech feature extraction  \n",
      " \n",
      "Shifted Delta Cepstrum (SDC):  Time derivatives apply \n",
      "to the cepstral coefficients obtained from the MFCC and \n",
      "combined with the delta coefficient to get SDC, as shown \n",
      "in Fig 6.       and   are the four parameters used in \n",
      "SDC [ 22].  M denotes the cepstral coefficient for each \n",
      "frame.   indicates added frames.   are the frames that \n",
      "append delta features from the new feature vector .   \n",
      "represents the delta values difference. So, the coefficient \n",
      "vectors represent as   \n",
      "     [                  ]   (5) \n",
      "   are MFCC coefficients and   is the coefficient index. \n",
      "For a given time,   an intermediate calculation is done to \n",
      "obtain the   coefficients.  \n",
      "                                 (6) \n",
      "Finally, the SDC coefficients vectors of   dimensions are \n",
      "obtained as  \n",
      "       [          (                )] (7) \n",
      " \n",
      " \n",
      "Fig. 6 Extraction of SDC speech feature  \n",
      " \n",
      "This speech feature helps to identify genders for a long -\n",
      "range dynamic appearance in speech signals.  \n",
      "Spectral Flux:  Spectral flux [ 20] measures the change in \n",
      "speed of the signal’s power spectrum compared to the \n",
      "previous frame. This feature estimates the speech signal’s \n",
      "power spectrum about  the power spectrum of one frame \n",
      "with others.  \n",
      "5-5- Feature Selection and Dimension Reduction  \n",
      "Feature s election [23] is selecting features from a large set \n",
      "of extracted features to eliminate redundant and unused \n",
      "information and decrease processing time. For this work, \n",
      "the extracted pitch contains lots of information. The global \n",
      "features, like the range of p itch values and mean pitch \n",
      "values, are selected. Removing redundant and unused \n",
      "information, such as additional zeroes, duplicate values, or \n",
      "frames, is part of MFCC feature extraction. The exact \n",
      "process removes the silent speech frames.  \n",
      "Fundamental frequen cy and SDC are two speech features \n",
      "to identify gender. Similarly, Spectral centroid, Chroma \n",
      "STFT, MFCC, and Spectral flux features are used for \n",
      "emotion identification to create a single set of data points \n",
      "from multiple speech features using multifeature fu sion \n",
      "technique [24]. \n",
      "Multifeature Fusion:  The extracted speech features hav e \n",
      "different dimensions, so feature proximity usages  average \n",
      "dimensional spacing between the vectors to denote the \n",
      "proximity between the diverse features  [24]. The average \n",
      "dimensional spacing between the vectors is computed as  \n",
      "          \n",
      "    √∑∑              (8) \n",
      "   =  \n",
      "     √∑∑              (9) \n",
      "\n",
      " \n",
      "Journal of Information Systems and Telecommunication, Vol.12, No.2,  April -June 2024  \n",
      " \n",
      " \n",
      "157 \n",
      "        and     are the mean and variance interval of \n",
      "average dimensions of the feature vector s.    and    are \n",
      "the mean value of   and   type of sound features.     and    \n",
      "are the mean value of   and   type of sound features . Then \n",
      "the subsequent dimensionality reduction [25] method \n",
      "follows once the feature selection process is complete. \n",
      "High data variance is present in the extracted features \n",
      "containing more information. Dimension reduction \n",
      "techniques reduce the dimensions from extracted feature \n",
      "vectors.  \n",
      " \n",
      "5-5-1- Principal Component Analysis  (PCA)  \n",
      "PCA [25] is an approach for redu cing the dimensionality \n",
      "of extensive datasets, transforming them into more \n",
      "compact representations while retaining crucial \n",
      "information . These reductions in the number of variables \n",
      "help to get more accurate results. A reduced dataset makes \n",
      "it easier and fas ter to visualize and analyze the data. \n",
      "Following steps followed to explore PCA .  \n",
      " Standardization  creates a normalized dataset \n",
      "when there is a significant difference in the range \n",
      "of initial variables or a larger range of datasets \n",
      "dominates the smaller range. It can be done by  \n",
      "             \n",
      "                     (10) \n",
      " Covariance matrix computation  helps  to know  \n",
      "how the input  dataset  varies  from  the mean value \n",
      "to each data point.  \n",
      " Eigenvectors and eigenvalues  of the covariance \n",
      "matrix help to identify the principal component.  \n",
      "5-6- Modelling  \n",
      "This implementation uses two deep l earning models. \n",
      "Bidirectional Long Short -Term Memory (BiLSTM) \n",
      "classifies the genders in whispered speech, and Deep \n",
      "Convolutional Neural Network (DCNN) identifies \n",
      "emotions.  \n",
      "BiLSTM  [26] combines  two Recurrent Neural Networks \n",
      "(RNN) that are placed independently and can traverse \n",
      "backward and forward directions at each time step, as \n",
      "shown in Fig 7. There are two ways to deal with data in \n",
      "this model - The first involves processing data from the \n",
      "past to the future , and the second operates in the opposite \n",
      "direction, from future  to past , where two hidden layers of \n",
      "neurons help to preserve the information in both \n",
      "directions. This approach helps to improve the information \n",
      "available in the algorithm.  \n",
      "For B iLSTM, the sequence of inputs is \n",
      "                   for a traditional RNN, which \n",
      "computes the hidden vector sequences \n",
      "                  and results in the output \n",
      "sequences                    for the iteration \n",
      "    to                            (11) \n",
      "                (12) \n",
      "  are the weight matrices;   is the bias vector, and   is \n",
      "the hidden layer function.  \n",
      "DCNN  [27] is a Convolutional Neural Network (CNN) \n",
      "type that helps to identify emotions from whispered \n",
      "speech . The input to the model is the speech data that \n",
      "traverses through many stages, such as the convolution \n",
      "layer, the pooling layer, Activation, Fully Connected, \n",
      "Batch normalization, and dropout layers, as shown in Fig \n",
      "8.  \n",
      " \n",
      " \n",
      " \n",
      "Fig 7. Structure of BiLSTM  \n",
      " \n",
      "To avoid overfitting, Leaky ReLU is used as an activation \n",
      "function in this implementation and evaluated as  \n",
      "{                      \n",
      "                        (13) \n",
      "The fully connected layer is a loss layer, measuring the \n",
      "inconsistency between the desired and actual outputs . Root \n",
      "The Mean Squared Propagation (RMSProp) optimization \n",
      "algorithm enhances the loss function, which varies in \n",
      "vertical directions.   \n",
      " \n",
      "Fig. 8 Structure of Deep Convolution Neural Network  \n",
      "5-7- Emotions  \n",
      "Deep learning techniques are applied to identify the \n",
      "emotions of human speech. Discrete Emotion [3] \n",
      "theory uses seven vital emotions in Human activities.  \n",
      " Sadness:  This is the feeling of dissatisfaction, \n",
      "sorrow, or fruitlessness.  \n",
      " Happiness:  This is the emotional state that elicits \n",
      "satisfaction and pleasure.  \n",
      " Fear:  This feeling triggers a fright response.  \n",
      " Anger:  This emotional state leads to feelings of \n",
      "hostility and frustration.  \n",
      " Surprise:  It is the state of mind that expresses \n",
      "either positive or negative following something \n",
      "unexpected.  \n",
      "\n",
      " \n",
      "Mohanty, & Cherukuri, Whispered Speech Emotion  Recognition with Gender Detection using BiLSTM and DCNN  \n",
      " \n",
      " \n",
      " \n",
      "158 \n",
      " Disgust:  A strong emotion that results in feeling \n",
      "repulsed.  \n",
      " Neutral:  This is the feeling of lack of particular \n",
      "preference.  \n",
      " \n",
      "5-8- Algorithm for Emotion Recognition  \n",
      "The algorithm to detect emotions from the whispered \n",
      "speech is in two parts  \n",
      " Gender Detection  \n",
      " Emotion Identification   \n",
      " \n",
      "Algorithm 1:  Geneder Detection  \n",
      "INPUT: Text  File (Whispered Speech Samples)  \n",
      "OUTPUT: Emotions (Sad, Happy, Fear, Anger, Surprise, \n",
      "Disgust, Neutral)  \n",
      " \n",
      "1: Begin:  \n",
      "2: Read speech  samples from Corpus   \n",
      "3: Pre-processing  the speech  samples:  \n",
      "4: Processed_speech=Pre_processing (Speech_Sample)   \n",
      "5: Extracted  SDC,  Pitch  features:  \n",
      "6: SDC, Pitch=Feature_Extraction(Processed_speech)   \n",
      "7: Multifeature  fusion:  \n",
      "8: multi _feature = multifeatured _fusion(SDC,  Pitch)   \n",
      "9: Dimension  reduction:  \n",
      "10: Dimension _R = Dimension _Reduction(multi _feature)  \n",
      "11: Determine  gender:  \n",
      "12: Gender = BiLSTM  (Dimension _R) \n",
      "13: Placed  in folders:  \n",
      "14: Gender=  (Male,  Female)   \n",
      "15: End:  \n",
      "Algorithm 2 Emotion Identification  \n",
      " \n",
      "1:  Begin:   \n",
      "2:  Speech Samples = (Male,  Female)  \n",
      "3:  Data Augmentation:  \n",
      "4:  Augemented _samples = Data Augmentation  (Speech          \n",
      "Samples)  \n",
      "5:  Pre-processed  the speech  samples:   \n",
      "6:  Processed _speech=Preprocessing  (Augemented _sam \n",
      "ples)  \n",
      "7:  Extracted  Spectral  Centroid,  ChromaSTFT,  MFCC,  \n",
      "Mel spectrogram  features:   \n",
      "8:  Spectral _Centroid,  Chroma _STFT,  MFCC,  \n",
      "Mel_spectrogram = Feature Extraction  (Processed _spe \n",
      "ech)  \n",
      "9:  Multifeature  fusion:   \n",
      "10:  multi _feature = \n",
      "multifeatured _fusion(Spectral _Centroid,  Chroma _STFT,  \n",
      "MFCC,  Mel_spectrogr am) \n",
      "11:  Dimension  reduction:  \n",
      "12:  Dimension_R = Dimension_Reduction(multi_feature)  \n",
      "13:  Emotion  detection:  14:  Emotions = DCNN(Dimension_R)   \n",
      "15:  OUTPUT Emotions  (Sad, Happy, Fear, Anger, \n",
      "Surprise, Disgust, Neutral)  \n",
      "16:  End:  \n",
      "6- Experiment and Result Analysis  \n",
      "The proposed SER has been implemented utilizing Python \n",
      "programming language and fortified by various machine \n",
      "learning libraries and additional supporting libraries . The \n",
      "experiment uses Python (Python 3.6.3rc1) and Librosa \n",
      "(Librosa 0.8.0) for audio processing. Graph plotting uses \n",
      "Seaborn and Matplotlib libraries, which help to analyze the \n",
      "speech data. BiLSTM and DCNN models implement the \n",
      "model using Keras (Keras - 2.6.0), TensorFlow \n",
      "(TensorFlow -2.6.0), and Scikit -learn libraries .  \n",
      "wTIMIT dataset is collected and divided into the train, \n",
      "cross -validation, and test samples. Subsequently, the \n",
      "speech processing takes place during implementation, \n",
      "incorporating data -augmentation techniques like shifting \n",
      "time, changing pitch, and changing speed . The pre -\n",
      "processing of speech content from each sample helps to \n",
      "analyze it and extract its features. The dimensionally \n",
      "reduced extracted features align with the data points in the \n",
      "BiLSTM model. The training dataset makes the model \n",
      "learn the features or data. Epochs continuously learn \n",
      "hidden features when the dataset completes backward and \n",
      "forward iterations. Then the cross -validation dataset is \n",
      "used to estimate the model’s performance on each epoch \n",
      "and prevent the model from being overfit ted. The test \n",
      "dataset helps to evaluate the trained model unbiasedly \n",
      "using performance metrics like precision, recall, f1 score, \n",
      "and confusion Matrix .  \n",
      "The parameters used in BiLSTM models are as in Table 1.  \n",
      "Table 1: Parameters of the BiLSTM model  \n",
      "Layers  Shape of output  Parameters#  \n",
      "Embedding  \n",
      "(Embedding)  (None, 216, 256)  \n",
      " 524544  \n",
      " \n",
      "Bidirectional  \n",
      " (None, 128)  \n",
      " 98816  \n",
      "batch normalization  (None, 128)  \n",
      " 512 \n",
      "Dense (Dense)  (None, 128)  \n",
      " 10512  \n",
      "dropout (Dropout)  (None, 128)  \n",
      " 0 \n",
      "flatten (Flatten)  (None, 128)  \n",
      " 1024  \n",
      "dense1(Dense)  \n",
      " (None, 12)  \n",
      " 0 \n",
      "dense2(Dense)  \n",
      " (None, 12)  \n",
      " 455 \n",
      " \n",
      " \n",
      "Journal of Information Systems and Telecommunication, Vol.12, No.2,  April -June 2024  \n",
      " \n",
      " \n",
      "159 \n",
      "The BiLSTM model helps to segregate the speech samples \n",
      "into male and female. The segregated speech samples \n",
      "move to their respective male and female folders.  \n",
      "Table 2: Parameters of the DCNN model  \n",
      "Layers  Shape of output  Parameters#  \n",
      "conv1d (Conv1D)  \n",
      " (None, 216, 256)  \n",
      " 2304  \n",
      "Activation  \n",
      "(Activation)  (None, 216, 256)  \n",
      " 0 \n",
      "conv1d 1 (Conv1D)  \n",
      " (None, 216, 256)  524544  \n",
      "batch normalization  \n",
      " (None, 216, 256)  1024 \n",
      "activation 1  \n",
      "(Activation)  \n",
      " (None, 216, 256) 0 \n",
      "dropout (Dropout)  (None, 216, 256)  \n",
      " 0 \n",
      "max pooling1d \n",
      "(MaxPooling1D)  \n",
      " (None, 13, 256)  \n",
      " 0 \n",
      "conv1d 2 (Conv1D)  \n",
      " (None, 13, 128)  \n",
      " 262272  \n",
      " \n",
      "activation 2 \n",
      "(Activation)  \n",
      " (None, 13, 128)  \n",
      " 0 \n",
      "conv1d 3 (Conv1D)  \n",
      " (None, 13, 128)  \n",
      " 131200  \n",
      " \n",
      "activation 3 \n",
      "(Activation)  \n",
      " (None, 13, 128)  \n",
      " 0 \n",
      "conv1d 4 (Conv1D)  \n",
      " (None, 13, 128)  \n",
      " 131200  \n",
      " \n",
      "batch normalization  \n",
      " (None, 13, 128)  \n",
      " 512 \n",
      "activation 4 \n",
      "(Activation)  \n",
      " (None, 13, 128)  \n",
      " 0 \n",
      "conv1d 5 (Conv1D)  \n",
      " (None, 13, 128)  \n",
      " 131200  \n",
      " \n",
      "batch normalization \n",
      "1 \n",
      " (None, 13, 128)  \n",
      " 512 \n",
      "activation 5 \n",
      "(Activation)  \n",
      " (None, 13, 128)  \n",
      " 0 \n",
      "dropout 1 (Dropout)  \n",
      " (None, 13, 128)  \n",
      " 0 \n",
      "max pooling1d 1 \n",
      "(MaxPooling1D)  \n",
      " (None, 1, 128)  \n",
      " 0 \n",
      "conv1d 6 (Conv1D)  \n",
      " (None, 1, 64)  \n",
      " 65600  \n",
      "activation 6 \n",
      "(Activation)  \n",
      " (None, 1, 64)  \n",
      " 0 \n",
      "conv1d 7 (Conv1D)  \n",
      " (None, 1, 64)  \n",
      " 32832  \n",
      "activation 7 \n",
      "(Activation)  \n",
      " (None, 1, 64)  \n",
      " 0 \n",
      "flatten (Flatten)  \n",
      " (None, 64)  \n",
      " 0 dense (Dense)  \n",
      " (None, 14)  \n",
      " 910 \n",
      "activation 8 \n",
      "(Activation)  \n",
      " (None, 14)  \n",
      " 0 \n",
      " \n",
      "After separating the speech samples into genders, Chroma \n",
      "STFT, Spectral centroid, Mel-scale spectrogram, and \n",
      "Spectral Flux feature s extraction happen ed. Then the \n",
      "created data points are dimensionally reduced to fit into \n",
      "the DCNN deep learning model to identify emotions. \n",
      "Metrics like precision, recall, F1 score, and the confusion \n",
      "matrix aid in assessing the model's performance.  Table 2 \n",
      "mentions the parameters of the DCNN model.  \n",
      "6-1- Result Analysis  \n",
      "The fusion of Fundamental frequency and SDC \n",
      "detect s the gender of speech samples. The initial \n",
      "stage involves pre -processing the speech samples, \n",
      "followed by feature extraction, which includes SDC \n",
      "and fundamental frequencies. The combination of \n",
      "SDC and Fundamental speech features create a \n",
      "multifeature fusion resulting in a single set of data \n",
      "points. The next step is to reduce the dimensions of \n",
      "data p oints to use them as input to the BiLSTM \n",
      "Model. The BiLSTM model predicts the data in the \n",
      "dataset as male and female, placed in separate \n",
      "folders. Of these, 3342 speech samples are available, \n",
      "and 3294 are correctly classified. So, the accuracy of \n",
      "the model prediction is 99.59%. The precision, recall, \n",
      "and f1 -score values, along with the confusion matrix, \n",
      "are shown in Fig s 9 and 10. The predicted model \n",
      "identifies the female speech samples more accurately \n",
      "than the male.   \n",
      " \n",
      "Fig. 9 Confusion Matrix for Gender detection by BiLSTM.  \n",
      " \n",
      "The graph in Fig 9 shows the gender detection from the \n",
      "speech sample. The X -axis represents the true label, and \n",
      "the Y-axis shows the predicted label.  This graph gives the \n",
      "\n",
      " \n",
      "Mohanty, & Cherukuri, Whispered Speech Emotion  Recognition with Gender Detection using BiLSTM and DCNN  \n",
      " \n",
      " \n",
      " \n",
      "160 \n",
      "count of detected gender spe ech that the Bi -LSTM model \n",
      "correctly and incorrectly detects.  \n",
      " \n",
      "Fig. 10 Accuracy measures of Gender Detection.  \n",
      " \n",
      "2031 female and 1263 male speech are correctly detected, \n",
      "whereas 13 females and 35 males are incorrectly detected. \n",
      "Pitch values identify the emotions after detecting genders \n",
      "from the speech samples.  \n",
      "The extraction of MFCC, Mel -scale spectrogram, Chroma \n",
      "STFT, Spectral Flux, and Spectral Centroid speech \n",
      "features happens from speech samples. Then, all five \n",
      "speech features are fused into a single feature and \n",
      "dimensionally reduced. Finally, the DCNN model is \n",
      "applied to predict emotions.  \n",
      "Individual emotions are detected based on gender as \n",
      "shown in Fig 11. The deviation in the male speech \n",
      "emotions is less than the female . Female fear and female \n",
      "neutral  emotions show divergent results from other \n",
      "emotions. The accuracy of the model is 98.54%.   \n",
      " \n",
      " \n",
      "Fig.11 Emotions based on gender.  \n",
      "6-2- Comparison Analysis  \n",
      "This experiment evaluates the current implementation \n",
      "using the wTIMIT  dataset, resulting in an impressive \n",
      "accuracy of 98.54%. The performance compares with \n",
      "three -layered Long short -term memory Bidirectional \n",
      "RNNs (LSTM BiRNN) that use No -Attention, Feed \n",
      "Forward Attention (FFA), and Improved Feed Forward \n",
      "Attention Mechanism ( IFFA) to evaluate the emotions in \n",
      "the dataset. This model uses 35 hidden states to train the dataset. The models are trained and validated with the help \n",
      "of the wTIMIT dataset [28] and tested with the help of the \n",
      "CHAINS dataset,  as shown in Table 3. This ex periment is \n",
      "a new hypothesis for  emotion detection with gender \n",
      "identification on whispered speech.  Hence, there are fewer \n",
      "references available for similar investigations.  \n",
      "Table 3: Comparison of the Implementation  \n",
      "  Sl# Model Implementation  Accuracy  \n",
      "1 FFA, IFFA, LSTM, Bi_RNN [2 8] 97.6 %  \n",
      "2 Proposed Model  98.54 %  \n",
      "The proposed method improves the performance of \n",
      "emotions after segregating the speech samples into genders.  \n",
      "By separating the approach into two models, this \n",
      "implementation excels in capturing natural and \n",
      "spontaneous expressions of emotions with low latency \n",
      "manner .  \n",
      "7- Conclusion  and Future Work  \n",
      "Identification of gender from whispered speech and \n",
      "recognizing emotions is a complicated task. This \n",
      "implementation initially detects the gender from the \n",
      "speech samples before identifying the emotions. Emotions \n",
      "might vary based on gender in the same situation. Speech \n",
      "features such as Fu ndamental Frequency and SDC help \n",
      "with gender identification. MFCC, Mel -scale spectrogram, \n",
      "Spectral flux, Spectral Centroid, and Chroma STFT play a \n",
      "vital role in detecting emotions. Multifeature fusion helps \n",
      "to combine speech features into a single set of d ata points . \n",
      "The concept verifies with the publicly available dataset \n",
      "wTIMIT with an accuracy of 98.54%. This approach helps \n",
      "to identify nearly inaudible emotions and is used to figure \n",
      "out the strategy. The proposed methodology can be \n",
      "improved using other s peech features, machine learning, \n",
      "and deep learning concepts.  \n",
      " \n",
      "References  \n",
      "[1]  ST Jovicic , and Z  Saric , ―Acoustic analysis of  consonants in \n",
      "whispered speech,‖ Journal of voice , vol 22, no. 3, pp. 263–\n",
      "274, 2008 . \n",
      "[2]  M Kumari , and I  Ali, ―An efficient algorithm for gender \n",
      "detection using voice samples,‖ 2015 Communication, \n",
      "Control and Intelligent Systems (CCIS),  2015 , Mathura, \n",
      "Utter Pradesh,  pp. 221–226, doi: \n",
      "10.1109/CCIntelS.20 15.7437912 . \n",
      "[3] S Motamed , S Setayeshi, A Rabiee , and A Sharifi, ―Speech \n",
      "Emotion Recognition Based on Fusion Method,‖ Journal of \n",
      "Information Systems and Telecommunication (JIST), vol. 3, \n",
      "pp. 50--56, 2017, doi: 10.7508/jist.2017.17.007 .  \n",
      "[4] JS Li, CC Huang , ST Sheu , and MW  Lin, ―Speech emotion \n",
      "recognition and its applications,‖ Proc. of Taiwan Institute of \n",
      "Kansei Conference , 2010  Paris , France , pp. 187–192.  \n",
      "\n",
      " \n",
      "Journal of Information Systems and Telecommunication, Vol.12, No.2,  April -June 2024  \n",
      " \n",
      " \n",
      "161 \n",
      "[5] A Guerrieri , E Braccili , F Sgro , and GN Meldolesi  ―Gender \n",
      "identification in a two -level hierarchical speech emotion \n",
      "recognition system for an Italian Social Robot,‖ Sensors, vol. \n",
      "22, no. 5, pp. 1714, 2022 , doi: 10.3390/s22051714 . \n",
      "[6] M Sarria -Paja, TH Falk, and D  O’Shaughnessy , ―Whispered \n",
      "speaker verification and gender detection using weighted \n",
      "instantaneous frequencies,‖ 2013 IEEE International \n",
      "Conference on Acoustics, Speech and Signal Processing , \n",
      "2013  (May 26 -31), Vancouver , Canada, pp. 7209 –7213 , doi: \n",
      "10.1109/ICASSP.2013.6639062 .  \n",
      "[7] J Deng , S Fruhholz, Z Zhang , and Bojrn Schuller , \n",
      "―Recognizing emotions from whispered speech based on \n",
      "acoustic feature transfer learning,‖ IEEE Access , vol.  5, pp. \n",
      "5235 –5246 , 2017 .  \n",
      "[8] M Cotescu , T Drugman, G Huybrechts, J Lorenzo -Trueba, \n",
      "and A Moinet , ―Voice conversion for whispered speech \n",
      "synthesis,‖  IEEE Signal Processing Letters , vol. 27, pp. 186–\n",
      "190, 2019 , doi: 10.1109/LSP.2019.2961213 .  \n",
      "[9] P Mishra , and R Sharma, ―Gender differentiated \n",
      "convolutional neural networks for speech emotion \n",
      "recognition,‖ 2020 12th International Congress on Ultra -\n",
      "Modern Telecommuni - cations and Control Systems and \n",
      "Workshops (ICUMT),  2020  (October 5 -7), Brno , Czech \n",
      "Republic,  pp. 142–148, doi: \n",
      "10.1109/ICUMT51630.2020.9222412 . \n",
      "[10] Mustaqeem S Kwon, ―Optimal feature selection based \n",
      "speech emotion recognition using two -stream deep \n",
      "convolutional neural network,‖ International Journal of \n",
      "Intelligent Systems , vol. 36, no. 9, pp. 5116 – 5135 , 2021 , \n",
      "doi:  10.1002/int.22505 . \n",
      "[11] J. Ancilin and A. Milton, ―Improved speech emotion \n",
      "recognition with mel frequency magnitude coefficient ,‖ \n",
      "Applied Acoustics , vol. 179, pp. 108046 , 2021 , doi: \n",
      "10.1016/j.apacoust.2021.108046 .  \n",
      "[12] S. Jothimani and K. Premalatha, ―Mff -saug: Multi feature \n",
      "fusion with spectrogram augmentatio n of speech emotion \n",
      "recognition using convolution neural network ,‖ Chaos, \n",
      "Solitons & Fractals , vol. 162, pp. 112512 , 2022 , doi: \n",
      "10.1016/j.chaos.2022.112512 .  \n",
      "[13] B Yalamanchili, SK Samayamantula, and KR Anne, ―Neural \n",
      "network -based blended ensemble learning for speech \n",
      "emotion recognition,‖ Multidimensional Systems and Signal \n",
      "Processing, vol. 33, no. 4, pp. 1323 --1348 , 2022 , doi: \n",
      "10.1007/s11045 -022-00845 -9.  \n",
      "[14] T Feng , R Hebbar , and S Narayanan, ―Trustser: On the \n",
      "trustworthiness of fine -tuning pre -trained speech embeddings \n",
      "for speech emotion recognition,‖ arXiv preprint \n",
      "arXiv:2305.11229, 2023 , doi:  10.48550/arXiv.2305.11229  \n",
      "[15] RV Darekar and M Chavan , S Sharanyaa,  and NR Ranjan , \n",
      "―A hybrid meta -heuristic ensemble based classification \n",
      "technique speech emotion recognition,‖ Advances in \n",
      "Engineering Software, vol. 180, pp. 103412 , 2023 . \n",
      "[16] J Rekimot o, ―Dualvoice: A speech interaction method using \n",
      "whisper -voice as commands,‖ CHI Conference on Human \n",
      "Factors in Computing Systems Extended Abstracts,  pp. 1–6, \n",
      "2022 , doi: 10.1145/3491101.3519700 . \n",
      "[17]H Dolka,  AX VM , and S Juliet,―  Speech  emotion r ecognition  \n",
      "using  ANN on MFCC  features,‖ 2021 3rd international \n",
      "conference on signal processing and communication (ICPSC) , \n",
      "2021, Coimbatore, India, pp. 431–435, doi: \n",
      "10.1109/ICSPC51351.2021.9451810 . [18] MK Reddy and KS Rao, ―Robust pitch extraction method \n",
      "for the hmm -based speech synthesis system,‖ IEEE signal \n",
      "processing letters , vol.  24, no. 8, pp. 1133 –1137 , 2017 , doi: \n",
      "10.1109/LSP.2017. 2712646 .  \n",
      "[19] J Chatterjee, V Mukesh, HH Hsu , G Vyas , and Z Liu , \n",
      "―Speech emotion recognition using cross - correlation and \n",
      "acoustic features,‖ 2018 IEEE 16th Intl Conf on Dependable, \n",
      "Autonomic and Secure Computing, 16th Intl Conf on \n",
      "Pervasive Intelligence and Computing, 4th Intl Conf on Big \n",
      "Data Intelligence and Computing and Cyber Science and \n",
      "Technology Congress , 2018  (August 12-15), Athens, Greece , \n",
      "pp. 243–249, doi: 10.1109/DASC/PiCom/DataCom/CyberSci  \n",
      " Tec.2018.00050 .  \n",
      "[20] S Rajesh and NJ Nalini, ―Musical instrument emotion \n",
      "recognition using deep recurrent neural network,‖ Procedia \n",
      "Computer Science, vol. 167, pp. 16 --25, 2020, doi: \n",
      "10.1016/j.procs.2020.03.178 . \n",
      "[21] M Aly , and NS Alotaibi, ―A novel deep learning model to \n",
      "detect covid -19 based on wavelet features extracted from \n",
      "mel- scale spectrogram of patients’ cough and breathing \n",
      "sounds,‖ Informatics in Medicine Unlocked , vol.  32, pp.  \n",
      "101049 , 2022, doi: 10.1016/j.imu.2022.101049 . \n",
      "[22] Z Qawaqneh,  AA Mallouh,  and BD Barkana, ― Age and \n",
      "gender  classification  from  speech and face images by jointly \n",
      "fine-tuned deep neural networks,‖ Expert Systems with \n",
      "Applications , vol. 85, pp. 76–86, 2017 , doi: \n",
      "10.1016/j.eswa.2017.05.037 .  \n",
      "[23] A Koduru, HB Valiveti and AK Budati, ―Feature extraction \n",
      "algorithms to improve the speech emotion recognition rate,‖ \n",
      "International Journal of Speech Technology , vol. 23, no. 1, \n",
      "pp. 45–55, 2020 , doi: 10.1007/s10772 -020-09672 -4.  \n",
      "[24] S Zhang , and C Li, ―Research on feature fusion speech \n",
      "emotion recognition technology for smart teaching,‖ Mobile \n",
      "Information Systems , vol. 2022 , 2022 , doi:  \n",
      "10.1155/2022/7785929 . \n",
      "[25] GB, Prasanna, SV Bhat, C Naik , and HN Champa, ―An \n",
      "Efficient Method for Handwritten Kannada Digit \n",
      "Recognition based on PCA and SVM Classifier,‖ Journal of \n",
      "Information Systems and Telecommunication (JIST), vol. 3, \n",
      "no. 35, pp. 169 2021, doi: 20.1001.1.23221437.2021.9.35.3.2 . \n",
      "[26] A Graves, N Jaitly, and A Mohamed, ―Hybrid speech \n",
      "recognition wit h deep bidirectional lstm,‖ 2013 IEEE \n",
      "workshop on automatic speech recognition and \n",
      "understanding , 2013, Olomouc, Czech Republic, pp.  273–\n",
      "278, doi: 10.1109/ASRU.2013.6707742 .  \n",
      "[27] N Aloysius and M Geetha, ―A review on deep convolutional \n",
      "neural networks,‖ 2017 international conference on \n",
      "communication and signal processing (ICCSP), 2017, pp. \n",
      "0588 –0592 , IEEE, doi: 10.1109/ICCSP.2017.8286426 .  \n",
      "[28] SBC Gutha, MAB Shaik, T Udayakumar , and AA \n",
      "Saunshikhar , ―Improved feed forward attention mechanism in \n",
      "bidirectional recurrent neural networks for robust sequence \n",
      "classification,‖ 2020 International Conference on Signal \n",
      "Processing and Communications (SPCOM) , 2020 , IISc, \n",
      "Bangalore.  IEEE , pp. 1—5, doi: \n",
      "0.9179606 10.1109/SPCOM50965.202  \n",
      "\n",
      "\n",
      "Reading: d:\\PythonCode\\document_portal\\notebook\\data\\CancerTumorDetectionusingGeneticMutatedDataandMachineLearningModels.pdf\n",
      " \n",
      " \n",
      " \n",
      "Journal of Information Systems and Telecommunication  \n",
      "Vol.12, No.2,  April -June 2024, 152-161 \n",
      " \n",
      "http://jist.acecr.org  \n",
      "ISSN 2322 -1437  / EISSN:2345 -2773  \n",
      " \n",
      "Whispered Speech Emotion Recognition with Gender Detection using \n",
      "BiLSTM and DCNN  \n",
      "Aniruddha Mohanty1*, Ravindranath C Cherukuri1 \n",
      " \n",
      "1.Department of Computer  science and Engineering, Christ (Deemed to be University), Bangalore, India  \n",
      " \n",
      " \n",
      "Received: 22 Aug 2023 / Revised: 04 Apr 2024 / Accepted: 21 May 2024  \n",
      " \n",
      " \n",
      "Abstract   \n",
      "Emotions are human mental states at a particular instance in time concerning one’s circumstances, mood, and relationships \n",
      "with others . Identifying emotions from the whispered speech is complicated as the conversation might be confidential. The \n",
      "representation of the speech relies on the magnitude of its information. Whispered speech is intelligible, a low -intensi ty \n",
      "signal, and varies from normal speech. Emotion identification is quite tricky from whispered speech. Both prosodic and \n",
      "spectral speech features help to identify emotions. The emotion identification in a whispered speech happens using prosodic \n",
      "speech fea tures such as zero -crossing rate  (ZCR) , pitch, and spectral features that include spectral centroid, chroma STFT, \n",
      "Mel scale spectrogram, Mel -frequency cepstral coefficient (MFCC), Shifted Delta Cepstrum (SDC), and Spectral Flux. \n",
      "There are two parts to the proposed implementation. Bidirectional Long  Short -Term Memory (BiLSTM) helps to identify \n",
      "the gender from the speech sample in the first step with SDC and pitch. The Deep Convolutional Neural Network (DCNN) \n",
      "model helps to identify the emotions in the second  step. This implementation is evaluated using the  wTIMIT data corpus \n",
      "and gives 98.54% accuracy. Emotions have a dynamic effect on genders, so this implementation performs better than \n",
      "traditional approaches. This approach helps to design online learning management systems, different applications for \n",
      "mobile devices, checking cyber -criminal activities, emotion detection for older people, automatic speaker identification and \n",
      "authentication, forensics, and surveillance . \n",
      " \n",
      "Keywords:  Whispered Speech; Emotion Re cognition; Speech Features; Data Corpus; BiLSTM;  DCNN . \n",
      " \n",
      "1- Introduction  \n",
      "Emotions are the humans’ short -lived feeling s that affects \n",
      "thinking, actions, relationships, and social interactions. \n",
      "Emotions express humans’ physiological and emotional \n",
      "states with facial expressions and body language. \n",
      "Whispered speech is a form of speec h that expresses \n",
      "emotions. Whispered speech is a type of communication \n",
      "produced with breath without any noise and excitation of \n",
      "voice. The whispered speech structure changes \n",
      "significantly because of the lack of periodic excitation in \n",
      "the voice. This result s in missing speech and reduced \n",
      "transparency in communication.  \n",
      "The difference between normal and whispered speech is \n",
      "the absence of vocal tract vibrations due to the vocal \n",
      "tract’s physiological blocking. The strength of whispered \n",
      "speech is minute and with out voice compared to phoned \n",
      "speech. The spectral and prosodic features help to detect \n",
      "the whispered speech. Prosodic features of speech vary \n",
      "over time. Spectral features of whispered speech are highly accurate over unvoiced consonants, voiced consonants, \n",
      "and formants in vowels [1]. \n",
      "The impacts on Speech Emotion Recognition ( SER) are \n",
      "due to various acoustic conditions such as compressed, \n",
      "noisy, telephonic conversation s and imitator speeches. \n",
      "Other environmental conditions,  such as stress, rhythm, \n",
      "and intonation, can affect  SER.  Whispered speech is also \n",
      "affected by similar acoustic and environmental conditions. \n",
      "SER depends on acoustic characteristics and gender in \n",
      "some scenarios. Hence gender plays a vital role in SER.  \n",
      "Nowadays, several whispered speech  samples of male and \n",
      "female voices are available. The effectiveness of the SER \n",
      "is more when the implementation is in two parts. The \n",
      "identification of the gender [2] happens initially using \n",
      "Bidirectional Long Short -term Memory (BiLSTM)  with \n",
      "speech features . The next step is detecting emotions using \n",
      "various speech features [3] and Deep Convolutional \n",
      "Neural Network (DCNN).   \n",
      "The structure of the paper includes various sections. \n",
      "Section 2 describes Human Emotions and their \n",
      "Applications. Details of the Related Work are in Section 3. \n",
      "Section 4 describes the System Model, which is the black \n",
      "box view of the implementation. Section 5 is the Model \n",
      " \n",
      "Journal of Information Systems and Telecommunication, Vol.12, No.2,  April -June 2024  \n",
      " \n",
      " \n",
      "153 \n",
      "Design that describes the details of the speech features and \n",
      "the deep learning models used in this implementation. \n",
      "Section 6 gives the Experiment and Result Analysis. \n",
      "Section 7 brief ly discuss  the Conclusion and Future Work.   \n",
      "2- Human  Emotions  and their  Applications  \n",
      "Human emotions are the mental state caused by countless \n",
      "associated views , feeling s, behavioral replies, and the \n",
      "degree of pressure and annoyance. It is often associated \n",
      "with attitude , temperament, behavior , disposition, and \n",
      "creativi ty [4]. Emotion recognition helps to detect a \n",
      "humans ’ emotional mood , which lasts hours and days. \n",
      "Speech conveys emotions such as anger, disgust, fear, \n",
      "happiness, neutrality, sadness, and surprise. The machine \n",
      "automatically detects various emotions from speech with \n",
      "the help of different algorithms. Emotion Recognition \n",
      "helps to : \n",
      " \n",
      "•  Detect customers ’ intentions based on the teleconference. \n",
      "•  Detect cybercrime.  \n",
      "•  Students ’ attention and teachers ’ content adjustment.  \n",
      "•  Disability assistance  \n",
      "•  Customer satisfaction  \n",
      "•  Stress monitoring  \n",
      "•  Social media analysis  \n",
      "•  Suspicious activity  \n",
      "•  Human -machine interaction and so on.   \n",
      "3- Related Work  \n",
      "Over time, numerous studies have detected emotions from \n",
      "whispered speech . The vario us deep learning models \n",
      "detect emotions based on the speech features extracted \n",
      "from the collected whispered speech data corpus . The SER \n",
      "for normal and whispered speech is diverse because of \n",
      "vocal excitation. Emotions vary with many factors; gender \n",
      "is among  the most influential factors  [5]. Identifying \n",
      "gender in the first step improves emotion detection from \n",
      "whispered speech. So, the related work explored is on \n",
      "gender detection and emotion recognition from whispered \n",
      "speech.  \n",
      "MFCC obtained by the Hilbert enve lope approach and \n",
      "weighted instantaneous frequencies (WIFs) obtained by \n",
      "the coherent demodulation help to detect gender in \n",
      "whispered speech  samples [6]. There is an opportunity to \n",
      "explore gender detection in noisy speech conditions using \n",
      "these approaches.  \n",
      "Autoencoder -enabled features  in the transfer learning \n",
      "framework propose to practice  phonated data to identify \n",
      "emotions from Whispered speech [7]. The feature \n",
      "extraction is from the Geneva Whispered Emotion Corpus \n",
      "(GeWEC) and Berlin Emotional Speech Databa se (EMO -\n",
      "DB) data corpus. The acoustic features such as Mel-frequency cepstral coefficient (MFCC), root mean square \n",
      "(RMS), frame energy, zero -crossing -rate (ZCR), pitch \n",
      "frequency (F0), probability of voicing autocorrelation \n",
      "function are the inputs to evaluate the framework. \n",
      "Implementing deep learning concepts on spontaneous data \n",
      "gives more accuracy than the current framework .  \n",
      "Gender is detected to target an anonymous speaker. The \n",
      "Deep Neural Network (DNN) model is used to generalize \n",
      "gender by using the MFCC speech feature. This \n",
      "implementation is applied to the wTIMIT dataset to verify \n",
      "the gender and recognize the speaker  [8]. The DNN model \n",
      "cannot generalize gender; evaluation can happen with \n",
      "other datasets.   \n",
      "MFCC and CNN, with the fully  connected network, detect \n",
      "gender and emotions like anger, disgust, fear, happiness, \n",
      "sadness, surprise, and a neutral state  [9]. The concept \n",
      "verification happens on RAVDESS, CREMA -D, SAVEE, \n",
      "and TESS datasets, having an accuracy of 92.283%, which \n",
      "is better than the tr aditional model.  \n",
      "The final prediction of the implemented model is to learn \n",
      "the mutually spatial -spectral features happens by a two -\n",
      "stream deep convolutional neural network with an iterative \n",
      "neighborhood  component analysis (INCA) and the most \n",
      "discriminatory optimal features  [10]. The concept \n",
      "verification happens on EMO -DB, SAVEE, and \n",
      "RAVDESS emotional speech corpora which perform with \n",
      "95%, 82%, and 85% accuracy rates. Real -time applications \n",
      "with natural and huge data corpora can help to extend this \n",
      "concept to identify emotions.  \n",
      "Mel Frequency Magnitude Coefficient (MFMC) and three \n",
      "spectral features, namely MFCC, log frequency power \n",
      "coefficient, and linear prediction cepstral coefficient, are \n",
      "used with the help of Support Vector Machine (SVM) \n",
      "modeling  [11]. The performance evaluation uses this \n",
      "concept  on Berlin, RAVDESS, SAVEE, EMOVO, and \n",
      "eNTERFACE data corpus . Feature selection, feature \n",
      "fusion, and multiple classification schemes improve the \n",
      "performance of MFMC.  \n",
      "The proposed MFF -SAug research in  which noise removal \n",
      "improves emotion. White Noise Injection, Pitch Tuning \n",
      "techniques, MFCC, ZCR, RMS speech features, and \n",
      "Convolutional Neural Network (CNN) modeling [12] \n",
      "detect emotions. Emotion detection during the interaction \n",
      "between people can extend the MFF -SAug approach .  \n",
      "The proposed Neural Network -based Blended Ensemble \n",
      "Learning (NNBEL) model is composed of a 1 -dimensional \n",
      "Convolution Neural Network (1D -CNN), Long Short -\n",
      "Term Memory (LSTM), and CapsuleNets. LSTM receives \n",
      "input from the Log Mel -spect rogram speech features, \n",
      "while 1D -CNN and CapsuleNets receive input from the \n",
      "MFCC.  Each model’s output is fed to Multi -Layer \n",
      "Perceptron (MLP) and predicts the final emotion s [13]. \n",
      "This model shows 95.3% and 94% accuracy on \n",
      "RAVDESS and IEMOCAP datasets, respectively.  \n",
      " \n",
      "Mohanty, & Cherukuri, Whispered Speech Emotion  Recognition with Gender Detection using BiLSTM and DCNN  \n",
      " \n",
      " \n",
      " \n",
      "154 \n",
      "TrustSER [14] implemented a general framework to \n",
      "determine the SER system's trustworthiness using deep \n",
      "learning techniques. Trustworthiness evaluates privacy \n",
      "(gender in formation, speaker demographics), safety, \n",
      "fairness, sustainability, and emotions (sad, angry, happy, \n",
      "neutral). The architecture of the TrustSET framework uses \n",
      "CNN encoder and Transformer encoder models. \n",
      "Trustworthy profiles under the Federated Learning \n",
      "scenario might improve privacy, fairness, and safety.  \n",
      "A hybrid meta -heuristic ensemble -based classification [15] \n",
      "helps to detect speech emotions. Raw speech samples are \n",
      "filtered using the Butterworth filter; then, spectrogram \n",
      "speech features are extracted from each frame to create a \n",
      "hybrid feature vector. The ensemble -based classification is \n",
      "applied to hybrid feature vectors to classify em otions. The \n",
      "ensemble -based classification contains a Recurrent Neural \n",
      "Network (RNN), a Deep Belief Network (DBN), and an \n",
      "Artificial Neural Network (ANN).  \n",
      "The above -related work shows that it is crucial to identify \n",
      "gender to segregate emotions in a speech. Male and female \n",
      "emotions are different based on situations. Whispered \n",
      "speech is an essential concept of emotion recognition. \n",
      "There is an opportunity to experiment further based on \n",
      "other datasets of whispered speech. So, this is a motivation \n",
      "to work on emot ion recognition in whispered speech, as \n",
      "this is a less explored area of research .  \n",
      "4- System Model  \n",
      "The analysis of e motion recognition from the whispered \n",
      "speech segregates into two parts. One is gender detection, \n",
      "and the second is emotion detection as shown in Fig 1.  \n",
      " \n",
      " \n",
      "Fig. 1 Emotion recognition from whispered speech  \n",
      " \n",
      "As part of gender detection, pre -processing of the speech \n",
      "samples is performed, and then pitch and SDC are \n",
      "extracted as part of prosodic and spectral features, \n",
      "respectively. Both the features are fused to get a single \n",
      "feature set with the help of multifeature fusion. PCA helps \n",
      "to reduce the dimensions of extracted features and is used \n",
      "as input to Bidirectional Long short -term memory \n",
      "(BiLSTM) to classify genders , as shown in Fig 2.  \n",
      " \n",
      " \n",
      "Fig .2 Gender detection from whispered speech  \n",
      " \n",
      "Following  Gender detection, the augmentation of speech \n",
      "data helps to get more realistic data. Then the speech features such as Chromatogram, Zero -Crossing Rate, \n",
      "Spectral Central, MFCC, Spectral Flux , and Mel \n",
      "Spectrog rams are extracted. All the extracted features \n",
      "combine to get a single feature using multifeature fusion. \n",
      "Then the dimensions are reduced to get the optimal data \n",
      "points. The data points are inserted into the Deep \n",
      "Convolutional Neural Network (DCNN) to identify the \n",
      "emotions , as shown in Fig 3.  \n",
      "5- System  Design  \n",
      "The implementation of system design happens in two parts. \n",
      "The first part identifies the male and female gender from \n",
      "the whispered speech samples. Then the determination of \n",
      "different emotions like sadn ess, happiness, fear, anger, \n",
      "surprise, disgust, and neutral are detected in gender -\n",
      "segregated speech samples.  \n",
      " \n",
      "Fig. 3 Emotion Recognition from Male and Female whispered speech.  \n",
      "5-1- Whispered Speech Data Corpus  \n",
      "wTIMIT [16] is a whispered voice data corpus ha ving 450 \n",
      "phonetically balanced sentences with 29 speakers. There \n",
      "are 11,324 utterances with a normal voice , which  can be \n",
      "used for normal and whispered training. This data is \n",
      "available in two parts - train and test divisions. The \n",
      "samples contain an 8 kHz sampling rate with a high -\n",
      "quality  and low-pass filter.  \n",
      "5-2-    Data Augmentation  \n",
      "Data augmentation [17 ] is the method of getting more \n",
      "realistic data from the existing data, which helps to add \n",
      "more training data to the model, reduce overfitting, and \n",
      "increase the model’s generalization ability. Below are the \n",
      "steps to generate synthetic data.  \n",
      " Shifting Time : Shifting time is a simple concept \n",
      "where the audio shifts to the left or right with a \n",
      "haphazard second. If the audio shifts to the left \n",
      "\n",
      " \n",
      "Journal of Information Systems and Telecommunication, Vol.12, No.2,  April -June 2024  \n",
      " \n",
      " \n",
      "155 \n",
      "with z seconds, then the first z seconds are \n",
      "marked as silence. Similarly, if the audio shifts to \n",
      "the right with z second s, the last z seconds are \n",
      "marked as silence.  \n",
      " Changing Pitch: Pitch change is adjusting the \n",
      "pitch randomly without upsetting the speediness \n",
      "of the audio file.  \n",
      " Changing Speed: Speed audio changes by \n",
      "stretching the time series data by a fixed rate.  \n",
      "5-3- Pre-processing  \n",
      "After the data collection and data augmentation, pre -\n",
      "processing [3] is the initial  phase  in training SER. \n",
      "Framing, Windowing, Voice activity detection, \n",
      "Normalization, and Noise reduction are pre -\n",
      "processing steps.   \n",
      "Framing:  Speech signals are quasip eriodic and vary \n",
      "over time. Hence, information and related emotions \n",
      "also fluctuate over time. The segregation of the \n",
      "signals happens in a shorter period to make the \n",
      "speech signal invariant. Twenty milliseconds to thirty \n",
      "milliseconds helps to make the speec h signal \n",
      "invariant, and five milliseconds of overlap of the \n",
      "frames avoid data leakage between the frames .  \n",
      "Windowing:  Windowing on each frame helps to diminish \n",
      "data leakage during Fast Fourier Transformation (FFT) \n",
      "after framing. The Hamming window allows  this step \n",
      "where the window size is N for the frames     , used in \n",
      "equation (1).  \n",
      " \n",
      "                  (   \n",
      "   )       (1) \n",
      "            \n",
      " \n",
      "Voice activity detection (VAD):  An utterance has three \n",
      "portions of speech activities : voiced, unvoiced, and silent. \n",
      "Zero Crossing Rate (ZCR) speech feature helps to detect \n",
      "VAD. ZCR represents the frequency of signal transitions \n",
      "between positive and negative values within a specific \n",
      "frame . Due to high energy, the ZCR value is low for \n",
      "voiced speech and high for unvoiced s peech because of \n",
      "low energy.  \n",
      "Normalization:  Normalization helps to reduce  speaker \n",
      "and recording inconsistency without affecting the features’ \n",
      "strength and enhancing the features’ generalization \n",
      "capability. The Z-normalization method is used more and \n",
      "represented as  \n",
      "       \n",
      "      (2) \n",
      "Where   is the speech signal,   and   are the mean and \n",
      "standard deviation of the data, respectively.  \n",
      "Noise Reduction:  The environmental noise captured while \n",
      "recording a speech signal  affects the recognition rate . \n",
      "Minimum mean square error (MMSE) and log -spectral \n",
      "amplitude MMSE (LogMMSE) reduce the noise from the speech signals. Noise reduction helps to get more accuracy \n",
      "in SER evaluation.  \n",
      "5-4- Feature Extraction  \n",
      "Specific emotions are present in prosodic and spectral \n",
      "features of speech. The extraction of prosodic and spectral \n",
      "features is a vital characteristic of emotion recognition \n",
      "after pre -processing the speech signals.  \n",
      "5-4-1-Prosodic Features  \n",
      "Prosodic features deal with the audio qualities of a speech \n",
      "when connected speeches use sounds as input. The \n",
      "production of the speech deals with the amount of energy, \n",
      "frequency, period, loudness, pitch, and duration. Speech \n",
      "signal com munication depends on intonation, stress, and \n",
      "rhythm, which prosodic features can detect. The prosodic \n",
      "features used in the implementation are : \n",
      "Zero -Crossing Rate (ZCR):  ZCR [17] measures the \n",
      "frequency of signal transitions between positive and \n",
      "negative values in every audio frame  and defined as  \n",
      "    \n",
      "    ∑       [ ]         [      \n",
      "    (3) \n",
      "Where        is the sign function . \n",
      "    [     ]  {        \n",
      "       \n",
      "Fundamental Frequency (Pitch):  Fundamental \n",
      "Frequency (F0) [18] is the minimum freque ncy of the \n",
      "periodic waveform. F0 is the significant parameter to \n",
      "differentiate male speech from female speech. Pitch also \n",
      "determines the voiced and unvoiced portion of the speech \n",
      "signal. This analysis uses pitch parameters like pitch mean \n",
      "value and pitch r ange.  \n",
      "The pitch range determines the number of octaves a speech  \n",
      "sample  can cover, from the lowest to the highest. F0 value \n",
      "varies from 85Hz to 180 HZ for the voiced speech of adult \n",
      "males.  \n",
      " \n",
      " \n",
      "Fig. 4 Representation of Pitch data  \n",
      " \n",
      "Similarly, the F0 value for adult females goes from 165Hz \n",
      "to 255Hz, as shown in Fig 4.  \n",
      "\n",
      " \n",
      "Mohanty, & Cherukuri, Whispered Speech Emotion  Recognition with Gender Detection using BiLSTM and DCNN  \n",
      " \n",
      " \n",
      " \n",
      "156 \n",
      "5-4-2-Spectral features  \n",
      "The representation of spectral features happens by \n",
      "converting the time -domain speech signal into the \n",
      "frequency domain with the help of the Fast Fourier \n",
      "Transform (FFT) which  benefits  to represent the \n",
      "characteristics of the Human Vocal Tract. The  spectral \n",
      "features are  \n",
      "Spectral Centroid (SC):  The Spectral Centroid [19] is the \n",
      "most used speech feature where the positioning of the \n",
      "―center of mass‖ of the audio signal spectr um defines the \n",
      "weight of the spectrum. The center of mass measures the \n",
      "weighted average of the frequency component located in \n",
      "the audio signal, defined as  \n",
      " \n",
      "      ∑            \n",
      "   \n",
      "∑       \n",
      "       (4) \n",
      "Where      represents the magnitude of bin numb er   and \n",
      "      represents the central frequency of the bin.  \n",
      "Chroma STFT:  Chroma STFT [20] is obtained using FFT \n",
      "on speech samples and the resulting spectrums a re a \n",
      "chromatogram in a vertical axis. This feature captures the \n",
      "harmonic feature of the speech s ignals.  \n",
      "Mel-scale Spectrogram:  Mel-scale Spectrogram [ 21] is a \n",
      "spectrogram in which frequencies convert to the Mel scale, \n",
      "which helps to differentiate the range of frequencies. Mel -\n",
      "spectrogram helps to understand emotions in a better way \n",
      "as humans can perceive sound on a logarithmic scale.  \n",
      "Mel Frequency Cepstral Coefficient (MFCC ): MFCC \n",
      "[17] i s SER’s  standard feature extraction technique. The \n",
      "vocal cord s, tongue, and teeth filter the sound and make it \n",
      "unique for each speaker in the Human Speech production \n",
      "system. Mel scale represents MFCC, where the frequency \n",
      "bands are equally spaced and close to the Human Auditory \n",
      "System’s response, as shown in Fig 5.  \n",
      " \n",
      "Fig 5 MFCC speech feature extraction  \n",
      " \n",
      "Shifted Delta Cepstrum (SDC):  Time derivatives apply \n",
      "to the cepstral coefficients obtained from the MFCC and \n",
      "combined with the delta coefficient to get SDC, as shown \n",
      "in Fig 6.       and   are the four parameters used in \n",
      "SDC [ 22].  M denotes the cepstral coefficient for each \n",
      "frame.   indicates added frames.   are the frames that \n",
      "append delta features from the new feature vector .   \n",
      "represents the delta values difference. So, the coefficient \n",
      "vectors represent as   \n",
      "     [                  ]   (5) \n",
      "   are MFCC coefficients and   is the coefficient index. \n",
      "For a given time,   an intermediate calculation is done to \n",
      "obtain the   coefficients.  \n",
      "                                 (6) \n",
      "Finally, the SDC coefficients vectors of   dimensions are \n",
      "obtained as  \n",
      "       [          (                )] (7) \n",
      " \n",
      " \n",
      "Fig. 6 Extraction of SDC speech feature  \n",
      " \n",
      "This speech feature helps to identify genders for a long -\n",
      "range dynamic appearance in speech signals.  \n",
      "Spectral Flux:  Spectral flux [ 20] measures the change in \n",
      "speed of the signal’s power spectrum compared to the \n",
      "previous frame. This feature estimates the speech signal’s \n",
      "power spectrum about  the power spectrum of one frame \n",
      "with others.  \n",
      "5-5- Feature Selection and Dimension Reduction  \n",
      "Feature s election [23] is selecting features from a large set \n",
      "of extracted features to eliminate redundant and unused \n",
      "information and decrease processing time. For this work, \n",
      "the extracted pitch contains lots of information. The global \n",
      "features, like the range of p itch values and mean pitch \n",
      "values, are selected. Removing redundant and unused \n",
      "information, such as additional zeroes, duplicate values, or \n",
      "frames, is part of MFCC feature extraction. The exact \n",
      "process removes the silent speech frames.  \n",
      "Fundamental frequen cy and SDC are two speech features \n",
      "to identify gender. Similarly, Spectral centroid, Chroma \n",
      "STFT, MFCC, and Spectral flux features are used for \n",
      "emotion identification to create a single set of data points \n",
      "from multiple speech features using multifeature fu sion \n",
      "technique [24]. \n",
      "Multifeature Fusion:  The extracted speech features hav e \n",
      "different dimensions, so feature proximity usages  average \n",
      "dimensional spacing between the vectors to denote the \n",
      "proximity between the diverse features  [24]. The average \n",
      "dimensional spacing between the vectors is computed as  \n",
      "          \n",
      "    √∑∑              (8) \n",
      "   =  \n",
      "     √∑∑              (9) \n",
      "\n",
      " \n",
      "Journal of Information Systems and Telecommunication, Vol.12, No.2,  April -June 2024  \n",
      " \n",
      " \n",
      "157 \n",
      "        and     are the mean and variance interval of \n",
      "average dimensions of the feature vector s.    and    are \n",
      "the mean value of   and   type of sound features.     and    \n",
      "are the mean value of   and   type of sound features . Then \n",
      "the subsequent dimensionality reduction [25] method \n",
      "follows once the feature selection process is complete. \n",
      "High data variance is present in the extracted features \n",
      "containing more information. Dimension reduction \n",
      "techniques reduce the dimensions from extracted feature \n",
      "vectors.  \n",
      " \n",
      "5-5-1- Principal Component Analysis  (PCA)  \n",
      "PCA [25] is an approach for redu cing the dimensionality \n",
      "of extensive datasets, transforming them into more \n",
      "compact representations while retaining crucial \n",
      "information . These reductions in the number of variables \n",
      "help to get more accurate results. A reduced dataset makes \n",
      "it easier and fas ter to visualize and analyze the data. \n",
      "Following steps followed to explore PCA .  \n",
      " Standardization  creates a normalized dataset \n",
      "when there is a significant difference in the range \n",
      "of initial variables or a larger range of datasets \n",
      "dominates the smaller range. It can be done by  \n",
      "             \n",
      "                     (10) \n",
      " Covariance matrix computation  helps  to know  \n",
      "how the input  dataset  varies  from  the mean value \n",
      "to each data point.  \n",
      " Eigenvectors and eigenvalues  of the covariance \n",
      "matrix help to identify the principal component.  \n",
      "5-6- Modelling  \n",
      "This implementation uses two deep l earning models. \n",
      "Bidirectional Long Short -Term Memory (BiLSTM) \n",
      "classifies the genders in whispered speech, and Deep \n",
      "Convolutional Neural Network (DCNN) identifies \n",
      "emotions.  \n",
      "BiLSTM  [26] combines  two Recurrent Neural Networks \n",
      "(RNN) that are placed independently and can traverse \n",
      "backward and forward directions at each time step, as \n",
      "shown in Fig 7. There are two ways to deal with data in \n",
      "this model - The first involves processing data from the \n",
      "past to the future , and the second operates in the opposite \n",
      "direction, from future  to past , where two hidden layers of \n",
      "neurons help to preserve the information in both \n",
      "directions. This approach helps to improve the information \n",
      "available in the algorithm.  \n",
      "For B iLSTM, the sequence of inputs is \n",
      "                   for a traditional RNN, which \n",
      "computes the hidden vector sequences \n",
      "                  and results in the output \n",
      "sequences                    for the iteration \n",
      "    to                            (11) \n",
      "                (12) \n",
      "  are the weight matrices;   is the bias vector, and   is \n",
      "the hidden layer function.  \n",
      "DCNN  [27] is a Convolutional Neural Network (CNN) \n",
      "type that helps to identify emotions from whispered \n",
      "speech . The input to the model is the speech data that \n",
      "traverses through many stages, such as the convolution \n",
      "layer, the pooling layer, Activation, Fully Connected, \n",
      "Batch normalization, and dropout layers, as shown in Fig \n",
      "8.  \n",
      " \n",
      " \n",
      " \n",
      "Fig 7. Structure of BiLSTM  \n",
      " \n",
      "To avoid overfitting, Leaky ReLU is used as an activation \n",
      "function in this implementation and evaluated as  \n",
      "{                      \n",
      "                        (13) \n",
      "The fully connected layer is a loss layer, measuring the \n",
      "inconsistency between the desired and actual outputs . Root \n",
      "The Mean Squared Propagation (RMSProp) optimization \n",
      "algorithm enhances the loss function, which varies in \n",
      "vertical directions.   \n",
      " \n",
      "Fig. 8 Structure of Deep Convolution Neural Network  \n",
      "5-7- Emotions  \n",
      "Deep learning techniques are applied to identify the \n",
      "emotions of human speech. Discrete Emotion [3] \n",
      "theory uses seven vital emotions in Human activities.  \n",
      " Sadness:  This is the feeling of dissatisfaction, \n",
      "sorrow, or fruitlessness.  \n",
      " Happiness:  This is the emotional state that elicits \n",
      "satisfaction and pleasure.  \n",
      " Fear:  This feeling triggers a fright response.  \n",
      " Anger:  This emotional state leads to feelings of \n",
      "hostility and frustration.  \n",
      " Surprise:  It is the state of mind that expresses \n",
      "either positive or negative following something \n",
      "unexpected.  \n",
      "\n",
      " \n",
      "Mohanty, & Cherukuri, Whispered Speech Emotion  Recognition with Gender Detection using BiLSTM and DCNN  \n",
      " \n",
      " \n",
      " \n",
      "158 \n",
      " Disgust:  A strong emotion that results in feeling \n",
      "repulsed.  \n",
      " Neutral:  This is the feeling of lack of particular \n",
      "preference.  \n",
      " \n",
      "5-8- Algorithm for Emotion Recognition  \n",
      "The algorithm to detect emotions from the whispered \n",
      "speech is in two parts  \n",
      " Gender Detection  \n",
      " Emotion Identification   \n",
      " \n",
      "Algorithm 1:  Geneder Detection  \n",
      "INPUT: Text  File (Whispered Speech Samples)  \n",
      "OUTPUT: Emotions (Sad, Happy, Fear, Anger, Surprise, \n",
      "Disgust, Neutral)  \n",
      " \n",
      "1: Begin:  \n",
      "2: Read speech  samples from Corpus   \n",
      "3: Pre-processing  the speech  samples:  \n",
      "4: Processed_speech=Pre_processing (Speech_Sample)   \n",
      "5: Extracted  SDC,  Pitch  features:  \n",
      "6: SDC, Pitch=Feature_Extraction(Processed_speech)   \n",
      "7: Multifeature  fusion:  \n",
      "8: multi _feature = multifeatured _fusion(SDC,  Pitch)   \n",
      "9: Dimension  reduction:  \n",
      "10: Dimension _R = Dimension _Reduction(multi _feature)  \n",
      "11: Determine  gender:  \n",
      "12: Gender = BiLSTM  (Dimension _R) \n",
      "13: Placed  in folders:  \n",
      "14: Gender=  (Male,  Female)   \n",
      "15: End:  \n",
      "Algorithm 2 Emotion Identification  \n",
      " \n",
      "1:  Begin:   \n",
      "2:  Speech Samples = (Male,  Female)  \n",
      "3:  Data Augmentation:  \n",
      "4:  Augemented _samples = Data Augmentation  (Speech          \n",
      "Samples)  \n",
      "5:  Pre-processed  the speech  samples:   \n",
      "6:  Processed _speech=Preprocessing  (Augemented _sam \n",
      "ples)  \n",
      "7:  Extracted  Spectral  Centroid,  ChromaSTFT,  MFCC,  \n",
      "Mel spectrogram  features:   \n",
      "8:  Spectral _Centroid,  Chroma _STFT,  MFCC,  \n",
      "Mel_spectrogram = Feature Extraction  (Processed _spe \n",
      "ech)  \n",
      "9:  Multifeature  fusion:   \n",
      "10:  multi _feature = \n",
      "multifeatured _fusion(Spectral _Centroid,  Chroma _STFT,  \n",
      "MFCC,  Mel_spectrogr am) \n",
      "11:  Dimension  reduction:  \n",
      "12:  Dimension_R = Dimension_Reduction(multi_feature)  \n",
      "13:  Emotion  detection:  14:  Emotions = DCNN(Dimension_R)   \n",
      "15:  OUTPUT Emotions  (Sad, Happy, Fear, Anger, \n",
      "Surprise, Disgust, Neutral)  \n",
      "16:  End:  \n",
      "6- Experiment and Result Analysis  \n",
      "The proposed SER has been implemented utilizing Python \n",
      "programming language and fortified by various machine \n",
      "learning libraries and additional supporting libraries . The \n",
      "experiment uses Python (Python 3.6.3rc1) and Librosa \n",
      "(Librosa 0.8.0) for audio processing. Graph plotting uses \n",
      "Seaborn and Matplotlib libraries, which help to analyze the \n",
      "speech data. BiLSTM and DCNN models implement the \n",
      "model using Keras (Keras - 2.6.0), TensorFlow \n",
      "(TensorFlow -2.6.0), and Scikit -learn libraries .  \n",
      "wTIMIT dataset is collected and divided into the train, \n",
      "cross -validation, and test samples. Subsequently, the \n",
      "speech processing takes place during implementation, \n",
      "incorporating data -augmentation techniques like shifting \n",
      "time, changing pitch, and changing speed . The pre -\n",
      "processing of speech content from each sample helps to \n",
      "analyze it and extract its features. The dimensionally \n",
      "reduced extracted features align with the data points in the \n",
      "BiLSTM model. The training dataset makes the model \n",
      "learn the features or data. Epochs continuously learn \n",
      "hidden features when the dataset completes backward and \n",
      "forward iterations. Then the cross -validation dataset is \n",
      "used to estimate the model’s performance on each epoch \n",
      "and prevent the model from being overfit ted. The test \n",
      "dataset helps to evaluate the trained model unbiasedly \n",
      "using performance metrics like precision, recall, f1 score, \n",
      "and confusion Matrix .  \n",
      "The parameters used in BiLSTM models are as in Table 1.  \n",
      "Table 1: Parameters of the BiLSTM model  \n",
      "Layers  Shape of output  Parameters#  \n",
      "Embedding  \n",
      "(Embedding)  (None, 216, 256)  \n",
      " 524544  \n",
      " \n",
      "Bidirectional  \n",
      " (None, 128)  \n",
      " 98816  \n",
      "batch normalization  (None, 128)  \n",
      " 512 \n",
      "Dense (Dense)  (None, 128)  \n",
      " 10512  \n",
      "dropout (Dropout)  (None, 128)  \n",
      " 0 \n",
      "flatten (Flatten)  (None, 128)  \n",
      " 1024  \n",
      "dense1(Dense)  \n",
      " (None, 12)  \n",
      " 0 \n",
      "dense2(Dense)  \n",
      " (None, 12)  \n",
      " 455 \n",
      " \n",
      " \n",
      "Journal of Information Systems and Telecommunication, Vol.12, No.2,  April -June 2024  \n",
      " \n",
      " \n",
      "159 \n",
      "The BiLSTM model helps to segregate the speech samples \n",
      "into male and female. The segregated speech samples \n",
      "move to their respective male and female folders.  \n",
      "Table 2: Parameters of the DCNN model  \n",
      "Layers  Shape of output  Parameters#  \n",
      "conv1d (Conv1D)  \n",
      " (None, 216, 256)  \n",
      " 2304  \n",
      "Activation  \n",
      "(Activation)  (None, 216, 256)  \n",
      " 0 \n",
      "conv1d 1 (Conv1D)  \n",
      " (None, 216, 256)  524544  \n",
      "batch normalization  \n",
      " (None, 216, 256)  1024 \n",
      "activation 1  \n",
      "(Activation)  \n",
      " (None, 216, 256) 0 \n",
      "dropout (Dropout)  (None, 216, 256)  \n",
      " 0 \n",
      "max pooling1d \n",
      "(MaxPooling1D)  \n",
      " (None, 13, 256)  \n",
      " 0 \n",
      "conv1d 2 (Conv1D)  \n",
      " (None, 13, 128)  \n",
      " 262272  \n",
      " \n",
      "activation 2 \n",
      "(Activation)  \n",
      " (None, 13, 128)  \n",
      " 0 \n",
      "conv1d 3 (Conv1D)  \n",
      " (None, 13, 128)  \n",
      " 131200  \n",
      " \n",
      "activation 3 \n",
      "(Activation)  \n",
      " (None, 13, 128)  \n",
      " 0 \n",
      "conv1d 4 (Conv1D)  \n",
      " (None, 13, 128)  \n",
      " 131200  \n",
      " \n",
      "batch normalization  \n",
      " (None, 13, 128)  \n",
      " 512 \n",
      "activation 4 \n",
      "(Activation)  \n",
      " (None, 13, 128)  \n",
      " 0 \n",
      "conv1d 5 (Conv1D)  \n",
      " (None, 13, 128)  \n",
      " 131200  \n",
      " \n",
      "batch normalization \n",
      "1 \n",
      " (None, 13, 128)  \n",
      " 512 \n",
      "activation 5 \n",
      "(Activation)  \n",
      " (None, 13, 128)  \n",
      " 0 \n",
      "dropout 1 (Dropout)  \n",
      " (None, 13, 128)  \n",
      " 0 \n",
      "max pooling1d 1 \n",
      "(MaxPooling1D)  \n",
      " (None, 1, 128)  \n",
      " 0 \n",
      "conv1d 6 (Conv1D)  \n",
      " (None, 1, 64)  \n",
      " 65600  \n",
      "activation 6 \n",
      "(Activation)  \n",
      " (None, 1, 64)  \n",
      " 0 \n",
      "conv1d 7 (Conv1D)  \n",
      " (None, 1, 64)  \n",
      " 32832  \n",
      "activation 7 \n",
      "(Activation)  \n",
      " (None, 1, 64)  \n",
      " 0 \n",
      "flatten (Flatten)  \n",
      " (None, 64)  \n",
      " 0 dense (Dense)  \n",
      " (None, 14)  \n",
      " 910 \n",
      "activation 8 \n",
      "(Activation)  \n",
      " (None, 14)  \n",
      " 0 \n",
      " \n",
      "After separating the speech samples into genders, Chroma \n",
      "STFT, Spectral centroid, Mel-scale spectrogram, and \n",
      "Spectral Flux feature s extraction happen ed. Then the \n",
      "created data points are dimensionally reduced to fit into \n",
      "the DCNN deep learning model to identify emotions. \n",
      "Metrics like precision, recall, F1 score, and the confusion \n",
      "matrix aid in assessing the model's performance.  Table 2 \n",
      "mentions the parameters of the DCNN model.  \n",
      "6-1- Result Analysis  \n",
      "The fusion of Fundamental frequency and SDC \n",
      "detect s the gender of speech samples. The initial \n",
      "stage involves pre -processing the speech samples, \n",
      "followed by feature extraction, which includes SDC \n",
      "and fundamental frequencies. The combination of \n",
      "SDC and Fundamental speech features create a \n",
      "multifeature fusion resulting in a single set of data \n",
      "points. The next step is to reduce the dimensions of \n",
      "data p oints to use them as input to the BiLSTM \n",
      "Model. The BiLSTM model predicts the data in the \n",
      "dataset as male and female, placed in separate \n",
      "folders. Of these, 3342 speech samples are available, \n",
      "and 3294 are correctly classified. So, the accuracy of \n",
      "the model prediction is 99.59%. The precision, recall, \n",
      "and f1 -score values, along with the confusion matrix, \n",
      "are shown in Fig s 9 and 10. The predicted model \n",
      "identifies the female speech samples more accurately \n",
      "than the male.   \n",
      " \n",
      "Fig. 9 Confusion Matrix for Gender detection by BiLSTM.  \n",
      " \n",
      "The graph in Fig 9 shows the gender detection from the \n",
      "speech sample. The X -axis represents the true label, and \n",
      "the Y-axis shows the predicted label.  This graph gives the \n",
      "\n",
      " \n",
      "Mohanty, & Cherukuri, Whispered Speech Emotion  Recognition with Gender Detection using BiLSTM and DCNN  \n",
      " \n",
      " \n",
      " \n",
      "160 \n",
      "count of detected gender spe ech that the Bi -LSTM model \n",
      "correctly and incorrectly detects.  \n",
      " \n",
      "Fig. 10 Accuracy measures of Gender Detection.  \n",
      " \n",
      "2031 female and 1263 male speech are correctly detected, \n",
      "whereas 13 females and 35 males are incorrectly detected. \n",
      "Pitch values identify the emotions after detecting genders \n",
      "from the speech samples.  \n",
      "The extraction of MFCC, Mel -scale spectrogram, Chroma \n",
      "STFT, Spectral Flux, and Spectral Centroid speech \n",
      "features happens from speech samples. Then, all five \n",
      "speech features are fused into a single feature and \n",
      "dimensionally reduced. Finally, the DCNN model is \n",
      "applied to predict emotions.  \n",
      "Individual emotions are detected based on gender as \n",
      "shown in Fig 11. The deviation in the male speech \n",
      "emotions is less than the female . Female fear and female \n",
      "neutral  emotions show divergent results from other \n",
      "emotions. The accuracy of the model is 98.54%.   \n",
      " \n",
      " \n",
      "Fig.11 Emotions based on gender.  \n",
      "6-2- Comparison Analysis  \n",
      "This experiment evaluates the current implementation \n",
      "using the wTIMIT  dataset, resulting in an impressive \n",
      "accuracy of 98.54%. The performance compares with \n",
      "three -layered Long short -term memory Bidirectional \n",
      "RNNs (LSTM BiRNN) that use No -Attention, Feed \n",
      "Forward Attention (FFA), and Improved Feed Forward \n",
      "Attention Mechanism ( IFFA) to evaluate the emotions in \n",
      "the dataset. This model uses 35 hidden states to train the dataset. The models are trained and validated with the help \n",
      "of the wTIMIT dataset [28] and tested with the help of the \n",
      "CHAINS dataset,  as shown in Table 3. This ex periment is \n",
      "a new hypothesis for  emotion detection with gender \n",
      "identification on whispered speech.  Hence, there are fewer \n",
      "references available for similar investigations.  \n",
      "Table 3: Comparison of the Implementation  \n",
      "  Sl# Model Implementation  Accuracy  \n",
      "1 FFA, IFFA, LSTM, Bi_RNN [2 8] 97.6 %  \n",
      "2 Proposed Model  98.54 %  \n",
      "The proposed method improves the performance of \n",
      "emotions after segregating the speech samples into genders.  \n",
      "By separating the approach into two models, this \n",
      "implementation excels in capturing natural and \n",
      "spontaneous expressions of emotions with low latency \n",
      "manner .  \n",
      "7- Conclusion  and Future Work  \n",
      "Identification of gender from whispered speech and \n",
      "recognizing emotions is a complicated task. This \n",
      "implementation initially detects the gender from the \n",
      "speech samples before identifying the emotions. Emotions \n",
      "might vary based on gender in the same situation. Speech \n",
      "features such as Fu ndamental Frequency and SDC help \n",
      "with gender identification. MFCC, Mel -scale spectrogram, \n",
      "Spectral flux, Spectral Centroid, and Chroma STFT play a \n",
      "vital role in detecting emotions. Multifeature fusion helps \n",
      "to combine speech features into a single set of d ata points . \n",
      "The concept verifies with the publicly available dataset \n",
      "wTIMIT with an accuracy of 98.54%. This approach helps \n",
      "to identify nearly inaudible emotions and is used to figure \n",
      "out the strategy. The proposed methodology can be \n",
      "improved using other s peech features, machine learning, \n",
      "and deep learning concepts.  \n",
      " \n",
      "References  \n",
      "[1]  ST Jovicic , and Z  Saric , ―Acoustic analysis of  consonants in \n",
      "whispered speech,‖ Journal of voice , vol 22, no. 3, pp. 263–\n",
      "274, 2008 . \n",
      "[2]  M Kumari , and I  Ali, ―An efficient algorithm for gender \n",
      "detection using voice samples,‖ 2015 Communication, \n",
      "Control and Intelligent Systems (CCIS),  2015 , Mathura, \n",
      "Utter Pradesh,  pp. 221–226, doi: \n",
      "10.1109/CCIntelS.20 15.7437912 . \n",
      "[3] S Motamed , S Setayeshi, A Rabiee , and A Sharifi, ―Speech \n",
      "Emotion Recognition Based on Fusion Method,‖ Journal of \n",
      "Information Systems and Telecommunication (JIST), vol. 3, \n",
      "pp. 50--56, 2017, doi: 10.7508/jist.2017.17.007 .  \n",
      "[4] JS Li, CC Huang , ST Sheu , and MW  Lin, ―Speech emotion \n",
      "recognition and its applications,‖ Proc. of Taiwan Institute of \n",
      "Kansei Conference , 2010  Paris , France , pp. 187–192.  \n",
      "\n",
      " \n",
      "Journal of Information Systems and Telecommunication, Vol.12, No.2,  April -June 2024  \n",
      " \n",
      " \n",
      "161 \n",
      "[5] A Guerrieri , E Braccili , F Sgro , and GN Meldolesi  ―Gender \n",
      "identification in a two -level hierarchical speech emotion \n",
      "recognition system for an Italian Social Robot,‖ Sensors, vol. \n",
      "22, no. 5, pp. 1714, 2022 , doi: 10.3390/s22051714 . \n",
      "[6] M Sarria -Paja, TH Falk, and D  O’Shaughnessy , ―Whispered \n",
      "speaker verification and gender detection using weighted \n",
      "instantaneous frequencies,‖ 2013 IEEE International \n",
      "Conference on Acoustics, Speech and Signal Processing , \n",
      "2013  (May 26 -31), Vancouver , Canada, pp. 7209 –7213 , doi: \n",
      "10.1109/ICASSP.2013.6639062 .  \n",
      "[7] J Deng , S Fruhholz, Z Zhang , and Bojrn Schuller , \n",
      "―Recognizing emotions from whispered speech based on \n",
      "acoustic feature transfer learning,‖ IEEE Access , vol.  5, pp. \n",
      "5235 –5246 , 2017 .  \n",
      "[8] M Cotescu , T Drugman, G Huybrechts, J Lorenzo -Trueba, \n",
      "and A Moinet , ―Voice conversion for whispered speech \n",
      "synthesis,‖  IEEE Signal Processing Letters , vol. 27, pp. 186–\n",
      "190, 2019 , doi: 10.1109/LSP.2019.2961213 .  \n",
      "[9] P Mishra , and R Sharma, ―Gender differentiated \n",
      "convolutional neural networks for speech emotion \n",
      "recognition,‖ 2020 12th International Congress on Ultra -\n",
      "Modern Telecommuni - cations and Control Systems and \n",
      "Workshops (ICUMT),  2020  (October 5 -7), Brno , Czech \n",
      "Republic,  pp. 142–148, doi: \n",
      "10.1109/ICUMT51630.2020.9222412 . \n",
      "[10] Mustaqeem S Kwon, ―Optimal feature selection based \n",
      "speech emotion recognition using two -stream deep \n",
      "convolutional neural network,‖ International Journal of \n",
      "Intelligent Systems , vol. 36, no. 9, pp. 5116 – 5135 , 2021 , \n",
      "doi:  10.1002/int.22505 . \n",
      "[11] J. Ancilin and A. Milton, ―Improved speech emotion \n",
      "recognition with mel frequency magnitude coefficient ,‖ \n",
      "Applied Acoustics , vol. 179, pp. 108046 , 2021 , doi: \n",
      "10.1016/j.apacoust.2021.108046 .  \n",
      "[12] S. Jothimani and K. Premalatha, ―Mff -saug: Multi feature \n",
      "fusion with spectrogram augmentatio n of speech emotion \n",
      "recognition using convolution neural network ,‖ Chaos, \n",
      "Solitons & Fractals , vol. 162, pp. 112512 , 2022 , doi: \n",
      "10.1016/j.chaos.2022.112512 .  \n",
      "[13] B Yalamanchili, SK Samayamantula, and KR Anne, ―Neural \n",
      "network -based blended ensemble learning for speech \n",
      "emotion recognition,‖ Multidimensional Systems and Signal \n",
      "Processing, vol. 33, no. 4, pp. 1323 --1348 , 2022 , doi: \n",
      "10.1007/s11045 -022-00845 -9.  \n",
      "[14] T Feng , R Hebbar , and S Narayanan, ―Trustser: On the \n",
      "trustworthiness of fine -tuning pre -trained speech embeddings \n",
      "for speech emotion recognition,‖ arXiv preprint \n",
      "arXiv:2305.11229, 2023 , doi:  10.48550/arXiv.2305.11229  \n",
      "[15] RV Darekar and M Chavan , S Sharanyaa,  and NR Ranjan , \n",
      "―A hybrid meta -heuristic ensemble based classification \n",
      "technique speech emotion recognition,‖ Advances in \n",
      "Engineering Software, vol. 180, pp. 103412 , 2023 . \n",
      "[16] J Rekimot o, ―Dualvoice: A speech interaction method using \n",
      "whisper -voice as commands,‖ CHI Conference on Human \n",
      "Factors in Computing Systems Extended Abstracts,  pp. 1–6, \n",
      "2022 , doi: 10.1145/3491101.3519700 . \n",
      "[17]H Dolka,  AX VM , and S Juliet,―  Speech  emotion r ecognition  \n",
      "using  ANN on MFCC  features,‖ 2021 3rd international \n",
      "conference on signal processing and communication (ICPSC) , \n",
      "2021, Coimbatore, India, pp. 431–435, doi: \n",
      "10.1109/ICSPC51351.2021.9451810 . [18] MK Reddy and KS Rao, ―Robust pitch extraction method \n",
      "for the hmm -based speech synthesis system,‖ IEEE signal \n",
      "processing letters , vol.  24, no. 8, pp. 1133 –1137 , 2017 , doi: \n",
      "10.1109/LSP.2017. 2712646 .  \n",
      "[19] J Chatterjee, V Mukesh, HH Hsu , G Vyas , and Z Liu , \n",
      "―Speech emotion recognition using cross - correlation and \n",
      "acoustic features,‖ 2018 IEEE 16th Intl Conf on Dependable, \n",
      "Autonomic and Secure Computing, 16th Intl Conf on \n",
      "Pervasive Intelligence and Computing, 4th Intl Conf on Big \n",
      "Data Intelligence and Computing and Cyber Science and \n",
      "Technology Congress , 2018  (August 12-15), Athens, Greece , \n",
      "pp. 243–249, doi: 10.1109/DASC/PiCom/DataCom/CyberSci  \n",
      " Tec.2018.00050 .  \n",
      "[20] S Rajesh and NJ Nalini, ―Musical instrument emotion \n",
      "recognition using deep recurrent neural network,‖ Procedia \n",
      "Computer Science, vol. 167, pp. 16 --25, 2020, doi: \n",
      "10.1016/j.procs.2020.03.178 . \n",
      "[21] M Aly , and NS Alotaibi, ―A novel deep learning model to \n",
      "detect covid -19 based on wavelet features extracted from \n",
      "mel- scale spectrogram of patients’ cough and breathing \n",
      "sounds,‖ Informatics in Medicine Unlocked , vol.  32, pp.  \n",
      "101049 , 2022, doi: 10.1016/j.imu.2022.101049 . \n",
      "[22] Z Qawaqneh,  AA Mallouh,  and BD Barkana, ― Age and \n",
      "gender  classification  from  speech and face images by jointly \n",
      "fine-tuned deep neural networks,‖ Expert Systems with \n",
      "Applications , vol. 85, pp. 76–86, 2017 , doi: \n",
      "10.1016/j.eswa.2017.05.037 .  \n",
      "[23] A Koduru, HB Valiveti and AK Budati, ―Feature extraction \n",
      "algorithms to improve the speech emotion recognition rate,‖ \n",
      "International Journal of Speech Technology , vol. 23, no. 1, \n",
      "pp. 45–55, 2020 , doi: 10.1007/s10772 -020-09672 -4.  \n",
      "[24] S Zhang , and C Li, ―Research on feature fusion speech \n",
      "emotion recognition technology for smart teaching,‖ Mobile \n",
      "Information Systems , vol. 2022 , 2022 , doi:  \n",
      "10.1155/2022/7785929 . \n",
      "[25] GB, Prasanna, SV Bhat, C Naik , and HN Champa, ―An \n",
      "Efficient Method for Handwritten Kannada Digit \n",
      "Recognition based on PCA and SVM Classifier,‖ Journal of \n",
      "Information Systems and Telecommunication (JIST), vol. 3, \n",
      "no. 35, pp. 169 2021, doi: 20.1001.1.23221437.2021.9.35.3.2 . \n",
      "[26] A Graves, N Jaitly, and A Mohamed, ―Hybrid speech \n",
      "recognition wit h deep bidirectional lstm,‖ 2013 IEEE \n",
      "workshop on automatic speech recognition and \n",
      "understanding , 2013, Olomouc, Czech Republic, pp.  273–\n",
      "278, doi: 10.1109/ASRU.2013.6707742 .  \n",
      "[27] N Aloysius and M Geetha, ―A review on deep convolutional \n",
      "neural networks,‖ 2017 international conference on \n",
      "communication and signal processing (ICCSP), 2017, pp. \n",
      "0588 –0592 , IEEE, doi: 10.1109/ICCSP.2017.8286426 .  \n",
      "[28] SBC Gutha, MAB Shaik, T Udayakumar , and AA \n",
      "Saunshikhar , ―Improved feed forward attention mechanism in \n",
      "bidirectional recurrent neural networks for robust sequence \n",
      "classification,‖ 2020 International Conference on Signal \n",
      "Processing and Communications (SPCOM) , 2020 , IISc, \n",
      "Bangalore.  IEEE , pp. 1—5, doi: \n",
      "0.9179606 10.1109/SPCOM50965.202  \n",
      "\n",
      "\n",
      "Reading: d:\\PythonCode\\document_portal\\notebook\\data\\ijcsit2012030339.pdf\n",
      " \n",
      " \n",
      " \n",
      "Journal of Information Systems and Telecommunication  \n",
      "Vol.12, No.2,  April -June 2024, 152-161 \n",
      " \n",
      "http://jist.acecr.org  \n",
      "ISSN 2322 -1437  / EISSN:2345 -2773  \n",
      " \n",
      "Whispered Speech Emotion Recognition with Gender Detection using \n",
      "BiLSTM and DCNN  \n",
      "Aniruddha Mohanty1*, Ravindranath C Cherukuri1 \n",
      " \n",
      "1.Department of Computer  science and Engineering, Christ (Deemed to be University), Bangalore, India  \n",
      " \n",
      " \n",
      "Received: 22 Aug 2023 / Revised: 04 Apr 2024 / Accepted: 21 May 2024  \n",
      " \n",
      " \n",
      "Abstract   \n",
      "Emotions are human mental states at a particular instance in time concerning one’s circumstances, mood, and relationships \n",
      "with others . Identifying emotions from the whispered speech is complicated as the conversation might be confidential. The \n",
      "representation of the speech relies on the magnitude of its information. Whispered speech is intelligible, a low -intensi ty \n",
      "signal, and varies from normal speech. Emotion identification is quite tricky from whispered speech. Both prosodic and \n",
      "spectral speech features help to identify emotions. The emotion identification in a whispered speech happens using prosodic \n",
      "speech fea tures such as zero -crossing rate  (ZCR) , pitch, and spectral features that include spectral centroid, chroma STFT, \n",
      "Mel scale spectrogram, Mel -frequency cepstral coefficient (MFCC), Shifted Delta Cepstrum (SDC), and Spectral Flux. \n",
      "There are two parts to the proposed implementation. Bidirectional Long  Short -Term Memory (BiLSTM) helps to identify \n",
      "the gender from the speech sample in the first step with SDC and pitch. The Deep Convolutional Neural Network (DCNN) \n",
      "model helps to identify the emotions in the second  step. This implementation is evaluated using the  wTIMIT data corpus \n",
      "and gives 98.54% accuracy. Emotions have a dynamic effect on genders, so this implementation performs better than \n",
      "traditional approaches. This approach helps to design online learning management systems, different applications for \n",
      "mobile devices, checking cyber -criminal activities, emotion detection for older people, automatic speaker identification and \n",
      "authentication, forensics, and surveillance . \n",
      " \n",
      "Keywords:  Whispered Speech; Emotion Re cognition; Speech Features; Data Corpus; BiLSTM;  DCNN . \n",
      " \n",
      "1- Introduction  \n",
      "Emotions are the humans’ short -lived feeling s that affects \n",
      "thinking, actions, relationships, and social interactions. \n",
      "Emotions express humans’ physiological and emotional \n",
      "states with facial expressions and body language. \n",
      "Whispered speech is a form of speec h that expresses \n",
      "emotions. Whispered speech is a type of communication \n",
      "produced with breath without any noise and excitation of \n",
      "voice. The whispered speech structure changes \n",
      "significantly because of the lack of periodic excitation in \n",
      "the voice. This result s in missing speech and reduced \n",
      "transparency in communication.  \n",
      "The difference between normal and whispered speech is \n",
      "the absence of vocal tract vibrations due to the vocal \n",
      "tract’s physiological blocking. The strength of whispered \n",
      "speech is minute and with out voice compared to phoned \n",
      "speech. The spectral and prosodic features help to detect \n",
      "the whispered speech. Prosodic features of speech vary \n",
      "over time. Spectral features of whispered speech are highly accurate over unvoiced consonants, voiced consonants, \n",
      "and formants in vowels [1]. \n",
      "The impacts on Speech Emotion Recognition ( SER) are \n",
      "due to various acoustic conditions such as compressed, \n",
      "noisy, telephonic conversation s and imitator speeches. \n",
      "Other environmental conditions,  such as stress, rhythm, \n",
      "and intonation, can affect  SER.  Whispered speech is also \n",
      "affected by similar acoustic and environmental conditions. \n",
      "SER depends on acoustic characteristics and gender in \n",
      "some scenarios. Hence gender plays a vital role in SER.  \n",
      "Nowadays, several whispered speech  samples of male and \n",
      "female voices are available. The effectiveness of the SER \n",
      "is more when the implementation is in two parts. The \n",
      "identification of the gender [2] happens initially using \n",
      "Bidirectional Long Short -term Memory (BiLSTM)  with \n",
      "speech features . The next step is detecting emotions using \n",
      "various speech features [3] and Deep Convolutional \n",
      "Neural Network (DCNN).   \n",
      "The structure of the paper includes various sections. \n",
      "Section 2 describes Human Emotions and their \n",
      "Applications. Details of the Related Work are in Section 3. \n",
      "Section 4 describes the System Model, which is the black \n",
      "box view of the implementation. Section 5 is the Model \n",
      " \n",
      "Journal of Information Systems and Telecommunication, Vol.12, No.2,  April -June 2024  \n",
      " \n",
      " \n",
      "153 \n",
      "Design that describes the details of the speech features and \n",
      "the deep learning models used in this implementation. \n",
      "Section 6 gives the Experiment and Result Analysis. \n",
      "Section 7 brief ly discuss  the Conclusion and Future Work.   \n",
      "2- Human  Emotions  and their  Applications  \n",
      "Human emotions are the mental state caused by countless \n",
      "associated views , feeling s, behavioral replies, and the \n",
      "degree of pressure and annoyance. It is often associated \n",
      "with attitude , temperament, behavior , disposition, and \n",
      "creativi ty [4]. Emotion recognition helps to detect a \n",
      "humans ’ emotional mood , which lasts hours and days. \n",
      "Speech conveys emotions such as anger, disgust, fear, \n",
      "happiness, neutrality, sadness, and surprise. The machine \n",
      "automatically detects various emotions from speech with \n",
      "the help of different algorithms. Emotion Recognition \n",
      "helps to : \n",
      " \n",
      "•  Detect customers ’ intentions based on the teleconference. \n",
      "•  Detect cybercrime.  \n",
      "•  Students ’ attention and teachers ’ content adjustment.  \n",
      "•  Disability assistance  \n",
      "•  Customer satisfaction  \n",
      "•  Stress monitoring  \n",
      "•  Social media analysis  \n",
      "•  Suspicious activity  \n",
      "•  Human -machine interaction and so on.   \n",
      "3- Related Work  \n",
      "Over time, numerous studies have detected emotions from \n",
      "whispered speech . The vario us deep learning models \n",
      "detect emotions based on the speech features extracted \n",
      "from the collected whispered speech data corpus . The SER \n",
      "for normal and whispered speech is diverse because of \n",
      "vocal excitation. Emotions vary with many factors; gender \n",
      "is among  the most influential factors  [5]. Identifying \n",
      "gender in the first step improves emotion detection from \n",
      "whispered speech. So, the related work explored is on \n",
      "gender detection and emotion recognition from whispered \n",
      "speech.  \n",
      "MFCC obtained by the Hilbert enve lope approach and \n",
      "weighted instantaneous frequencies (WIFs) obtained by \n",
      "the coherent demodulation help to detect gender in \n",
      "whispered speech  samples [6]. There is an opportunity to \n",
      "explore gender detection in noisy speech conditions using \n",
      "these approaches.  \n",
      "Autoencoder -enabled features  in the transfer learning \n",
      "framework propose to practice  phonated data to identify \n",
      "emotions from Whispered speech [7]. The feature \n",
      "extraction is from the Geneva Whispered Emotion Corpus \n",
      "(GeWEC) and Berlin Emotional Speech Databa se (EMO -\n",
      "DB) data corpus. The acoustic features such as Mel-frequency cepstral coefficient (MFCC), root mean square \n",
      "(RMS), frame energy, zero -crossing -rate (ZCR), pitch \n",
      "frequency (F0), probability of voicing autocorrelation \n",
      "function are the inputs to evaluate the framework. \n",
      "Implementing deep learning concepts on spontaneous data \n",
      "gives more accuracy than the current framework .  \n",
      "Gender is detected to target an anonymous speaker. The \n",
      "Deep Neural Network (DNN) model is used to generalize \n",
      "gender by using the MFCC speech feature. This \n",
      "implementation is applied to the wTIMIT dataset to verify \n",
      "the gender and recognize the speaker  [8]. The DNN model \n",
      "cannot generalize gender; evaluation can happen with \n",
      "other datasets.   \n",
      "MFCC and CNN, with the fully  connected network, detect \n",
      "gender and emotions like anger, disgust, fear, happiness, \n",
      "sadness, surprise, and a neutral state  [9]. The concept \n",
      "verification happens on RAVDESS, CREMA -D, SAVEE, \n",
      "and TESS datasets, having an accuracy of 92.283%, which \n",
      "is better than the tr aditional model.  \n",
      "The final prediction of the implemented model is to learn \n",
      "the mutually spatial -spectral features happens by a two -\n",
      "stream deep convolutional neural network with an iterative \n",
      "neighborhood  component analysis (INCA) and the most \n",
      "discriminatory optimal features  [10]. The concept \n",
      "verification happens on EMO -DB, SAVEE, and \n",
      "RAVDESS emotional speech corpora which perform with \n",
      "95%, 82%, and 85% accuracy rates. Real -time applications \n",
      "with natural and huge data corpora can help to extend this \n",
      "concept to identify emotions.  \n",
      "Mel Frequency Magnitude Coefficient (MFMC) and three \n",
      "spectral features, namely MFCC, log frequency power \n",
      "coefficient, and linear prediction cepstral coefficient, are \n",
      "used with the help of Support Vector Machine (SVM) \n",
      "modeling  [11]. The performance evaluation uses this \n",
      "concept  on Berlin, RAVDESS, SAVEE, EMOVO, and \n",
      "eNTERFACE data corpus . Feature selection, feature \n",
      "fusion, and multiple classification schemes improve the \n",
      "performance of MFMC.  \n",
      "The proposed MFF -SAug research in  which noise removal \n",
      "improves emotion. White Noise Injection, Pitch Tuning \n",
      "techniques, MFCC, ZCR, RMS speech features, and \n",
      "Convolutional Neural Network (CNN) modeling [12] \n",
      "detect emotions. Emotion detection during the interaction \n",
      "between people can extend the MFF -SAug approach .  \n",
      "The proposed Neural Network -based Blended Ensemble \n",
      "Learning (NNBEL) model is composed of a 1 -dimensional \n",
      "Convolution Neural Network (1D -CNN), Long Short -\n",
      "Term Memory (LSTM), and CapsuleNets. LSTM receives \n",
      "input from the Log Mel -spect rogram speech features, \n",
      "while 1D -CNN and CapsuleNets receive input from the \n",
      "MFCC.  Each model’s output is fed to Multi -Layer \n",
      "Perceptron (MLP) and predicts the final emotion s [13]. \n",
      "This model shows 95.3% and 94% accuracy on \n",
      "RAVDESS and IEMOCAP datasets, respectively.  \n",
      " \n",
      "Mohanty, & Cherukuri, Whispered Speech Emotion  Recognition with Gender Detection using BiLSTM and DCNN  \n",
      " \n",
      " \n",
      " \n",
      "154 \n",
      "TrustSER [14] implemented a general framework to \n",
      "determine the SER system's trustworthiness using deep \n",
      "learning techniques. Trustworthiness evaluates privacy \n",
      "(gender in formation, speaker demographics), safety, \n",
      "fairness, sustainability, and emotions (sad, angry, happy, \n",
      "neutral). The architecture of the TrustSET framework uses \n",
      "CNN encoder and Transformer encoder models. \n",
      "Trustworthy profiles under the Federated Learning \n",
      "scenario might improve privacy, fairness, and safety.  \n",
      "A hybrid meta -heuristic ensemble -based classification [15] \n",
      "helps to detect speech emotions. Raw speech samples are \n",
      "filtered using the Butterworth filter; then, spectrogram \n",
      "speech features are extracted from each frame to create a \n",
      "hybrid feature vector. The ensemble -based classification is \n",
      "applied to hybrid feature vectors to classify em otions. The \n",
      "ensemble -based classification contains a Recurrent Neural \n",
      "Network (RNN), a Deep Belief Network (DBN), and an \n",
      "Artificial Neural Network (ANN).  \n",
      "The above -related work shows that it is crucial to identify \n",
      "gender to segregate emotions in a speech. Male and female \n",
      "emotions are different based on situations. Whispered \n",
      "speech is an essential concept of emotion recognition. \n",
      "There is an opportunity to experiment further based on \n",
      "other datasets of whispered speech. So, this is a motivation \n",
      "to work on emot ion recognition in whispered speech, as \n",
      "this is a less explored area of research .  \n",
      "4- System Model  \n",
      "The analysis of e motion recognition from the whispered \n",
      "speech segregates into two parts. One is gender detection, \n",
      "and the second is emotion detection as shown in Fig 1.  \n",
      " \n",
      " \n",
      "Fig. 1 Emotion recognition from whispered speech  \n",
      " \n",
      "As part of gender detection, pre -processing of the speech \n",
      "samples is performed, and then pitch and SDC are \n",
      "extracted as part of prosodic and spectral features, \n",
      "respectively. Both the features are fused to get a single \n",
      "feature set with the help of multifeature fusion. PCA helps \n",
      "to reduce the dimensions of extracted features and is used \n",
      "as input to Bidirectional Long short -term memory \n",
      "(BiLSTM) to classify genders , as shown in Fig 2.  \n",
      " \n",
      " \n",
      "Fig .2 Gender detection from whispered speech  \n",
      " \n",
      "Following  Gender detection, the augmentation of speech \n",
      "data helps to get more realistic data. Then the speech features such as Chromatogram, Zero -Crossing Rate, \n",
      "Spectral Central, MFCC, Spectral Flux , and Mel \n",
      "Spectrog rams are extracted. All the extracted features \n",
      "combine to get a single feature using multifeature fusion. \n",
      "Then the dimensions are reduced to get the optimal data \n",
      "points. The data points are inserted into the Deep \n",
      "Convolutional Neural Network (DCNN) to identify the \n",
      "emotions , as shown in Fig 3.  \n",
      "5- System  Design  \n",
      "The implementation of system design happens in two parts. \n",
      "The first part identifies the male and female gender from \n",
      "the whispered speech samples. Then the determination of \n",
      "different emotions like sadn ess, happiness, fear, anger, \n",
      "surprise, disgust, and neutral are detected in gender -\n",
      "segregated speech samples.  \n",
      " \n",
      "Fig. 3 Emotion Recognition from Male and Female whispered speech.  \n",
      "5-1- Whispered Speech Data Corpus  \n",
      "wTIMIT [16] is a whispered voice data corpus ha ving 450 \n",
      "phonetically balanced sentences with 29 speakers. There \n",
      "are 11,324 utterances with a normal voice , which  can be \n",
      "used for normal and whispered training. This data is \n",
      "available in two parts - train and test divisions. The \n",
      "samples contain an 8 kHz sampling rate with a high -\n",
      "quality  and low-pass filter.  \n",
      "5-2-    Data Augmentation  \n",
      "Data augmentation [17 ] is the method of getting more \n",
      "realistic data from the existing data, which helps to add \n",
      "more training data to the model, reduce overfitting, and \n",
      "increase the model’s generalization ability. Below are the \n",
      "steps to generate synthetic data.  \n",
      " Shifting Time : Shifting time is a simple concept \n",
      "where the audio shifts to the left or right with a \n",
      "haphazard second. If the audio shifts to the left \n",
      "\n",
      " \n",
      "Journal of Information Systems and Telecommunication, Vol.12, No.2,  April -June 2024  \n",
      " \n",
      " \n",
      "155 \n",
      "with z seconds, then the first z seconds are \n",
      "marked as silence. Similarly, if the audio shifts to \n",
      "the right with z second s, the last z seconds are \n",
      "marked as silence.  \n",
      " Changing Pitch: Pitch change is adjusting the \n",
      "pitch randomly without upsetting the speediness \n",
      "of the audio file.  \n",
      " Changing Speed: Speed audio changes by \n",
      "stretching the time series data by a fixed rate.  \n",
      "5-3- Pre-processing  \n",
      "After the data collection and data augmentation, pre -\n",
      "processing [3] is the initial  phase  in training SER. \n",
      "Framing, Windowing, Voice activity detection, \n",
      "Normalization, and Noise reduction are pre -\n",
      "processing steps.   \n",
      "Framing:  Speech signals are quasip eriodic and vary \n",
      "over time. Hence, information and related emotions \n",
      "also fluctuate over time. The segregation of the \n",
      "signals happens in a shorter period to make the \n",
      "speech signal invariant. Twenty milliseconds to thirty \n",
      "milliseconds helps to make the speec h signal \n",
      "invariant, and five milliseconds of overlap of the \n",
      "frames avoid data leakage between the frames .  \n",
      "Windowing:  Windowing on each frame helps to diminish \n",
      "data leakage during Fast Fourier Transformation (FFT) \n",
      "after framing. The Hamming window allows  this step \n",
      "where the window size is N for the frames     , used in \n",
      "equation (1).  \n",
      " \n",
      "                  (   \n",
      "   )       (1) \n",
      "            \n",
      " \n",
      "Voice activity detection (VAD):  An utterance has three \n",
      "portions of speech activities : voiced, unvoiced, and silent. \n",
      "Zero Crossing Rate (ZCR) speech feature helps to detect \n",
      "VAD. ZCR represents the frequency of signal transitions \n",
      "between positive and negative values within a specific \n",
      "frame . Due to high energy, the ZCR value is low for \n",
      "voiced speech and high for unvoiced s peech because of \n",
      "low energy.  \n",
      "Normalization:  Normalization helps to reduce  speaker \n",
      "and recording inconsistency without affecting the features’ \n",
      "strength and enhancing the features’ generalization \n",
      "capability. The Z-normalization method is used more and \n",
      "represented as  \n",
      "       \n",
      "      (2) \n",
      "Where   is the speech signal,   and   are the mean and \n",
      "standard deviation of the data, respectively.  \n",
      "Noise Reduction:  The environmental noise captured while \n",
      "recording a speech signal  affects the recognition rate . \n",
      "Minimum mean square error (MMSE) and log -spectral \n",
      "amplitude MMSE (LogMMSE) reduce the noise from the speech signals. Noise reduction helps to get more accuracy \n",
      "in SER evaluation.  \n",
      "5-4- Feature Extraction  \n",
      "Specific emotions are present in prosodic and spectral \n",
      "features of speech. The extraction of prosodic and spectral \n",
      "features is a vital characteristic of emotion recognition \n",
      "after pre -processing the speech signals.  \n",
      "5-4-1-Prosodic Features  \n",
      "Prosodic features deal with the audio qualities of a speech \n",
      "when connected speeches use sounds as input. The \n",
      "production of the speech deals with the amount of energy, \n",
      "frequency, period, loudness, pitch, and duration. Speech \n",
      "signal com munication depends on intonation, stress, and \n",
      "rhythm, which prosodic features can detect. The prosodic \n",
      "features used in the implementation are : \n",
      "Zero -Crossing Rate (ZCR):  ZCR [17] measures the \n",
      "frequency of signal transitions between positive and \n",
      "negative values in every audio frame  and defined as  \n",
      "    \n",
      "    ∑       [ ]         [      \n",
      "    (3) \n",
      "Where        is the sign function . \n",
      "    [     ]  {        \n",
      "       \n",
      "Fundamental Frequency (Pitch):  Fundamental \n",
      "Frequency (F0) [18] is the minimum freque ncy of the \n",
      "periodic waveform. F0 is the significant parameter to \n",
      "differentiate male speech from female speech. Pitch also \n",
      "determines the voiced and unvoiced portion of the speech \n",
      "signal. This analysis uses pitch parameters like pitch mean \n",
      "value and pitch r ange.  \n",
      "The pitch range determines the number of octaves a speech  \n",
      "sample  can cover, from the lowest to the highest. F0 value \n",
      "varies from 85Hz to 180 HZ for the voiced speech of adult \n",
      "males.  \n",
      " \n",
      " \n",
      "Fig. 4 Representation of Pitch data  \n",
      " \n",
      "Similarly, the F0 value for adult females goes from 165Hz \n",
      "to 255Hz, as shown in Fig 4.  \n",
      "\n",
      " \n",
      "Mohanty, & Cherukuri, Whispered Speech Emotion  Recognition with Gender Detection using BiLSTM and DCNN  \n",
      " \n",
      " \n",
      " \n",
      "156 \n",
      "5-4-2-Spectral features  \n",
      "The representation of spectral features happens by \n",
      "converting the time -domain speech signal into the \n",
      "frequency domain with the help of the Fast Fourier \n",
      "Transform (FFT) which  benefits  to represent the \n",
      "characteristics of the Human Vocal Tract. The  spectral \n",
      "features are  \n",
      "Spectral Centroid (SC):  The Spectral Centroid [19] is the \n",
      "most used speech feature where the positioning of the \n",
      "―center of mass‖ of the audio signal spectr um defines the \n",
      "weight of the spectrum. The center of mass measures the \n",
      "weighted average of the frequency component located in \n",
      "the audio signal, defined as  \n",
      " \n",
      "      ∑            \n",
      "   \n",
      "∑       \n",
      "       (4) \n",
      "Where      represents the magnitude of bin numb er   and \n",
      "      represents the central frequency of the bin.  \n",
      "Chroma STFT:  Chroma STFT [20] is obtained using FFT \n",
      "on speech samples and the resulting spectrums a re a \n",
      "chromatogram in a vertical axis. This feature captures the \n",
      "harmonic feature of the speech s ignals.  \n",
      "Mel-scale Spectrogram:  Mel-scale Spectrogram [ 21] is a \n",
      "spectrogram in which frequencies convert to the Mel scale, \n",
      "which helps to differentiate the range of frequencies. Mel -\n",
      "spectrogram helps to understand emotions in a better way \n",
      "as humans can perceive sound on a logarithmic scale.  \n",
      "Mel Frequency Cepstral Coefficient (MFCC ): MFCC \n",
      "[17] i s SER’s  standard feature extraction technique. The \n",
      "vocal cord s, tongue, and teeth filter the sound and make it \n",
      "unique for each speaker in the Human Speech production \n",
      "system. Mel scale represents MFCC, where the frequency \n",
      "bands are equally spaced and close to the Human Auditory \n",
      "System’s response, as shown in Fig 5.  \n",
      " \n",
      "Fig 5 MFCC speech feature extraction  \n",
      " \n",
      "Shifted Delta Cepstrum (SDC):  Time derivatives apply \n",
      "to the cepstral coefficients obtained from the MFCC and \n",
      "combined with the delta coefficient to get SDC, as shown \n",
      "in Fig 6.       and   are the four parameters used in \n",
      "SDC [ 22].  M denotes the cepstral coefficient for each \n",
      "frame.   indicates added frames.   are the frames that \n",
      "append delta features from the new feature vector .   \n",
      "represents the delta values difference. So, the coefficient \n",
      "vectors represent as   \n",
      "     [                  ]   (5) \n",
      "   are MFCC coefficients and   is the coefficient index. \n",
      "For a given time,   an intermediate calculation is done to \n",
      "obtain the   coefficients.  \n",
      "                                 (6) \n",
      "Finally, the SDC coefficients vectors of   dimensions are \n",
      "obtained as  \n",
      "       [          (                )] (7) \n",
      " \n",
      " \n",
      "Fig. 6 Extraction of SDC speech feature  \n",
      " \n",
      "This speech feature helps to identify genders for a long -\n",
      "range dynamic appearance in speech signals.  \n",
      "Spectral Flux:  Spectral flux [ 20] measures the change in \n",
      "speed of the signal’s power spectrum compared to the \n",
      "previous frame. This feature estimates the speech signal’s \n",
      "power spectrum about  the power spectrum of one frame \n",
      "with others.  \n",
      "5-5- Feature Selection and Dimension Reduction  \n",
      "Feature s election [23] is selecting features from a large set \n",
      "of extracted features to eliminate redundant and unused \n",
      "information and decrease processing time. For this work, \n",
      "the extracted pitch contains lots of information. The global \n",
      "features, like the range of p itch values and mean pitch \n",
      "values, are selected. Removing redundant and unused \n",
      "information, such as additional zeroes, duplicate values, or \n",
      "frames, is part of MFCC feature extraction. The exact \n",
      "process removes the silent speech frames.  \n",
      "Fundamental frequen cy and SDC are two speech features \n",
      "to identify gender. Similarly, Spectral centroid, Chroma \n",
      "STFT, MFCC, and Spectral flux features are used for \n",
      "emotion identification to create a single set of data points \n",
      "from multiple speech features using multifeature fu sion \n",
      "technique [24]. \n",
      "Multifeature Fusion:  The extracted speech features hav e \n",
      "different dimensions, so feature proximity usages  average \n",
      "dimensional spacing between the vectors to denote the \n",
      "proximity between the diverse features  [24]. The average \n",
      "dimensional spacing between the vectors is computed as  \n",
      "          \n",
      "    √∑∑              (8) \n",
      "   =  \n",
      "     √∑∑              (9) \n",
      "\n",
      " \n",
      "Journal of Information Systems and Telecommunication, Vol.12, No.2,  April -June 2024  \n",
      " \n",
      " \n",
      "157 \n",
      "        and     are the mean and variance interval of \n",
      "average dimensions of the feature vector s.    and    are \n",
      "the mean value of   and   type of sound features.     and    \n",
      "are the mean value of   and   type of sound features . Then \n",
      "the subsequent dimensionality reduction [25] method \n",
      "follows once the feature selection process is complete. \n",
      "High data variance is present in the extracted features \n",
      "containing more information. Dimension reduction \n",
      "techniques reduce the dimensions from extracted feature \n",
      "vectors.  \n",
      " \n",
      "5-5-1- Principal Component Analysis  (PCA)  \n",
      "PCA [25] is an approach for redu cing the dimensionality \n",
      "of extensive datasets, transforming them into more \n",
      "compact representations while retaining crucial \n",
      "information . These reductions in the number of variables \n",
      "help to get more accurate results. A reduced dataset makes \n",
      "it easier and fas ter to visualize and analyze the data. \n",
      "Following steps followed to explore PCA .  \n",
      " Standardization  creates a normalized dataset \n",
      "when there is a significant difference in the range \n",
      "of initial variables or a larger range of datasets \n",
      "dominates the smaller range. It can be done by  \n",
      "             \n",
      "                     (10) \n",
      " Covariance matrix computation  helps  to know  \n",
      "how the input  dataset  varies  from  the mean value \n",
      "to each data point.  \n",
      " Eigenvectors and eigenvalues  of the covariance \n",
      "matrix help to identify the principal component.  \n",
      "5-6- Modelling  \n",
      "This implementation uses two deep l earning models. \n",
      "Bidirectional Long Short -Term Memory (BiLSTM) \n",
      "classifies the genders in whispered speech, and Deep \n",
      "Convolutional Neural Network (DCNN) identifies \n",
      "emotions.  \n",
      "BiLSTM  [26] combines  two Recurrent Neural Networks \n",
      "(RNN) that are placed independently and can traverse \n",
      "backward and forward directions at each time step, as \n",
      "shown in Fig 7. There are two ways to deal with data in \n",
      "this model - The first involves processing data from the \n",
      "past to the future , and the second operates in the opposite \n",
      "direction, from future  to past , where two hidden layers of \n",
      "neurons help to preserve the information in both \n",
      "directions. This approach helps to improve the information \n",
      "available in the algorithm.  \n",
      "For B iLSTM, the sequence of inputs is \n",
      "                   for a traditional RNN, which \n",
      "computes the hidden vector sequences \n",
      "                  and results in the output \n",
      "sequences                    for the iteration \n",
      "    to                            (11) \n",
      "                (12) \n",
      "  are the weight matrices;   is the bias vector, and   is \n",
      "the hidden layer function.  \n",
      "DCNN  [27] is a Convolutional Neural Network (CNN) \n",
      "type that helps to identify emotions from whispered \n",
      "speech . The input to the model is the speech data that \n",
      "traverses through many stages, such as the convolution \n",
      "layer, the pooling layer, Activation, Fully Connected, \n",
      "Batch normalization, and dropout layers, as shown in Fig \n",
      "8.  \n",
      " \n",
      " \n",
      " \n",
      "Fig 7. Structure of BiLSTM  \n",
      " \n",
      "To avoid overfitting, Leaky ReLU is used as an activation \n",
      "function in this implementation and evaluated as  \n",
      "{                      \n",
      "                        (13) \n",
      "The fully connected layer is a loss layer, measuring the \n",
      "inconsistency between the desired and actual outputs . Root \n",
      "The Mean Squared Propagation (RMSProp) optimization \n",
      "algorithm enhances the loss function, which varies in \n",
      "vertical directions.   \n",
      " \n",
      "Fig. 8 Structure of Deep Convolution Neural Network  \n",
      "5-7- Emotions  \n",
      "Deep learning techniques are applied to identify the \n",
      "emotions of human speech. Discrete Emotion [3] \n",
      "theory uses seven vital emotions in Human activities.  \n",
      " Sadness:  This is the feeling of dissatisfaction, \n",
      "sorrow, or fruitlessness.  \n",
      " Happiness:  This is the emotional state that elicits \n",
      "satisfaction and pleasure.  \n",
      " Fear:  This feeling triggers a fright response.  \n",
      " Anger:  This emotional state leads to feelings of \n",
      "hostility and frustration.  \n",
      " Surprise:  It is the state of mind that expresses \n",
      "either positive or negative following something \n",
      "unexpected.  \n",
      "\n",
      " \n",
      "Mohanty, & Cherukuri, Whispered Speech Emotion  Recognition with Gender Detection using BiLSTM and DCNN  \n",
      " \n",
      " \n",
      " \n",
      "158 \n",
      " Disgust:  A strong emotion that results in feeling \n",
      "repulsed.  \n",
      " Neutral:  This is the feeling of lack of particular \n",
      "preference.  \n",
      " \n",
      "5-8- Algorithm for Emotion Recognition  \n",
      "The algorithm to detect emotions from the whispered \n",
      "speech is in two parts  \n",
      " Gender Detection  \n",
      " Emotion Identification   \n",
      " \n",
      "Algorithm 1:  Geneder Detection  \n",
      "INPUT: Text  File (Whispered Speech Samples)  \n",
      "OUTPUT: Emotions (Sad, Happy, Fear, Anger, Surprise, \n",
      "Disgust, Neutral)  \n",
      " \n",
      "1: Begin:  \n",
      "2: Read speech  samples from Corpus   \n",
      "3: Pre-processing  the speech  samples:  \n",
      "4: Processed_speech=Pre_processing (Speech_Sample)   \n",
      "5: Extracted  SDC,  Pitch  features:  \n",
      "6: SDC, Pitch=Feature_Extraction(Processed_speech)   \n",
      "7: Multifeature  fusion:  \n",
      "8: multi _feature = multifeatured _fusion(SDC,  Pitch)   \n",
      "9: Dimension  reduction:  \n",
      "10: Dimension _R = Dimension _Reduction(multi _feature)  \n",
      "11: Determine  gender:  \n",
      "12: Gender = BiLSTM  (Dimension _R) \n",
      "13: Placed  in folders:  \n",
      "14: Gender=  (Male,  Female)   \n",
      "15: End:  \n",
      "Algorithm 2 Emotion Identification  \n",
      " \n",
      "1:  Begin:   \n",
      "2:  Speech Samples = (Male,  Female)  \n",
      "3:  Data Augmentation:  \n",
      "4:  Augemented _samples = Data Augmentation  (Speech          \n",
      "Samples)  \n",
      "5:  Pre-processed  the speech  samples:   \n",
      "6:  Processed _speech=Preprocessing  (Augemented _sam \n",
      "ples)  \n",
      "7:  Extracted  Spectral  Centroid,  ChromaSTFT,  MFCC,  \n",
      "Mel spectrogram  features:   \n",
      "8:  Spectral _Centroid,  Chroma _STFT,  MFCC,  \n",
      "Mel_spectrogram = Feature Extraction  (Processed _spe \n",
      "ech)  \n",
      "9:  Multifeature  fusion:   \n",
      "10:  multi _feature = \n",
      "multifeatured _fusion(Spectral _Centroid,  Chroma _STFT,  \n",
      "MFCC,  Mel_spectrogr am) \n",
      "11:  Dimension  reduction:  \n",
      "12:  Dimension_R = Dimension_Reduction(multi_feature)  \n",
      "13:  Emotion  detection:  14:  Emotions = DCNN(Dimension_R)   \n",
      "15:  OUTPUT Emotions  (Sad, Happy, Fear, Anger, \n",
      "Surprise, Disgust, Neutral)  \n",
      "16:  End:  \n",
      "6- Experiment and Result Analysis  \n",
      "The proposed SER has been implemented utilizing Python \n",
      "programming language and fortified by various machine \n",
      "learning libraries and additional supporting libraries . The \n",
      "experiment uses Python (Python 3.6.3rc1) and Librosa \n",
      "(Librosa 0.8.0) for audio processing. Graph plotting uses \n",
      "Seaborn and Matplotlib libraries, which help to analyze the \n",
      "speech data. BiLSTM and DCNN models implement the \n",
      "model using Keras (Keras - 2.6.0), TensorFlow \n",
      "(TensorFlow -2.6.0), and Scikit -learn libraries .  \n",
      "wTIMIT dataset is collected and divided into the train, \n",
      "cross -validation, and test samples. Subsequently, the \n",
      "speech processing takes place during implementation, \n",
      "incorporating data -augmentation techniques like shifting \n",
      "time, changing pitch, and changing speed . The pre -\n",
      "processing of speech content from each sample helps to \n",
      "analyze it and extract its features. The dimensionally \n",
      "reduced extracted features align with the data points in the \n",
      "BiLSTM model. The training dataset makes the model \n",
      "learn the features or data. Epochs continuously learn \n",
      "hidden features when the dataset completes backward and \n",
      "forward iterations. Then the cross -validation dataset is \n",
      "used to estimate the model’s performance on each epoch \n",
      "and prevent the model from being overfit ted. The test \n",
      "dataset helps to evaluate the trained model unbiasedly \n",
      "using performance metrics like precision, recall, f1 score, \n",
      "and confusion Matrix .  \n",
      "The parameters used in BiLSTM models are as in Table 1.  \n",
      "Table 1: Parameters of the BiLSTM model  \n",
      "Layers  Shape of output  Parameters#  \n",
      "Embedding  \n",
      "(Embedding)  (None, 216, 256)  \n",
      " 524544  \n",
      " \n",
      "Bidirectional  \n",
      " (None, 128)  \n",
      " 98816  \n",
      "batch normalization  (None, 128)  \n",
      " 512 \n",
      "Dense (Dense)  (None, 128)  \n",
      " 10512  \n",
      "dropout (Dropout)  (None, 128)  \n",
      " 0 \n",
      "flatten (Flatten)  (None, 128)  \n",
      " 1024  \n",
      "dense1(Dense)  \n",
      " (None, 12)  \n",
      " 0 \n",
      "dense2(Dense)  \n",
      " (None, 12)  \n",
      " 455 \n",
      " \n",
      " \n",
      "Journal of Information Systems and Telecommunication, Vol.12, No.2,  April -June 2024  \n",
      " \n",
      " \n",
      "159 \n",
      "The BiLSTM model helps to segregate the speech samples \n",
      "into male and female. The segregated speech samples \n",
      "move to their respective male and female folders.  \n",
      "Table 2: Parameters of the DCNN model  \n",
      "Layers  Shape of output  Parameters#  \n",
      "conv1d (Conv1D)  \n",
      " (None, 216, 256)  \n",
      " 2304  \n",
      "Activation  \n",
      "(Activation)  (None, 216, 256)  \n",
      " 0 \n",
      "conv1d 1 (Conv1D)  \n",
      " (None, 216, 256)  524544  \n",
      "batch normalization  \n",
      " (None, 216, 256)  1024 \n",
      "activation 1  \n",
      "(Activation)  \n",
      " (None, 216, 256) 0 \n",
      "dropout (Dropout)  (None, 216, 256)  \n",
      " 0 \n",
      "max pooling1d \n",
      "(MaxPooling1D)  \n",
      " (None, 13, 256)  \n",
      " 0 \n",
      "conv1d 2 (Conv1D)  \n",
      " (None, 13, 128)  \n",
      " 262272  \n",
      " \n",
      "activation 2 \n",
      "(Activation)  \n",
      " (None, 13, 128)  \n",
      " 0 \n",
      "conv1d 3 (Conv1D)  \n",
      " (None, 13, 128)  \n",
      " 131200  \n",
      " \n",
      "activation 3 \n",
      "(Activation)  \n",
      " (None, 13, 128)  \n",
      " 0 \n",
      "conv1d 4 (Conv1D)  \n",
      " (None, 13, 128)  \n",
      " 131200  \n",
      " \n",
      "batch normalization  \n",
      " (None, 13, 128)  \n",
      " 512 \n",
      "activation 4 \n",
      "(Activation)  \n",
      " (None, 13, 128)  \n",
      " 0 \n",
      "conv1d 5 (Conv1D)  \n",
      " (None, 13, 128)  \n",
      " 131200  \n",
      " \n",
      "batch normalization \n",
      "1 \n",
      " (None, 13, 128)  \n",
      " 512 \n",
      "activation 5 \n",
      "(Activation)  \n",
      " (None, 13, 128)  \n",
      " 0 \n",
      "dropout 1 (Dropout)  \n",
      " (None, 13, 128)  \n",
      " 0 \n",
      "max pooling1d 1 \n",
      "(MaxPooling1D)  \n",
      " (None, 1, 128)  \n",
      " 0 \n",
      "conv1d 6 (Conv1D)  \n",
      " (None, 1, 64)  \n",
      " 65600  \n",
      "activation 6 \n",
      "(Activation)  \n",
      " (None, 1, 64)  \n",
      " 0 \n",
      "conv1d 7 (Conv1D)  \n",
      " (None, 1, 64)  \n",
      " 32832  \n",
      "activation 7 \n",
      "(Activation)  \n",
      " (None, 1, 64)  \n",
      " 0 \n",
      "flatten (Flatten)  \n",
      " (None, 64)  \n",
      " 0 dense (Dense)  \n",
      " (None, 14)  \n",
      " 910 \n",
      "activation 8 \n",
      "(Activation)  \n",
      " (None, 14)  \n",
      " 0 \n",
      " \n",
      "After separating the speech samples into genders, Chroma \n",
      "STFT, Spectral centroid, Mel-scale spectrogram, and \n",
      "Spectral Flux feature s extraction happen ed. Then the \n",
      "created data points are dimensionally reduced to fit into \n",
      "the DCNN deep learning model to identify emotions. \n",
      "Metrics like precision, recall, F1 score, and the confusion \n",
      "matrix aid in assessing the model's performance.  Table 2 \n",
      "mentions the parameters of the DCNN model.  \n",
      "6-1- Result Analysis  \n",
      "The fusion of Fundamental frequency and SDC \n",
      "detect s the gender of speech samples. The initial \n",
      "stage involves pre -processing the speech samples, \n",
      "followed by feature extraction, which includes SDC \n",
      "and fundamental frequencies. The combination of \n",
      "SDC and Fundamental speech features create a \n",
      "multifeature fusion resulting in a single set of data \n",
      "points. The next step is to reduce the dimensions of \n",
      "data p oints to use them as input to the BiLSTM \n",
      "Model. The BiLSTM model predicts the data in the \n",
      "dataset as male and female, placed in separate \n",
      "folders. Of these, 3342 speech samples are available, \n",
      "and 3294 are correctly classified. So, the accuracy of \n",
      "the model prediction is 99.59%. The precision, recall, \n",
      "and f1 -score values, along with the confusion matrix, \n",
      "are shown in Fig s 9 and 10. The predicted model \n",
      "identifies the female speech samples more accurately \n",
      "than the male.   \n",
      " \n",
      "Fig. 9 Confusion Matrix for Gender detection by BiLSTM.  \n",
      " \n",
      "The graph in Fig 9 shows the gender detection from the \n",
      "speech sample. The X -axis represents the true label, and \n",
      "the Y-axis shows the predicted label.  This graph gives the \n",
      "\n",
      " \n",
      "Mohanty, & Cherukuri, Whispered Speech Emotion  Recognition with Gender Detection using BiLSTM and DCNN  \n",
      " \n",
      " \n",
      " \n",
      "160 \n",
      "count of detected gender spe ech that the Bi -LSTM model \n",
      "correctly and incorrectly detects.  \n",
      " \n",
      "Fig. 10 Accuracy measures of Gender Detection.  \n",
      " \n",
      "2031 female and 1263 male speech are correctly detected, \n",
      "whereas 13 females and 35 males are incorrectly detected. \n",
      "Pitch values identify the emotions after detecting genders \n",
      "from the speech samples.  \n",
      "The extraction of MFCC, Mel -scale spectrogram, Chroma \n",
      "STFT, Spectral Flux, and Spectral Centroid speech \n",
      "features happens from speech samples. Then, all five \n",
      "speech features are fused into a single feature and \n",
      "dimensionally reduced. Finally, the DCNN model is \n",
      "applied to predict emotions.  \n",
      "Individual emotions are detected based on gender as \n",
      "shown in Fig 11. The deviation in the male speech \n",
      "emotions is less than the female . Female fear and female \n",
      "neutral  emotions show divergent results from other \n",
      "emotions. The accuracy of the model is 98.54%.   \n",
      " \n",
      " \n",
      "Fig.11 Emotions based on gender.  \n",
      "6-2- Comparison Analysis  \n",
      "This experiment evaluates the current implementation \n",
      "using the wTIMIT  dataset, resulting in an impressive \n",
      "accuracy of 98.54%. The performance compares with \n",
      "three -layered Long short -term memory Bidirectional \n",
      "RNNs (LSTM BiRNN) that use No -Attention, Feed \n",
      "Forward Attention (FFA), and Improved Feed Forward \n",
      "Attention Mechanism ( IFFA) to evaluate the emotions in \n",
      "the dataset. This model uses 35 hidden states to train the dataset. The models are trained and validated with the help \n",
      "of the wTIMIT dataset [28] and tested with the help of the \n",
      "CHAINS dataset,  as shown in Table 3. This ex periment is \n",
      "a new hypothesis for  emotion detection with gender \n",
      "identification on whispered speech.  Hence, there are fewer \n",
      "references available for similar investigations.  \n",
      "Table 3: Comparison of the Implementation  \n",
      "  Sl# Model Implementation  Accuracy  \n",
      "1 FFA, IFFA, LSTM, Bi_RNN [2 8] 97.6 %  \n",
      "2 Proposed Model  98.54 %  \n",
      "The proposed method improves the performance of \n",
      "emotions after segregating the speech samples into genders.  \n",
      "By separating the approach into two models, this \n",
      "implementation excels in capturing natural and \n",
      "spontaneous expressions of emotions with low latency \n",
      "manner .  \n",
      "7- Conclusion  and Future Work  \n",
      "Identification of gender from whispered speech and \n",
      "recognizing emotions is a complicated task. This \n",
      "implementation initially detects the gender from the \n",
      "speech samples before identifying the emotions. Emotions \n",
      "might vary based on gender in the same situation. Speech \n",
      "features such as Fu ndamental Frequency and SDC help \n",
      "with gender identification. MFCC, Mel -scale spectrogram, \n",
      "Spectral flux, Spectral Centroid, and Chroma STFT play a \n",
      "vital role in detecting emotions. Multifeature fusion helps \n",
      "to combine speech features into a single set of d ata points . \n",
      "The concept verifies with the publicly available dataset \n",
      "wTIMIT with an accuracy of 98.54%. This approach helps \n",
      "to identify nearly inaudible emotions and is used to figure \n",
      "out the strategy. The proposed methodology can be \n",
      "improved using other s peech features, machine learning, \n",
      "and deep learning concepts.  \n",
      " \n",
      "References  \n",
      "[1]  ST Jovicic , and Z  Saric , ―Acoustic analysis of  consonants in \n",
      "whispered speech,‖ Journal of voice , vol 22, no. 3, pp. 263–\n",
      "274, 2008 . \n",
      "[2]  M Kumari , and I  Ali, ―An efficient algorithm for gender \n",
      "detection using voice samples,‖ 2015 Communication, \n",
      "Control and Intelligent Systems (CCIS),  2015 , Mathura, \n",
      "Utter Pradesh,  pp. 221–226, doi: \n",
      "10.1109/CCIntelS.20 15.7437912 . \n",
      "[3] S Motamed , S Setayeshi, A Rabiee , and A Sharifi, ―Speech \n",
      "Emotion Recognition Based on Fusion Method,‖ Journal of \n",
      "Information Systems and Telecommunication (JIST), vol. 3, \n",
      "pp. 50--56, 2017, doi: 10.7508/jist.2017.17.007 .  \n",
      "[4] JS Li, CC Huang , ST Sheu , and MW  Lin, ―Speech emotion \n",
      "recognition and its applications,‖ Proc. of Taiwan Institute of \n",
      "Kansei Conference , 2010  Paris , France , pp. 187–192.  \n",
      "\n",
      " \n",
      "Journal of Information Systems and Telecommunication, Vol.12, No.2,  April -June 2024  \n",
      " \n",
      " \n",
      "161 \n",
      "[5] A Guerrieri , E Braccili , F Sgro , and GN Meldolesi  ―Gender \n",
      "identification in a two -level hierarchical speech emotion \n",
      "recognition system for an Italian Social Robot,‖ Sensors, vol. \n",
      "22, no. 5, pp. 1714, 2022 , doi: 10.3390/s22051714 . \n",
      "[6] M Sarria -Paja, TH Falk, and D  O’Shaughnessy , ―Whispered \n",
      "speaker verification and gender detection using weighted \n",
      "instantaneous frequencies,‖ 2013 IEEE International \n",
      "Conference on Acoustics, Speech and Signal Processing , \n",
      "2013  (May 26 -31), Vancouver , Canada, pp. 7209 –7213 , doi: \n",
      "10.1109/ICASSP.2013.6639062 .  \n",
      "[7] J Deng , S Fruhholz, Z Zhang , and Bojrn Schuller , \n",
      "―Recognizing emotions from whispered speech based on \n",
      "acoustic feature transfer learning,‖ IEEE Access , vol.  5, pp. \n",
      "5235 –5246 , 2017 .  \n",
      "[8] M Cotescu , T Drugman, G Huybrechts, J Lorenzo -Trueba, \n",
      "and A Moinet , ―Voice conversion for whispered speech \n",
      "synthesis,‖  IEEE Signal Processing Letters , vol. 27, pp. 186–\n",
      "190, 2019 , doi: 10.1109/LSP.2019.2961213 .  \n",
      "[9] P Mishra , and R Sharma, ―Gender differentiated \n",
      "convolutional neural networks for speech emotion \n",
      "recognition,‖ 2020 12th International Congress on Ultra -\n",
      "Modern Telecommuni - cations and Control Systems and \n",
      "Workshops (ICUMT),  2020  (October 5 -7), Brno , Czech \n",
      "Republic,  pp. 142–148, doi: \n",
      "10.1109/ICUMT51630.2020.9222412 . \n",
      "[10] Mustaqeem S Kwon, ―Optimal feature selection based \n",
      "speech emotion recognition using two -stream deep \n",
      "convolutional neural network,‖ International Journal of \n",
      "Intelligent Systems , vol. 36, no. 9, pp. 5116 – 5135 , 2021 , \n",
      "doi:  10.1002/int.22505 . \n",
      "[11] J. Ancilin and A. Milton, ―Improved speech emotion \n",
      "recognition with mel frequency magnitude coefficient ,‖ \n",
      "Applied Acoustics , vol. 179, pp. 108046 , 2021 , doi: \n",
      "10.1016/j.apacoust.2021.108046 .  \n",
      "[12] S. Jothimani and K. Premalatha, ―Mff -saug: Multi feature \n",
      "fusion with spectrogram augmentatio n of speech emotion \n",
      "recognition using convolution neural network ,‖ Chaos, \n",
      "Solitons & Fractals , vol. 162, pp. 112512 , 2022 , doi: \n",
      "10.1016/j.chaos.2022.112512 .  \n",
      "[13] B Yalamanchili, SK Samayamantula, and KR Anne, ―Neural \n",
      "network -based blended ensemble learning for speech \n",
      "emotion recognition,‖ Multidimensional Systems and Signal \n",
      "Processing, vol. 33, no. 4, pp. 1323 --1348 , 2022 , doi: \n",
      "10.1007/s11045 -022-00845 -9.  \n",
      "[14] T Feng , R Hebbar , and S Narayanan, ―Trustser: On the \n",
      "trustworthiness of fine -tuning pre -trained speech embeddings \n",
      "for speech emotion recognition,‖ arXiv preprint \n",
      "arXiv:2305.11229, 2023 , doi:  10.48550/arXiv.2305.11229  \n",
      "[15] RV Darekar and M Chavan , S Sharanyaa,  and NR Ranjan , \n",
      "―A hybrid meta -heuristic ensemble based classification \n",
      "technique speech emotion recognition,‖ Advances in \n",
      "Engineering Software, vol. 180, pp. 103412 , 2023 . \n",
      "[16] J Rekimot o, ―Dualvoice: A speech interaction method using \n",
      "whisper -voice as commands,‖ CHI Conference on Human \n",
      "Factors in Computing Systems Extended Abstracts,  pp. 1–6, \n",
      "2022 , doi: 10.1145/3491101.3519700 . \n",
      "[17]H Dolka,  AX VM , and S Juliet,―  Speech  emotion r ecognition  \n",
      "using  ANN on MFCC  features,‖ 2021 3rd international \n",
      "conference on signal processing and communication (ICPSC) , \n",
      "2021, Coimbatore, India, pp. 431–435, doi: \n",
      "10.1109/ICSPC51351.2021.9451810 . [18] MK Reddy and KS Rao, ―Robust pitch extraction method \n",
      "for the hmm -based speech synthesis system,‖ IEEE signal \n",
      "processing letters , vol.  24, no. 8, pp. 1133 –1137 , 2017 , doi: \n",
      "10.1109/LSP.2017. 2712646 .  \n",
      "[19] J Chatterjee, V Mukesh, HH Hsu , G Vyas , and Z Liu , \n",
      "―Speech emotion recognition using cross - correlation and \n",
      "acoustic features,‖ 2018 IEEE 16th Intl Conf on Dependable, \n",
      "Autonomic and Secure Computing, 16th Intl Conf on \n",
      "Pervasive Intelligence and Computing, 4th Intl Conf on Big \n",
      "Data Intelligence and Computing and Cyber Science and \n",
      "Technology Congress , 2018  (August 12-15), Athens, Greece , \n",
      "pp. 243–249, doi: 10.1109/DASC/PiCom/DataCom/CyberSci  \n",
      " Tec.2018.00050 .  \n",
      "[20] S Rajesh and NJ Nalini, ―Musical instrument emotion \n",
      "recognition using deep recurrent neural network,‖ Procedia \n",
      "Computer Science, vol. 167, pp. 16 --25, 2020, doi: \n",
      "10.1016/j.procs.2020.03.178 . \n",
      "[21] M Aly , and NS Alotaibi, ―A novel deep learning model to \n",
      "detect covid -19 based on wavelet features extracted from \n",
      "mel- scale spectrogram of patients’ cough and breathing \n",
      "sounds,‖ Informatics in Medicine Unlocked , vol.  32, pp.  \n",
      "101049 , 2022, doi: 10.1016/j.imu.2022.101049 . \n",
      "[22] Z Qawaqneh,  AA Mallouh,  and BD Barkana, ― Age and \n",
      "gender  classification  from  speech and face images by jointly \n",
      "fine-tuned deep neural networks,‖ Expert Systems with \n",
      "Applications , vol. 85, pp. 76–86, 2017 , doi: \n",
      "10.1016/j.eswa.2017.05.037 .  \n",
      "[23] A Koduru, HB Valiveti and AK Budati, ―Feature extraction \n",
      "algorithms to improve the speech emotion recognition rate,‖ \n",
      "International Journal of Speech Technology , vol. 23, no. 1, \n",
      "pp. 45–55, 2020 , doi: 10.1007/s10772 -020-09672 -4.  \n",
      "[24] S Zhang , and C Li, ―Research on feature fusion speech \n",
      "emotion recognition technology for smart teaching,‖ Mobile \n",
      "Information Systems , vol. 2022 , 2022 , doi:  \n",
      "10.1155/2022/7785929 . \n",
      "[25] GB, Prasanna, SV Bhat, C Naik , and HN Champa, ―An \n",
      "Efficient Method for Handwritten Kannada Digit \n",
      "Recognition based on PCA and SVM Classifier,‖ Journal of \n",
      "Information Systems and Telecommunication (JIST), vol. 3, \n",
      "no. 35, pp. 169 2021, doi: 20.1001.1.23221437.2021.9.35.3.2 . \n",
      "[26] A Graves, N Jaitly, and A Mohamed, ―Hybrid speech \n",
      "recognition wit h deep bidirectional lstm,‖ 2013 IEEE \n",
      "workshop on automatic speech recognition and \n",
      "understanding , 2013, Olomouc, Czech Republic, pp.  273–\n",
      "278, doi: 10.1109/ASRU.2013.6707742 .  \n",
      "[27] N Aloysius and M Geetha, ―A review on deep convolutional \n",
      "neural networks,‖ 2017 international conference on \n",
      "communication and signal processing (ICCSP), 2017, pp. \n",
      "0588 –0592 , IEEE, doi: 10.1109/ICCSP.2017.8286426 .  \n",
      "[28] SBC Gutha, MAB Shaik, T Udayakumar , and AA \n",
      "Saunshikhar , ―Improved feed forward attention mechanism in \n",
      "bidirectional recurrent neural networks for robust sequence \n",
      "classification,‖ 2020 International Conference on Signal \n",
      "Processing and Communications (SPCOM) , 2020 , IISc, \n",
      "Bangalore.  IEEE , pp. 1—5, doi: \n",
      "0.9179606 10.1109/SPCOM50965.202  \n",
      "\n",
      "\n",
      "Reading: d:\\PythonCode\\document_portal\\notebook\\data\\Improvement_of_Speech_Emotion_Recognition_by_Deep_Convolutional_Neural_Network.pdf\n",
      " \n",
      " \n",
      " \n",
      "Journal of Information Systems and Telecommunication  \n",
      "Vol.12, No.2,  April -June 2024, 152-161 \n",
      " \n",
      "http://jist.acecr.org  \n",
      "ISSN 2322 -1437  / EISSN:2345 -2773  \n",
      " \n",
      "Whispered Speech Emotion Recognition with Gender Detection using \n",
      "BiLSTM and DCNN  \n",
      "Aniruddha Mohanty1*, Ravindranath C Cherukuri1 \n",
      " \n",
      "1.Department of Computer  science and Engineering, Christ (Deemed to be University), Bangalore, India  \n",
      " \n",
      " \n",
      "Received: 22 Aug 2023 / Revised: 04 Apr 2024 / Accepted: 21 May 2024  \n",
      " \n",
      " \n",
      "Abstract   \n",
      "Emotions are human mental states at a particular instance in time concerning one’s circumstances, mood, and relationships \n",
      "with others . Identifying emotions from the whispered speech is complicated as the conversation might be confidential. The \n",
      "representation of the speech relies on the magnitude of its information. Whispered speech is intelligible, a low -intensi ty \n",
      "signal, and varies from normal speech. Emotion identification is quite tricky from whispered speech. Both prosodic and \n",
      "spectral speech features help to identify emotions. The emotion identification in a whispered speech happens using prosodic \n",
      "speech fea tures such as zero -crossing rate  (ZCR) , pitch, and spectral features that include spectral centroid, chroma STFT, \n",
      "Mel scale spectrogram, Mel -frequency cepstral coefficient (MFCC), Shifted Delta Cepstrum (SDC), and Spectral Flux. \n",
      "There are two parts to the proposed implementation. Bidirectional Long  Short -Term Memory (BiLSTM) helps to identify \n",
      "the gender from the speech sample in the first step with SDC and pitch. The Deep Convolutional Neural Network (DCNN) \n",
      "model helps to identify the emotions in the second  step. This implementation is evaluated using the  wTIMIT data corpus \n",
      "and gives 98.54% accuracy. Emotions have a dynamic effect on genders, so this implementation performs better than \n",
      "traditional approaches. This approach helps to design online learning management systems, different applications for \n",
      "mobile devices, checking cyber -criminal activities, emotion detection for older people, automatic speaker identification and \n",
      "authentication, forensics, and surveillance . \n",
      " \n",
      "Keywords:  Whispered Speech; Emotion Re cognition; Speech Features; Data Corpus; BiLSTM;  DCNN . \n",
      " \n",
      "1- Introduction  \n",
      "Emotions are the humans’ short -lived feeling s that affects \n",
      "thinking, actions, relationships, and social interactions. \n",
      "Emotions express humans’ physiological and emotional \n",
      "states with facial expressions and body language. \n",
      "Whispered speech is a form of speec h that expresses \n",
      "emotions. Whispered speech is a type of communication \n",
      "produced with breath without any noise and excitation of \n",
      "voice. The whispered speech structure changes \n",
      "significantly because of the lack of periodic excitation in \n",
      "the voice. This result s in missing speech and reduced \n",
      "transparency in communication.  \n",
      "The difference between normal and whispered speech is \n",
      "the absence of vocal tract vibrations due to the vocal \n",
      "tract’s physiological blocking. The strength of whispered \n",
      "speech is minute and with out voice compared to phoned \n",
      "speech. The spectral and prosodic features help to detect \n",
      "the whispered speech. Prosodic features of speech vary \n",
      "over time. Spectral features of whispered speech are highly accurate over unvoiced consonants, voiced consonants, \n",
      "and formants in vowels [1]. \n",
      "The impacts on Speech Emotion Recognition ( SER) are \n",
      "due to various acoustic conditions such as compressed, \n",
      "noisy, telephonic conversation s and imitator speeches. \n",
      "Other environmental conditions,  such as stress, rhythm, \n",
      "and intonation, can affect  SER.  Whispered speech is also \n",
      "affected by similar acoustic and environmental conditions. \n",
      "SER depends on acoustic characteristics and gender in \n",
      "some scenarios. Hence gender plays a vital role in SER.  \n",
      "Nowadays, several whispered speech  samples of male and \n",
      "female voices are available. The effectiveness of the SER \n",
      "is more when the implementation is in two parts. The \n",
      "identification of the gender [2] happens initially using \n",
      "Bidirectional Long Short -term Memory (BiLSTM)  with \n",
      "speech features . The next step is detecting emotions using \n",
      "various speech features [3] and Deep Convolutional \n",
      "Neural Network (DCNN).   \n",
      "The structure of the paper includes various sections. \n",
      "Section 2 describes Human Emotions and their \n",
      "Applications. Details of the Related Work are in Section 3. \n",
      "Section 4 describes the System Model, which is the black \n",
      "box view of the implementation. Section 5 is the Model \n",
      " \n",
      "Journal of Information Systems and Telecommunication, Vol.12, No.2,  April -June 2024  \n",
      " \n",
      " \n",
      "153 \n",
      "Design that describes the details of the speech features and \n",
      "the deep learning models used in this implementation. \n",
      "Section 6 gives the Experiment and Result Analysis. \n",
      "Section 7 brief ly discuss  the Conclusion and Future Work.   \n",
      "2- Human  Emotions  and their  Applications  \n",
      "Human emotions are the mental state caused by countless \n",
      "associated views , feeling s, behavioral replies, and the \n",
      "degree of pressure and annoyance. It is often associated \n",
      "with attitude , temperament, behavior , disposition, and \n",
      "creativi ty [4]. Emotion recognition helps to detect a \n",
      "humans ’ emotional mood , which lasts hours and days. \n",
      "Speech conveys emotions such as anger, disgust, fear, \n",
      "happiness, neutrality, sadness, and surprise. The machine \n",
      "automatically detects various emotions from speech with \n",
      "the help of different algorithms. Emotion Recognition \n",
      "helps to : \n",
      " \n",
      "•  Detect customers ’ intentions based on the teleconference. \n",
      "•  Detect cybercrime.  \n",
      "•  Students ’ attention and teachers ’ content adjustment.  \n",
      "•  Disability assistance  \n",
      "•  Customer satisfaction  \n",
      "•  Stress monitoring  \n",
      "•  Social media analysis  \n",
      "•  Suspicious activity  \n",
      "•  Human -machine interaction and so on.   \n",
      "3- Related Work  \n",
      "Over time, numerous studies have detected emotions from \n",
      "whispered speech . The vario us deep learning models \n",
      "detect emotions based on the speech features extracted \n",
      "from the collected whispered speech data corpus . The SER \n",
      "for normal and whispered speech is diverse because of \n",
      "vocal excitation. Emotions vary with many factors; gender \n",
      "is among  the most influential factors  [5]. Identifying \n",
      "gender in the first step improves emotion detection from \n",
      "whispered speech. So, the related work explored is on \n",
      "gender detection and emotion recognition from whispered \n",
      "speech.  \n",
      "MFCC obtained by the Hilbert enve lope approach and \n",
      "weighted instantaneous frequencies (WIFs) obtained by \n",
      "the coherent demodulation help to detect gender in \n",
      "whispered speech  samples [6]. There is an opportunity to \n",
      "explore gender detection in noisy speech conditions using \n",
      "these approaches.  \n",
      "Autoencoder -enabled features  in the transfer learning \n",
      "framework propose to practice  phonated data to identify \n",
      "emotions from Whispered speech [7]. The feature \n",
      "extraction is from the Geneva Whispered Emotion Corpus \n",
      "(GeWEC) and Berlin Emotional Speech Databa se (EMO -\n",
      "DB) data corpus. The acoustic features such as Mel-frequency cepstral coefficient (MFCC), root mean square \n",
      "(RMS), frame energy, zero -crossing -rate (ZCR), pitch \n",
      "frequency (F0), probability of voicing autocorrelation \n",
      "function are the inputs to evaluate the framework. \n",
      "Implementing deep learning concepts on spontaneous data \n",
      "gives more accuracy than the current framework .  \n",
      "Gender is detected to target an anonymous speaker. The \n",
      "Deep Neural Network (DNN) model is used to generalize \n",
      "gender by using the MFCC speech feature. This \n",
      "implementation is applied to the wTIMIT dataset to verify \n",
      "the gender and recognize the speaker  [8]. The DNN model \n",
      "cannot generalize gender; evaluation can happen with \n",
      "other datasets.   \n",
      "MFCC and CNN, with the fully  connected network, detect \n",
      "gender and emotions like anger, disgust, fear, happiness, \n",
      "sadness, surprise, and a neutral state  [9]. The concept \n",
      "verification happens on RAVDESS, CREMA -D, SAVEE, \n",
      "and TESS datasets, having an accuracy of 92.283%, which \n",
      "is better than the tr aditional model.  \n",
      "The final prediction of the implemented model is to learn \n",
      "the mutually spatial -spectral features happens by a two -\n",
      "stream deep convolutional neural network with an iterative \n",
      "neighborhood  component analysis (INCA) and the most \n",
      "discriminatory optimal features  [10]. The concept \n",
      "verification happens on EMO -DB, SAVEE, and \n",
      "RAVDESS emotional speech corpora which perform with \n",
      "95%, 82%, and 85% accuracy rates. Real -time applications \n",
      "with natural and huge data corpora can help to extend this \n",
      "concept to identify emotions.  \n",
      "Mel Frequency Magnitude Coefficient (MFMC) and three \n",
      "spectral features, namely MFCC, log frequency power \n",
      "coefficient, and linear prediction cepstral coefficient, are \n",
      "used with the help of Support Vector Machine (SVM) \n",
      "modeling  [11]. The performance evaluation uses this \n",
      "concept  on Berlin, RAVDESS, SAVEE, EMOVO, and \n",
      "eNTERFACE data corpus . Feature selection, feature \n",
      "fusion, and multiple classification schemes improve the \n",
      "performance of MFMC.  \n",
      "The proposed MFF -SAug research in  which noise removal \n",
      "improves emotion. White Noise Injection, Pitch Tuning \n",
      "techniques, MFCC, ZCR, RMS speech features, and \n",
      "Convolutional Neural Network (CNN) modeling [12] \n",
      "detect emotions. Emotion detection during the interaction \n",
      "between people can extend the MFF -SAug approach .  \n",
      "The proposed Neural Network -based Blended Ensemble \n",
      "Learning (NNBEL) model is composed of a 1 -dimensional \n",
      "Convolution Neural Network (1D -CNN), Long Short -\n",
      "Term Memory (LSTM), and CapsuleNets. LSTM receives \n",
      "input from the Log Mel -spect rogram speech features, \n",
      "while 1D -CNN and CapsuleNets receive input from the \n",
      "MFCC.  Each model’s output is fed to Multi -Layer \n",
      "Perceptron (MLP) and predicts the final emotion s [13]. \n",
      "This model shows 95.3% and 94% accuracy on \n",
      "RAVDESS and IEMOCAP datasets, respectively.  \n",
      " \n",
      "Mohanty, & Cherukuri, Whispered Speech Emotion  Recognition with Gender Detection using BiLSTM and DCNN  \n",
      " \n",
      " \n",
      " \n",
      "154 \n",
      "TrustSER [14] implemented a general framework to \n",
      "determine the SER system's trustworthiness using deep \n",
      "learning techniques. Trustworthiness evaluates privacy \n",
      "(gender in formation, speaker demographics), safety, \n",
      "fairness, sustainability, and emotions (sad, angry, happy, \n",
      "neutral). The architecture of the TrustSET framework uses \n",
      "CNN encoder and Transformer encoder models. \n",
      "Trustworthy profiles under the Federated Learning \n",
      "scenario might improve privacy, fairness, and safety.  \n",
      "A hybrid meta -heuristic ensemble -based classification [15] \n",
      "helps to detect speech emotions. Raw speech samples are \n",
      "filtered using the Butterworth filter; then, spectrogram \n",
      "speech features are extracted from each frame to create a \n",
      "hybrid feature vector. The ensemble -based classification is \n",
      "applied to hybrid feature vectors to classify em otions. The \n",
      "ensemble -based classification contains a Recurrent Neural \n",
      "Network (RNN), a Deep Belief Network (DBN), and an \n",
      "Artificial Neural Network (ANN).  \n",
      "The above -related work shows that it is crucial to identify \n",
      "gender to segregate emotions in a speech. Male and female \n",
      "emotions are different based on situations. Whispered \n",
      "speech is an essential concept of emotion recognition. \n",
      "There is an opportunity to experiment further based on \n",
      "other datasets of whispered speech. So, this is a motivation \n",
      "to work on emot ion recognition in whispered speech, as \n",
      "this is a less explored area of research .  \n",
      "4- System Model  \n",
      "The analysis of e motion recognition from the whispered \n",
      "speech segregates into two parts. One is gender detection, \n",
      "and the second is emotion detection as shown in Fig 1.  \n",
      " \n",
      " \n",
      "Fig. 1 Emotion recognition from whispered speech  \n",
      " \n",
      "As part of gender detection, pre -processing of the speech \n",
      "samples is performed, and then pitch and SDC are \n",
      "extracted as part of prosodic and spectral features, \n",
      "respectively. Both the features are fused to get a single \n",
      "feature set with the help of multifeature fusion. PCA helps \n",
      "to reduce the dimensions of extracted features and is used \n",
      "as input to Bidirectional Long short -term memory \n",
      "(BiLSTM) to classify genders , as shown in Fig 2.  \n",
      " \n",
      " \n",
      "Fig .2 Gender detection from whispered speech  \n",
      " \n",
      "Following  Gender detection, the augmentation of speech \n",
      "data helps to get more realistic data. Then the speech features such as Chromatogram, Zero -Crossing Rate, \n",
      "Spectral Central, MFCC, Spectral Flux , and Mel \n",
      "Spectrog rams are extracted. All the extracted features \n",
      "combine to get a single feature using multifeature fusion. \n",
      "Then the dimensions are reduced to get the optimal data \n",
      "points. The data points are inserted into the Deep \n",
      "Convolutional Neural Network (DCNN) to identify the \n",
      "emotions , as shown in Fig 3.  \n",
      "5- System  Design  \n",
      "The implementation of system design happens in two parts. \n",
      "The first part identifies the male and female gender from \n",
      "the whispered speech samples. Then the determination of \n",
      "different emotions like sadn ess, happiness, fear, anger, \n",
      "surprise, disgust, and neutral are detected in gender -\n",
      "segregated speech samples.  \n",
      " \n",
      "Fig. 3 Emotion Recognition from Male and Female whispered speech.  \n",
      "5-1- Whispered Speech Data Corpus  \n",
      "wTIMIT [16] is a whispered voice data corpus ha ving 450 \n",
      "phonetically balanced sentences with 29 speakers. There \n",
      "are 11,324 utterances with a normal voice , which  can be \n",
      "used for normal and whispered training. This data is \n",
      "available in two parts - train and test divisions. The \n",
      "samples contain an 8 kHz sampling rate with a high -\n",
      "quality  and low-pass filter.  \n",
      "5-2-    Data Augmentation  \n",
      "Data augmentation [17 ] is the method of getting more \n",
      "realistic data from the existing data, which helps to add \n",
      "more training data to the model, reduce overfitting, and \n",
      "increase the model’s generalization ability. Below are the \n",
      "steps to generate synthetic data.  \n",
      " Shifting Time : Shifting time is a simple concept \n",
      "where the audio shifts to the left or right with a \n",
      "haphazard second. If the audio shifts to the left \n",
      "\n",
      " \n",
      "Journal of Information Systems and Telecommunication, Vol.12, No.2,  April -June 2024  \n",
      " \n",
      " \n",
      "155 \n",
      "with z seconds, then the first z seconds are \n",
      "marked as silence. Similarly, if the audio shifts to \n",
      "the right with z second s, the last z seconds are \n",
      "marked as silence.  \n",
      " Changing Pitch: Pitch change is adjusting the \n",
      "pitch randomly without upsetting the speediness \n",
      "of the audio file.  \n",
      " Changing Speed: Speed audio changes by \n",
      "stretching the time series data by a fixed rate.  \n",
      "5-3- Pre-processing  \n",
      "After the data collection and data augmentation, pre -\n",
      "processing [3] is the initial  phase  in training SER. \n",
      "Framing, Windowing, Voice activity detection, \n",
      "Normalization, and Noise reduction are pre -\n",
      "processing steps.   \n",
      "Framing:  Speech signals are quasip eriodic and vary \n",
      "over time. Hence, information and related emotions \n",
      "also fluctuate over time. The segregation of the \n",
      "signals happens in a shorter period to make the \n",
      "speech signal invariant. Twenty milliseconds to thirty \n",
      "milliseconds helps to make the speec h signal \n",
      "invariant, and five milliseconds of overlap of the \n",
      "frames avoid data leakage between the frames .  \n",
      "Windowing:  Windowing on each frame helps to diminish \n",
      "data leakage during Fast Fourier Transformation (FFT) \n",
      "after framing. The Hamming window allows  this step \n",
      "where the window size is N for the frames     , used in \n",
      "equation (1).  \n",
      " \n",
      "                  (   \n",
      "   )       (1) \n",
      "            \n",
      " \n",
      "Voice activity detection (VAD):  An utterance has three \n",
      "portions of speech activities : voiced, unvoiced, and silent. \n",
      "Zero Crossing Rate (ZCR) speech feature helps to detect \n",
      "VAD. ZCR represents the frequency of signal transitions \n",
      "between positive and negative values within a specific \n",
      "frame . Due to high energy, the ZCR value is low for \n",
      "voiced speech and high for unvoiced s peech because of \n",
      "low energy.  \n",
      "Normalization:  Normalization helps to reduce  speaker \n",
      "and recording inconsistency without affecting the features’ \n",
      "strength and enhancing the features’ generalization \n",
      "capability. The Z-normalization method is used more and \n",
      "represented as  \n",
      "       \n",
      "      (2) \n",
      "Where   is the speech signal,   and   are the mean and \n",
      "standard deviation of the data, respectively.  \n",
      "Noise Reduction:  The environmental noise captured while \n",
      "recording a speech signal  affects the recognition rate . \n",
      "Minimum mean square error (MMSE) and log -spectral \n",
      "amplitude MMSE (LogMMSE) reduce the noise from the speech signals. Noise reduction helps to get more accuracy \n",
      "in SER evaluation.  \n",
      "5-4- Feature Extraction  \n",
      "Specific emotions are present in prosodic and spectral \n",
      "features of speech. The extraction of prosodic and spectral \n",
      "features is a vital characteristic of emotion recognition \n",
      "after pre -processing the speech signals.  \n",
      "5-4-1-Prosodic Features  \n",
      "Prosodic features deal with the audio qualities of a speech \n",
      "when connected speeches use sounds as input. The \n",
      "production of the speech deals with the amount of energy, \n",
      "frequency, period, loudness, pitch, and duration. Speech \n",
      "signal com munication depends on intonation, stress, and \n",
      "rhythm, which prosodic features can detect. The prosodic \n",
      "features used in the implementation are : \n",
      "Zero -Crossing Rate (ZCR):  ZCR [17] measures the \n",
      "frequency of signal transitions between positive and \n",
      "negative values in every audio frame  and defined as  \n",
      "    \n",
      "    ∑       [ ]         [      \n",
      "    (3) \n",
      "Where        is the sign function . \n",
      "    [     ]  {        \n",
      "       \n",
      "Fundamental Frequency (Pitch):  Fundamental \n",
      "Frequency (F0) [18] is the minimum freque ncy of the \n",
      "periodic waveform. F0 is the significant parameter to \n",
      "differentiate male speech from female speech. Pitch also \n",
      "determines the voiced and unvoiced portion of the speech \n",
      "signal. This analysis uses pitch parameters like pitch mean \n",
      "value and pitch r ange.  \n",
      "The pitch range determines the number of octaves a speech  \n",
      "sample  can cover, from the lowest to the highest. F0 value \n",
      "varies from 85Hz to 180 HZ for the voiced speech of adult \n",
      "males.  \n",
      " \n",
      " \n",
      "Fig. 4 Representation of Pitch data  \n",
      " \n",
      "Similarly, the F0 value for adult females goes from 165Hz \n",
      "to 255Hz, as shown in Fig 4.  \n",
      "\n",
      " \n",
      "Mohanty, & Cherukuri, Whispered Speech Emotion  Recognition with Gender Detection using BiLSTM and DCNN  \n",
      " \n",
      " \n",
      " \n",
      "156 \n",
      "5-4-2-Spectral features  \n",
      "The representation of spectral features happens by \n",
      "converting the time -domain speech signal into the \n",
      "frequency domain with the help of the Fast Fourier \n",
      "Transform (FFT) which  benefits  to represent the \n",
      "characteristics of the Human Vocal Tract. The  spectral \n",
      "features are  \n",
      "Spectral Centroid (SC):  The Spectral Centroid [19] is the \n",
      "most used speech feature where the positioning of the \n",
      "―center of mass‖ of the audio signal spectr um defines the \n",
      "weight of the spectrum. The center of mass measures the \n",
      "weighted average of the frequency component located in \n",
      "the audio signal, defined as  \n",
      " \n",
      "      ∑            \n",
      "   \n",
      "∑       \n",
      "       (4) \n",
      "Where      represents the magnitude of bin numb er   and \n",
      "      represents the central frequency of the bin.  \n",
      "Chroma STFT:  Chroma STFT [20] is obtained using FFT \n",
      "on speech samples and the resulting spectrums a re a \n",
      "chromatogram in a vertical axis. This feature captures the \n",
      "harmonic feature of the speech s ignals.  \n",
      "Mel-scale Spectrogram:  Mel-scale Spectrogram [ 21] is a \n",
      "spectrogram in which frequencies convert to the Mel scale, \n",
      "which helps to differentiate the range of frequencies. Mel -\n",
      "spectrogram helps to understand emotions in a better way \n",
      "as humans can perceive sound on a logarithmic scale.  \n",
      "Mel Frequency Cepstral Coefficient (MFCC ): MFCC \n",
      "[17] i s SER’s  standard feature extraction technique. The \n",
      "vocal cord s, tongue, and teeth filter the sound and make it \n",
      "unique for each speaker in the Human Speech production \n",
      "system. Mel scale represents MFCC, where the frequency \n",
      "bands are equally spaced and close to the Human Auditory \n",
      "System’s response, as shown in Fig 5.  \n",
      " \n",
      "Fig 5 MFCC speech feature extraction  \n",
      " \n",
      "Shifted Delta Cepstrum (SDC):  Time derivatives apply \n",
      "to the cepstral coefficients obtained from the MFCC and \n",
      "combined with the delta coefficient to get SDC, as shown \n",
      "in Fig 6.       and   are the four parameters used in \n",
      "SDC [ 22].  M denotes the cepstral coefficient for each \n",
      "frame.   indicates added frames.   are the frames that \n",
      "append delta features from the new feature vector .   \n",
      "represents the delta values difference. So, the coefficient \n",
      "vectors represent as   \n",
      "     [                  ]   (5) \n",
      "   are MFCC coefficients and   is the coefficient index. \n",
      "For a given time,   an intermediate calculation is done to \n",
      "obtain the   coefficients.  \n",
      "                                 (6) \n",
      "Finally, the SDC coefficients vectors of   dimensions are \n",
      "obtained as  \n",
      "       [          (                )] (7) \n",
      " \n",
      " \n",
      "Fig. 6 Extraction of SDC speech feature  \n",
      " \n",
      "This speech feature helps to identify genders for a long -\n",
      "range dynamic appearance in speech signals.  \n",
      "Spectral Flux:  Spectral flux [ 20] measures the change in \n",
      "speed of the signal’s power spectrum compared to the \n",
      "previous frame. This feature estimates the speech signal’s \n",
      "power spectrum about  the power spectrum of one frame \n",
      "with others.  \n",
      "5-5- Feature Selection and Dimension Reduction  \n",
      "Feature s election [23] is selecting features from a large set \n",
      "of extracted features to eliminate redundant and unused \n",
      "information and decrease processing time. For this work, \n",
      "the extracted pitch contains lots of information. The global \n",
      "features, like the range of p itch values and mean pitch \n",
      "values, are selected. Removing redundant and unused \n",
      "information, such as additional zeroes, duplicate values, or \n",
      "frames, is part of MFCC feature extraction. The exact \n",
      "process removes the silent speech frames.  \n",
      "Fundamental frequen cy and SDC are two speech features \n",
      "to identify gender. Similarly, Spectral centroid, Chroma \n",
      "STFT, MFCC, and Spectral flux features are used for \n",
      "emotion identification to create a single set of data points \n",
      "from multiple speech features using multifeature fu sion \n",
      "technique [24]. \n",
      "Multifeature Fusion:  The extracted speech features hav e \n",
      "different dimensions, so feature proximity usages  average \n",
      "dimensional spacing between the vectors to denote the \n",
      "proximity between the diverse features  [24]. The average \n",
      "dimensional spacing between the vectors is computed as  \n",
      "          \n",
      "    √∑∑              (8) \n",
      "   =  \n",
      "     √∑∑              (9) \n",
      "\n",
      " \n",
      "Journal of Information Systems and Telecommunication, Vol.12, No.2,  April -June 2024  \n",
      " \n",
      " \n",
      "157 \n",
      "        and     are the mean and variance interval of \n",
      "average dimensions of the feature vector s.    and    are \n",
      "the mean value of   and   type of sound features.     and    \n",
      "are the mean value of   and   type of sound features . Then \n",
      "the subsequent dimensionality reduction [25] method \n",
      "follows once the feature selection process is complete. \n",
      "High data variance is present in the extracted features \n",
      "containing more information. Dimension reduction \n",
      "techniques reduce the dimensions from extracted feature \n",
      "vectors.  \n",
      " \n",
      "5-5-1- Principal Component Analysis  (PCA)  \n",
      "PCA [25] is an approach for redu cing the dimensionality \n",
      "of extensive datasets, transforming them into more \n",
      "compact representations while retaining crucial \n",
      "information . These reductions in the number of variables \n",
      "help to get more accurate results. A reduced dataset makes \n",
      "it easier and fas ter to visualize and analyze the data. \n",
      "Following steps followed to explore PCA .  \n",
      " Standardization  creates a normalized dataset \n",
      "when there is a significant difference in the range \n",
      "of initial variables or a larger range of datasets \n",
      "dominates the smaller range. It can be done by  \n",
      "             \n",
      "                     (10) \n",
      " Covariance matrix computation  helps  to know  \n",
      "how the input  dataset  varies  from  the mean value \n",
      "to each data point.  \n",
      " Eigenvectors and eigenvalues  of the covariance \n",
      "matrix help to identify the principal component.  \n",
      "5-6- Modelling  \n",
      "This implementation uses two deep l earning models. \n",
      "Bidirectional Long Short -Term Memory (BiLSTM) \n",
      "classifies the genders in whispered speech, and Deep \n",
      "Convolutional Neural Network (DCNN) identifies \n",
      "emotions.  \n",
      "BiLSTM  [26] combines  two Recurrent Neural Networks \n",
      "(RNN) that are placed independently and can traverse \n",
      "backward and forward directions at each time step, as \n",
      "shown in Fig 7. There are two ways to deal with data in \n",
      "this model - The first involves processing data from the \n",
      "past to the future , and the second operates in the opposite \n",
      "direction, from future  to past , where two hidden layers of \n",
      "neurons help to preserve the information in both \n",
      "directions. This approach helps to improve the information \n",
      "available in the algorithm.  \n",
      "For B iLSTM, the sequence of inputs is \n",
      "                   for a traditional RNN, which \n",
      "computes the hidden vector sequences \n",
      "                  and results in the output \n",
      "sequences                    for the iteration \n",
      "    to                            (11) \n",
      "                (12) \n",
      "  are the weight matrices;   is the bias vector, and   is \n",
      "the hidden layer function.  \n",
      "DCNN  [27] is a Convolutional Neural Network (CNN) \n",
      "type that helps to identify emotions from whispered \n",
      "speech . The input to the model is the speech data that \n",
      "traverses through many stages, such as the convolution \n",
      "layer, the pooling layer, Activation, Fully Connected, \n",
      "Batch normalization, and dropout layers, as shown in Fig \n",
      "8.  \n",
      " \n",
      " \n",
      " \n",
      "Fig 7. Structure of BiLSTM  \n",
      " \n",
      "To avoid overfitting, Leaky ReLU is used as an activation \n",
      "function in this implementation and evaluated as  \n",
      "{                      \n",
      "                        (13) \n",
      "The fully connected layer is a loss layer, measuring the \n",
      "inconsistency between the desired and actual outputs . Root \n",
      "The Mean Squared Propagation (RMSProp) optimization \n",
      "algorithm enhances the loss function, which varies in \n",
      "vertical directions.   \n",
      " \n",
      "Fig. 8 Structure of Deep Convolution Neural Network  \n",
      "5-7- Emotions  \n",
      "Deep learning techniques are applied to identify the \n",
      "emotions of human speech. Discrete Emotion [3] \n",
      "theory uses seven vital emotions in Human activities.  \n",
      " Sadness:  This is the feeling of dissatisfaction, \n",
      "sorrow, or fruitlessness.  \n",
      " Happiness:  This is the emotional state that elicits \n",
      "satisfaction and pleasure.  \n",
      " Fear:  This feeling triggers a fright response.  \n",
      " Anger:  This emotional state leads to feelings of \n",
      "hostility and frustration.  \n",
      " Surprise:  It is the state of mind that expresses \n",
      "either positive or negative following something \n",
      "unexpected.  \n",
      "\n",
      " \n",
      "Mohanty, & Cherukuri, Whispered Speech Emotion  Recognition with Gender Detection using BiLSTM and DCNN  \n",
      " \n",
      " \n",
      " \n",
      "158 \n",
      " Disgust:  A strong emotion that results in feeling \n",
      "repulsed.  \n",
      " Neutral:  This is the feeling of lack of particular \n",
      "preference.  \n",
      " \n",
      "5-8- Algorithm for Emotion Recognition  \n",
      "The algorithm to detect emotions from the whispered \n",
      "speech is in two parts  \n",
      " Gender Detection  \n",
      " Emotion Identification   \n",
      " \n",
      "Algorithm 1:  Geneder Detection  \n",
      "INPUT: Text  File (Whispered Speech Samples)  \n",
      "OUTPUT: Emotions (Sad, Happy, Fear, Anger, Surprise, \n",
      "Disgust, Neutral)  \n",
      " \n",
      "1: Begin:  \n",
      "2: Read speech  samples from Corpus   \n",
      "3: Pre-processing  the speech  samples:  \n",
      "4: Processed_speech=Pre_processing (Speech_Sample)   \n",
      "5: Extracted  SDC,  Pitch  features:  \n",
      "6: SDC, Pitch=Feature_Extraction(Processed_speech)   \n",
      "7: Multifeature  fusion:  \n",
      "8: multi _feature = multifeatured _fusion(SDC,  Pitch)   \n",
      "9: Dimension  reduction:  \n",
      "10: Dimension _R = Dimension _Reduction(multi _feature)  \n",
      "11: Determine  gender:  \n",
      "12: Gender = BiLSTM  (Dimension _R) \n",
      "13: Placed  in folders:  \n",
      "14: Gender=  (Male,  Female)   \n",
      "15: End:  \n",
      "Algorithm 2 Emotion Identification  \n",
      " \n",
      "1:  Begin:   \n",
      "2:  Speech Samples = (Male,  Female)  \n",
      "3:  Data Augmentation:  \n",
      "4:  Augemented _samples = Data Augmentation  (Speech          \n",
      "Samples)  \n",
      "5:  Pre-processed  the speech  samples:   \n",
      "6:  Processed _speech=Preprocessing  (Augemented _sam \n",
      "ples)  \n",
      "7:  Extracted  Spectral  Centroid,  ChromaSTFT,  MFCC,  \n",
      "Mel spectrogram  features:   \n",
      "8:  Spectral _Centroid,  Chroma _STFT,  MFCC,  \n",
      "Mel_spectrogram = Feature Extraction  (Processed _spe \n",
      "ech)  \n",
      "9:  Multifeature  fusion:   \n",
      "10:  multi _feature = \n",
      "multifeatured _fusion(Spectral _Centroid,  Chroma _STFT,  \n",
      "MFCC,  Mel_spectrogr am) \n",
      "11:  Dimension  reduction:  \n",
      "12:  Dimension_R = Dimension_Reduction(multi_feature)  \n",
      "13:  Emotion  detection:  14:  Emotions = DCNN(Dimension_R)   \n",
      "15:  OUTPUT Emotions  (Sad, Happy, Fear, Anger, \n",
      "Surprise, Disgust, Neutral)  \n",
      "16:  End:  \n",
      "6- Experiment and Result Analysis  \n",
      "The proposed SER has been implemented utilizing Python \n",
      "programming language and fortified by various machine \n",
      "learning libraries and additional supporting libraries . The \n",
      "experiment uses Python (Python 3.6.3rc1) and Librosa \n",
      "(Librosa 0.8.0) for audio processing. Graph plotting uses \n",
      "Seaborn and Matplotlib libraries, which help to analyze the \n",
      "speech data. BiLSTM and DCNN models implement the \n",
      "model using Keras (Keras - 2.6.0), TensorFlow \n",
      "(TensorFlow -2.6.0), and Scikit -learn libraries .  \n",
      "wTIMIT dataset is collected and divided into the train, \n",
      "cross -validation, and test samples. Subsequently, the \n",
      "speech processing takes place during implementation, \n",
      "incorporating data -augmentation techniques like shifting \n",
      "time, changing pitch, and changing speed . The pre -\n",
      "processing of speech content from each sample helps to \n",
      "analyze it and extract its features. The dimensionally \n",
      "reduced extracted features align with the data points in the \n",
      "BiLSTM model. The training dataset makes the model \n",
      "learn the features or data. Epochs continuously learn \n",
      "hidden features when the dataset completes backward and \n",
      "forward iterations. Then the cross -validation dataset is \n",
      "used to estimate the model’s performance on each epoch \n",
      "and prevent the model from being overfit ted. The test \n",
      "dataset helps to evaluate the trained model unbiasedly \n",
      "using performance metrics like precision, recall, f1 score, \n",
      "and confusion Matrix .  \n",
      "The parameters used in BiLSTM models are as in Table 1.  \n",
      "Table 1: Parameters of the BiLSTM model  \n",
      "Layers  Shape of output  Parameters#  \n",
      "Embedding  \n",
      "(Embedding)  (None, 216, 256)  \n",
      " 524544  \n",
      " \n",
      "Bidirectional  \n",
      " (None, 128)  \n",
      " 98816  \n",
      "batch normalization  (None, 128)  \n",
      " 512 \n",
      "Dense (Dense)  (None, 128)  \n",
      " 10512  \n",
      "dropout (Dropout)  (None, 128)  \n",
      " 0 \n",
      "flatten (Flatten)  (None, 128)  \n",
      " 1024  \n",
      "dense1(Dense)  \n",
      " (None, 12)  \n",
      " 0 \n",
      "dense2(Dense)  \n",
      " (None, 12)  \n",
      " 455 \n",
      " \n",
      " \n",
      "Journal of Information Systems and Telecommunication, Vol.12, No.2,  April -June 2024  \n",
      " \n",
      " \n",
      "159 \n",
      "The BiLSTM model helps to segregate the speech samples \n",
      "into male and female. The segregated speech samples \n",
      "move to their respective male and female folders.  \n",
      "Table 2: Parameters of the DCNN model  \n",
      "Layers  Shape of output  Parameters#  \n",
      "conv1d (Conv1D)  \n",
      " (None, 216, 256)  \n",
      " 2304  \n",
      "Activation  \n",
      "(Activation)  (None, 216, 256)  \n",
      " 0 \n",
      "conv1d 1 (Conv1D)  \n",
      " (None, 216, 256)  524544  \n",
      "batch normalization  \n",
      " (None, 216, 256)  1024 \n",
      "activation 1  \n",
      "(Activation)  \n",
      " (None, 216, 256) 0 \n",
      "dropout (Dropout)  (None, 216, 256)  \n",
      " 0 \n",
      "max pooling1d \n",
      "(MaxPooling1D)  \n",
      " (None, 13, 256)  \n",
      " 0 \n",
      "conv1d 2 (Conv1D)  \n",
      " (None, 13, 128)  \n",
      " 262272  \n",
      " \n",
      "activation 2 \n",
      "(Activation)  \n",
      " (None, 13, 128)  \n",
      " 0 \n",
      "conv1d 3 (Conv1D)  \n",
      " (None, 13, 128)  \n",
      " 131200  \n",
      " \n",
      "activation 3 \n",
      "(Activation)  \n",
      " (None, 13, 128)  \n",
      " 0 \n",
      "conv1d 4 (Conv1D)  \n",
      " (None, 13, 128)  \n",
      " 131200  \n",
      " \n",
      "batch normalization  \n",
      " (None, 13, 128)  \n",
      " 512 \n",
      "activation 4 \n",
      "(Activation)  \n",
      " (None, 13, 128)  \n",
      " 0 \n",
      "conv1d 5 (Conv1D)  \n",
      " (None, 13, 128)  \n",
      " 131200  \n",
      " \n",
      "batch normalization \n",
      "1 \n",
      " (None, 13, 128)  \n",
      " 512 \n",
      "activation 5 \n",
      "(Activation)  \n",
      " (None, 13, 128)  \n",
      " 0 \n",
      "dropout 1 (Dropout)  \n",
      " (None, 13, 128)  \n",
      " 0 \n",
      "max pooling1d 1 \n",
      "(MaxPooling1D)  \n",
      " (None, 1, 128)  \n",
      " 0 \n",
      "conv1d 6 (Conv1D)  \n",
      " (None, 1, 64)  \n",
      " 65600  \n",
      "activation 6 \n",
      "(Activation)  \n",
      " (None, 1, 64)  \n",
      " 0 \n",
      "conv1d 7 (Conv1D)  \n",
      " (None, 1, 64)  \n",
      " 32832  \n",
      "activation 7 \n",
      "(Activation)  \n",
      " (None, 1, 64)  \n",
      " 0 \n",
      "flatten (Flatten)  \n",
      " (None, 64)  \n",
      " 0 dense (Dense)  \n",
      " (None, 14)  \n",
      " 910 \n",
      "activation 8 \n",
      "(Activation)  \n",
      " (None, 14)  \n",
      " 0 \n",
      " \n",
      "After separating the speech samples into genders, Chroma \n",
      "STFT, Spectral centroid, Mel-scale spectrogram, and \n",
      "Spectral Flux feature s extraction happen ed. Then the \n",
      "created data points are dimensionally reduced to fit into \n",
      "the DCNN deep learning model to identify emotions. \n",
      "Metrics like precision, recall, F1 score, and the confusion \n",
      "matrix aid in assessing the model's performance.  Table 2 \n",
      "mentions the parameters of the DCNN model.  \n",
      "6-1- Result Analysis  \n",
      "The fusion of Fundamental frequency and SDC \n",
      "detect s the gender of speech samples. The initial \n",
      "stage involves pre -processing the speech samples, \n",
      "followed by feature extraction, which includes SDC \n",
      "and fundamental frequencies. The combination of \n",
      "SDC and Fundamental speech features create a \n",
      "multifeature fusion resulting in a single set of data \n",
      "points. The next step is to reduce the dimensions of \n",
      "data p oints to use them as input to the BiLSTM \n",
      "Model. The BiLSTM model predicts the data in the \n",
      "dataset as male and female, placed in separate \n",
      "folders. Of these, 3342 speech samples are available, \n",
      "and 3294 are correctly classified. So, the accuracy of \n",
      "the model prediction is 99.59%. The precision, recall, \n",
      "and f1 -score values, along with the confusion matrix, \n",
      "are shown in Fig s 9 and 10. The predicted model \n",
      "identifies the female speech samples more accurately \n",
      "than the male.   \n",
      " \n",
      "Fig. 9 Confusion Matrix for Gender detection by BiLSTM.  \n",
      " \n",
      "The graph in Fig 9 shows the gender detection from the \n",
      "speech sample. The X -axis represents the true label, and \n",
      "the Y-axis shows the predicted label.  This graph gives the \n",
      "\n",
      " \n",
      "Mohanty, & Cherukuri, Whispered Speech Emotion  Recognition with Gender Detection using BiLSTM and DCNN  \n",
      " \n",
      " \n",
      " \n",
      "160 \n",
      "count of detected gender spe ech that the Bi -LSTM model \n",
      "correctly and incorrectly detects.  \n",
      " \n",
      "Fig. 10 Accuracy measures of Gender Detection.  \n",
      " \n",
      "2031 female and 1263 male speech are correctly detected, \n",
      "whereas 13 females and 35 males are incorrectly detected. \n",
      "Pitch values identify the emotions after detecting genders \n",
      "from the speech samples.  \n",
      "The extraction of MFCC, Mel -scale spectrogram, Chroma \n",
      "STFT, Spectral Flux, and Spectral Centroid speech \n",
      "features happens from speech samples. Then, all five \n",
      "speech features are fused into a single feature and \n",
      "dimensionally reduced. Finally, the DCNN model is \n",
      "applied to predict emotions.  \n",
      "Individual emotions are detected based on gender as \n",
      "shown in Fig 11. The deviation in the male speech \n",
      "emotions is less than the female . Female fear and female \n",
      "neutral  emotions show divergent results from other \n",
      "emotions. The accuracy of the model is 98.54%.   \n",
      " \n",
      " \n",
      "Fig.11 Emotions based on gender.  \n",
      "6-2- Comparison Analysis  \n",
      "This experiment evaluates the current implementation \n",
      "using the wTIMIT  dataset, resulting in an impressive \n",
      "accuracy of 98.54%. The performance compares with \n",
      "three -layered Long short -term memory Bidirectional \n",
      "RNNs (LSTM BiRNN) that use No -Attention, Feed \n",
      "Forward Attention (FFA), and Improved Feed Forward \n",
      "Attention Mechanism ( IFFA) to evaluate the emotions in \n",
      "the dataset. This model uses 35 hidden states to train the dataset. The models are trained and validated with the help \n",
      "of the wTIMIT dataset [28] and tested with the help of the \n",
      "CHAINS dataset,  as shown in Table 3. This ex periment is \n",
      "a new hypothesis for  emotion detection with gender \n",
      "identification on whispered speech.  Hence, there are fewer \n",
      "references available for similar investigations.  \n",
      "Table 3: Comparison of the Implementation  \n",
      "  Sl# Model Implementation  Accuracy  \n",
      "1 FFA, IFFA, LSTM, Bi_RNN [2 8] 97.6 %  \n",
      "2 Proposed Model  98.54 %  \n",
      "The proposed method improves the performance of \n",
      "emotions after segregating the speech samples into genders.  \n",
      "By separating the approach into two models, this \n",
      "implementation excels in capturing natural and \n",
      "spontaneous expressions of emotions with low latency \n",
      "manner .  \n",
      "7- Conclusion  and Future Work  \n",
      "Identification of gender from whispered speech and \n",
      "recognizing emotions is a complicated task. This \n",
      "implementation initially detects the gender from the \n",
      "speech samples before identifying the emotions. Emotions \n",
      "might vary based on gender in the same situation. Speech \n",
      "features such as Fu ndamental Frequency and SDC help \n",
      "with gender identification. MFCC, Mel -scale spectrogram, \n",
      "Spectral flux, Spectral Centroid, and Chroma STFT play a \n",
      "vital role in detecting emotions. Multifeature fusion helps \n",
      "to combine speech features into a single set of d ata points . \n",
      "The concept verifies with the publicly available dataset \n",
      "wTIMIT with an accuracy of 98.54%. This approach helps \n",
      "to identify nearly inaudible emotions and is used to figure \n",
      "out the strategy. The proposed methodology can be \n",
      "improved using other s peech features, machine learning, \n",
      "and deep learning concepts.  \n",
      " \n",
      "References  \n",
      "[1]  ST Jovicic , and Z  Saric , ―Acoustic analysis of  consonants in \n",
      "whispered speech,‖ Journal of voice , vol 22, no. 3, pp. 263–\n",
      "274, 2008 . \n",
      "[2]  M Kumari , and I  Ali, ―An efficient algorithm for gender \n",
      "detection using voice samples,‖ 2015 Communication, \n",
      "Control and Intelligent Systems (CCIS),  2015 , Mathura, \n",
      "Utter Pradesh,  pp. 221–226, doi: \n",
      "10.1109/CCIntelS.20 15.7437912 . \n",
      "[3] S Motamed , S Setayeshi, A Rabiee , and A Sharifi, ―Speech \n",
      "Emotion Recognition Based on Fusion Method,‖ Journal of \n",
      "Information Systems and Telecommunication (JIST), vol. 3, \n",
      "pp. 50--56, 2017, doi: 10.7508/jist.2017.17.007 .  \n",
      "[4] JS Li, CC Huang , ST Sheu , and MW  Lin, ―Speech emotion \n",
      "recognition and its applications,‖ Proc. of Taiwan Institute of \n",
      "Kansei Conference , 2010  Paris , France , pp. 187–192.  \n",
      "\n",
      " \n",
      "Journal of Information Systems and Telecommunication, Vol.12, No.2,  April -June 2024  \n",
      " \n",
      " \n",
      "161 \n",
      "[5] A Guerrieri , E Braccili , F Sgro , and GN Meldolesi  ―Gender \n",
      "identification in a two -level hierarchical speech emotion \n",
      "recognition system for an Italian Social Robot,‖ Sensors, vol. \n",
      "22, no. 5, pp. 1714, 2022 , doi: 10.3390/s22051714 . \n",
      "[6] M Sarria -Paja, TH Falk, and D  O’Shaughnessy , ―Whispered \n",
      "speaker verification and gender detection using weighted \n",
      "instantaneous frequencies,‖ 2013 IEEE International \n",
      "Conference on Acoustics, Speech and Signal Processing , \n",
      "2013  (May 26 -31), Vancouver , Canada, pp. 7209 –7213 , doi: \n",
      "10.1109/ICASSP.2013.6639062 .  \n",
      "[7] J Deng , S Fruhholz, Z Zhang , and Bojrn Schuller , \n",
      "―Recognizing emotions from whispered speech based on \n",
      "acoustic feature transfer learning,‖ IEEE Access , vol.  5, pp. \n",
      "5235 –5246 , 2017 .  \n",
      "[8] M Cotescu , T Drugman, G Huybrechts, J Lorenzo -Trueba, \n",
      "and A Moinet , ―Voice conversion for whispered speech \n",
      "synthesis,‖  IEEE Signal Processing Letters , vol. 27, pp. 186–\n",
      "190, 2019 , doi: 10.1109/LSP.2019.2961213 .  \n",
      "[9] P Mishra , and R Sharma, ―Gender differentiated \n",
      "convolutional neural networks for speech emotion \n",
      "recognition,‖ 2020 12th International Congress on Ultra -\n",
      "Modern Telecommuni - cations and Control Systems and \n",
      "Workshops (ICUMT),  2020  (October 5 -7), Brno , Czech \n",
      "Republic,  pp. 142–148, doi: \n",
      "10.1109/ICUMT51630.2020.9222412 . \n",
      "[10] Mustaqeem S Kwon, ―Optimal feature selection based \n",
      "speech emotion recognition using two -stream deep \n",
      "convolutional neural network,‖ International Journal of \n",
      "Intelligent Systems , vol. 36, no. 9, pp. 5116 – 5135 , 2021 , \n",
      "doi:  10.1002/int.22505 . \n",
      "[11] J. Ancilin and A. Milton, ―Improved speech emotion \n",
      "recognition with mel frequency magnitude coefficient ,‖ \n",
      "Applied Acoustics , vol. 179, pp. 108046 , 2021 , doi: \n",
      "10.1016/j.apacoust.2021.108046 .  \n",
      "[12] S. Jothimani and K. Premalatha, ―Mff -saug: Multi feature \n",
      "fusion with spectrogram augmentatio n of speech emotion \n",
      "recognition using convolution neural network ,‖ Chaos, \n",
      "Solitons & Fractals , vol. 162, pp. 112512 , 2022 , doi: \n",
      "10.1016/j.chaos.2022.112512 .  \n",
      "[13] B Yalamanchili, SK Samayamantula, and KR Anne, ―Neural \n",
      "network -based blended ensemble learning for speech \n",
      "emotion recognition,‖ Multidimensional Systems and Signal \n",
      "Processing, vol. 33, no. 4, pp. 1323 --1348 , 2022 , doi: \n",
      "10.1007/s11045 -022-00845 -9.  \n",
      "[14] T Feng , R Hebbar , and S Narayanan, ―Trustser: On the \n",
      "trustworthiness of fine -tuning pre -trained speech embeddings \n",
      "for speech emotion recognition,‖ arXiv preprint \n",
      "arXiv:2305.11229, 2023 , doi:  10.48550/arXiv.2305.11229  \n",
      "[15] RV Darekar and M Chavan , S Sharanyaa,  and NR Ranjan , \n",
      "―A hybrid meta -heuristic ensemble based classification \n",
      "technique speech emotion recognition,‖ Advances in \n",
      "Engineering Software, vol. 180, pp. 103412 , 2023 . \n",
      "[16] J Rekimot o, ―Dualvoice: A speech interaction method using \n",
      "whisper -voice as commands,‖ CHI Conference on Human \n",
      "Factors in Computing Systems Extended Abstracts,  pp. 1–6, \n",
      "2022 , doi: 10.1145/3491101.3519700 . \n",
      "[17]H Dolka,  AX VM , and S Juliet,―  Speech  emotion r ecognition  \n",
      "using  ANN on MFCC  features,‖ 2021 3rd international \n",
      "conference on signal processing and communication (ICPSC) , \n",
      "2021, Coimbatore, India, pp. 431–435, doi: \n",
      "10.1109/ICSPC51351.2021.9451810 . [18] MK Reddy and KS Rao, ―Robust pitch extraction method \n",
      "for the hmm -based speech synthesis system,‖ IEEE signal \n",
      "processing letters , vol.  24, no. 8, pp. 1133 –1137 , 2017 , doi: \n",
      "10.1109/LSP.2017. 2712646 .  \n",
      "[19] J Chatterjee, V Mukesh, HH Hsu , G Vyas , and Z Liu , \n",
      "―Speech emotion recognition using cross - correlation and \n",
      "acoustic features,‖ 2018 IEEE 16th Intl Conf on Dependable, \n",
      "Autonomic and Secure Computing, 16th Intl Conf on \n",
      "Pervasive Intelligence and Computing, 4th Intl Conf on Big \n",
      "Data Intelligence and Computing and Cyber Science and \n",
      "Technology Congress , 2018  (August 12-15), Athens, Greece , \n",
      "pp. 243–249, doi: 10.1109/DASC/PiCom/DataCom/CyberSci  \n",
      " Tec.2018.00050 .  \n",
      "[20] S Rajesh and NJ Nalini, ―Musical instrument emotion \n",
      "recognition using deep recurrent neural network,‖ Procedia \n",
      "Computer Science, vol. 167, pp. 16 --25, 2020, doi: \n",
      "10.1016/j.procs.2020.03.178 . \n",
      "[21] M Aly , and NS Alotaibi, ―A novel deep learning model to \n",
      "detect covid -19 based on wavelet features extracted from \n",
      "mel- scale spectrogram of patients’ cough and breathing \n",
      "sounds,‖ Informatics in Medicine Unlocked , vol.  32, pp.  \n",
      "101049 , 2022, doi: 10.1016/j.imu.2022.101049 . \n",
      "[22] Z Qawaqneh,  AA Mallouh,  and BD Barkana, ― Age and \n",
      "gender  classification  from  speech and face images by jointly \n",
      "fine-tuned deep neural networks,‖ Expert Systems with \n",
      "Applications , vol. 85, pp. 76–86, 2017 , doi: \n",
      "10.1016/j.eswa.2017.05.037 .  \n",
      "[23] A Koduru, HB Valiveti and AK Budati, ―Feature extraction \n",
      "algorithms to improve the speech emotion recognition rate,‖ \n",
      "International Journal of Speech Technology , vol. 23, no. 1, \n",
      "pp. 45–55, 2020 , doi: 10.1007/s10772 -020-09672 -4.  \n",
      "[24] S Zhang , and C Li, ―Research on feature fusion speech \n",
      "emotion recognition technology for smart teaching,‖ Mobile \n",
      "Information Systems , vol. 2022 , 2022 , doi:  \n",
      "10.1155/2022/7785929 . \n",
      "[25] GB, Prasanna, SV Bhat, C Naik , and HN Champa, ―An \n",
      "Efficient Method for Handwritten Kannada Digit \n",
      "Recognition based on PCA and SVM Classifier,‖ Journal of \n",
      "Information Systems and Telecommunication (JIST), vol. 3, \n",
      "no. 35, pp. 169 2021, doi: 20.1001.1.23221437.2021.9.35.3.2 . \n",
      "[26] A Graves, N Jaitly, and A Mohamed, ―Hybrid speech \n",
      "recognition wit h deep bidirectional lstm,‖ 2013 IEEE \n",
      "workshop on automatic speech recognition and \n",
      "understanding , 2013, Olomouc, Czech Republic, pp.  273–\n",
      "278, doi: 10.1109/ASRU.2013.6707742 .  \n",
      "[27] N Aloysius and M Geetha, ―A review on deep convolutional \n",
      "neural networks,‖ 2017 international conference on \n",
      "communication and signal processing (ICCSP), 2017, pp. \n",
      "0588 –0592 , IEEE, doi: 10.1109/ICCSP.2017.8286426 .  \n",
      "[28] SBC Gutha, MAB Shaik, T Udayakumar , and AA \n",
      "Saunshikhar , ―Improved feed forward attention mechanism in \n",
      "bidirectional recurrent neural networks for robust sequence \n",
      "classification,‖ 2020 International Conference on Signal \n",
      "Processing and Communications (SPCOM) , 2020 , IISc, \n",
      "Bangalore.  IEEE , pp. 1—5, doi: \n",
      "0.9179606 10.1109/SPCOM50965.202  \n",
      "\n",
      "\n",
      "Reading: d:\\PythonCode\\document_portal\\notebook\\data\\Improving_Speaker_Gender_Detection_by_Combining_Pitch_and_SDC__1_.pdf\n",
      " \n",
      " \n",
      " \n",
      "Journal of Information Systems and Telecommunication  \n",
      "Vol.12, No.2,  April -June 2024, 152-161 \n",
      " \n",
      "http://jist.acecr.org  \n",
      "ISSN 2322 -1437  / EISSN:2345 -2773  \n",
      " \n",
      "Whispered Speech Emotion Recognition with Gender Detection using \n",
      "BiLSTM and DCNN  \n",
      "Aniruddha Mohanty1*, Ravindranath C Cherukuri1 \n",
      " \n",
      "1.Department of Computer  science and Engineering, Christ (Deemed to be University), Bangalore, India  \n",
      " \n",
      " \n",
      "Received: 22 Aug 2023 / Revised: 04 Apr 2024 / Accepted: 21 May 2024  \n",
      " \n",
      " \n",
      "Abstract   \n",
      "Emotions are human mental states at a particular instance in time concerning one’s circumstances, mood, and relationships \n",
      "with others . Identifying emotions from the whispered speech is complicated as the conversation might be confidential. The \n",
      "representation of the speech relies on the magnitude of its information. Whispered speech is intelligible, a low -intensi ty \n",
      "signal, and varies from normal speech. Emotion identification is quite tricky from whispered speech. Both prosodic and \n",
      "spectral speech features help to identify emotions. The emotion identification in a whispered speech happens using prosodic \n",
      "speech fea tures such as zero -crossing rate  (ZCR) , pitch, and spectral features that include spectral centroid, chroma STFT, \n",
      "Mel scale spectrogram, Mel -frequency cepstral coefficient (MFCC), Shifted Delta Cepstrum (SDC), and Spectral Flux. \n",
      "There are two parts to the proposed implementation. Bidirectional Long  Short -Term Memory (BiLSTM) helps to identify \n",
      "the gender from the speech sample in the first step with SDC and pitch. The Deep Convolutional Neural Network (DCNN) \n",
      "model helps to identify the emotions in the second  step. This implementation is evaluated using the  wTIMIT data corpus \n",
      "and gives 98.54% accuracy. Emotions have a dynamic effect on genders, so this implementation performs better than \n",
      "traditional approaches. This approach helps to design online learning management systems, different applications for \n",
      "mobile devices, checking cyber -criminal activities, emotion detection for older people, automatic speaker identification and \n",
      "authentication, forensics, and surveillance . \n",
      " \n",
      "Keywords:  Whispered Speech; Emotion Re cognition; Speech Features; Data Corpus; BiLSTM;  DCNN . \n",
      " \n",
      "1- Introduction  \n",
      "Emotions are the humans’ short -lived feeling s that affects \n",
      "thinking, actions, relationships, and social interactions. \n",
      "Emotions express humans’ physiological and emotional \n",
      "states with facial expressions and body language. \n",
      "Whispered speech is a form of speec h that expresses \n",
      "emotions. Whispered speech is a type of communication \n",
      "produced with breath without any noise and excitation of \n",
      "voice. The whispered speech structure changes \n",
      "significantly because of the lack of periodic excitation in \n",
      "the voice. This result s in missing speech and reduced \n",
      "transparency in communication.  \n",
      "The difference between normal and whispered speech is \n",
      "the absence of vocal tract vibrations due to the vocal \n",
      "tract’s physiological blocking. The strength of whispered \n",
      "speech is minute and with out voice compared to phoned \n",
      "speech. The spectral and prosodic features help to detect \n",
      "the whispered speech. Prosodic features of speech vary \n",
      "over time. Spectral features of whispered speech are highly accurate over unvoiced consonants, voiced consonants, \n",
      "and formants in vowels [1]. \n",
      "The impacts on Speech Emotion Recognition ( SER) are \n",
      "due to various acoustic conditions such as compressed, \n",
      "noisy, telephonic conversation s and imitator speeches. \n",
      "Other environmental conditions,  such as stress, rhythm, \n",
      "and intonation, can affect  SER.  Whispered speech is also \n",
      "affected by similar acoustic and environmental conditions. \n",
      "SER depends on acoustic characteristics and gender in \n",
      "some scenarios. Hence gender plays a vital role in SER.  \n",
      "Nowadays, several whispered speech  samples of male and \n",
      "female voices are available. The effectiveness of the SER \n",
      "is more when the implementation is in two parts. The \n",
      "identification of the gender [2] happens initially using \n",
      "Bidirectional Long Short -term Memory (BiLSTM)  with \n",
      "speech features . The next step is detecting emotions using \n",
      "various speech features [3] and Deep Convolutional \n",
      "Neural Network (DCNN).   \n",
      "The structure of the paper includes various sections. \n",
      "Section 2 describes Human Emotions and their \n",
      "Applications. Details of the Related Work are in Section 3. \n",
      "Section 4 describes the System Model, which is the black \n",
      "box view of the implementation. Section 5 is the Model \n",
      " \n",
      "Journal of Information Systems and Telecommunication, Vol.12, No.2,  April -June 2024  \n",
      " \n",
      " \n",
      "153 \n",
      "Design that describes the details of the speech features and \n",
      "the deep learning models used in this implementation. \n",
      "Section 6 gives the Experiment and Result Analysis. \n",
      "Section 7 brief ly discuss  the Conclusion and Future Work.   \n",
      "2- Human  Emotions  and their  Applications  \n",
      "Human emotions are the mental state caused by countless \n",
      "associated views , feeling s, behavioral replies, and the \n",
      "degree of pressure and annoyance. It is often associated \n",
      "with attitude , temperament, behavior , disposition, and \n",
      "creativi ty [4]. Emotion recognition helps to detect a \n",
      "humans ’ emotional mood , which lasts hours and days. \n",
      "Speech conveys emotions such as anger, disgust, fear, \n",
      "happiness, neutrality, sadness, and surprise. The machine \n",
      "automatically detects various emotions from speech with \n",
      "the help of different algorithms. Emotion Recognition \n",
      "helps to : \n",
      " \n",
      "•  Detect customers ’ intentions based on the teleconference. \n",
      "•  Detect cybercrime.  \n",
      "•  Students ’ attention and teachers ’ content adjustment.  \n",
      "•  Disability assistance  \n",
      "•  Customer satisfaction  \n",
      "•  Stress monitoring  \n",
      "•  Social media analysis  \n",
      "•  Suspicious activity  \n",
      "•  Human -machine interaction and so on.   \n",
      "3- Related Work  \n",
      "Over time, numerous studies have detected emotions from \n",
      "whispered speech . The vario us deep learning models \n",
      "detect emotions based on the speech features extracted \n",
      "from the collected whispered speech data corpus . The SER \n",
      "for normal and whispered speech is diverse because of \n",
      "vocal excitation. Emotions vary with many factors; gender \n",
      "is among  the most influential factors  [5]. Identifying \n",
      "gender in the first step improves emotion detection from \n",
      "whispered speech. So, the related work explored is on \n",
      "gender detection and emotion recognition from whispered \n",
      "speech.  \n",
      "MFCC obtained by the Hilbert enve lope approach and \n",
      "weighted instantaneous frequencies (WIFs) obtained by \n",
      "the coherent demodulation help to detect gender in \n",
      "whispered speech  samples [6]. There is an opportunity to \n",
      "explore gender detection in noisy speech conditions using \n",
      "these approaches.  \n",
      "Autoencoder -enabled features  in the transfer learning \n",
      "framework propose to practice  phonated data to identify \n",
      "emotions from Whispered speech [7]. The feature \n",
      "extraction is from the Geneva Whispered Emotion Corpus \n",
      "(GeWEC) and Berlin Emotional Speech Databa se (EMO -\n",
      "DB) data corpus. The acoustic features such as Mel-frequency cepstral coefficient (MFCC), root mean square \n",
      "(RMS), frame energy, zero -crossing -rate (ZCR), pitch \n",
      "frequency (F0), probability of voicing autocorrelation \n",
      "function are the inputs to evaluate the framework. \n",
      "Implementing deep learning concepts on spontaneous data \n",
      "gives more accuracy than the current framework .  \n",
      "Gender is detected to target an anonymous speaker. The \n",
      "Deep Neural Network (DNN) model is used to generalize \n",
      "gender by using the MFCC speech feature. This \n",
      "implementation is applied to the wTIMIT dataset to verify \n",
      "the gender and recognize the speaker  [8]. The DNN model \n",
      "cannot generalize gender; evaluation can happen with \n",
      "other datasets.   \n",
      "MFCC and CNN, with the fully  connected network, detect \n",
      "gender and emotions like anger, disgust, fear, happiness, \n",
      "sadness, surprise, and a neutral state  [9]. The concept \n",
      "verification happens on RAVDESS, CREMA -D, SAVEE, \n",
      "and TESS datasets, having an accuracy of 92.283%, which \n",
      "is better than the tr aditional model.  \n",
      "The final prediction of the implemented model is to learn \n",
      "the mutually spatial -spectral features happens by a two -\n",
      "stream deep convolutional neural network with an iterative \n",
      "neighborhood  component analysis (INCA) and the most \n",
      "discriminatory optimal features  [10]. The concept \n",
      "verification happens on EMO -DB, SAVEE, and \n",
      "RAVDESS emotional speech corpora which perform with \n",
      "95%, 82%, and 85% accuracy rates. Real -time applications \n",
      "with natural and huge data corpora can help to extend this \n",
      "concept to identify emotions.  \n",
      "Mel Frequency Magnitude Coefficient (MFMC) and three \n",
      "spectral features, namely MFCC, log frequency power \n",
      "coefficient, and linear prediction cepstral coefficient, are \n",
      "used with the help of Support Vector Machine (SVM) \n",
      "modeling  [11]. The performance evaluation uses this \n",
      "concept  on Berlin, RAVDESS, SAVEE, EMOVO, and \n",
      "eNTERFACE data corpus . Feature selection, feature \n",
      "fusion, and multiple classification schemes improve the \n",
      "performance of MFMC.  \n",
      "The proposed MFF -SAug research in  which noise removal \n",
      "improves emotion. White Noise Injection, Pitch Tuning \n",
      "techniques, MFCC, ZCR, RMS speech features, and \n",
      "Convolutional Neural Network (CNN) modeling [12] \n",
      "detect emotions. Emotion detection during the interaction \n",
      "between people can extend the MFF -SAug approach .  \n",
      "The proposed Neural Network -based Blended Ensemble \n",
      "Learning (NNBEL) model is composed of a 1 -dimensional \n",
      "Convolution Neural Network (1D -CNN), Long Short -\n",
      "Term Memory (LSTM), and CapsuleNets. LSTM receives \n",
      "input from the Log Mel -spect rogram speech features, \n",
      "while 1D -CNN and CapsuleNets receive input from the \n",
      "MFCC.  Each model’s output is fed to Multi -Layer \n",
      "Perceptron (MLP) and predicts the final emotion s [13]. \n",
      "This model shows 95.3% and 94% accuracy on \n",
      "RAVDESS and IEMOCAP datasets, respectively.  \n",
      " \n",
      "Mohanty, & Cherukuri, Whispered Speech Emotion  Recognition with Gender Detection using BiLSTM and DCNN  \n",
      " \n",
      " \n",
      " \n",
      "154 \n",
      "TrustSER [14] implemented a general framework to \n",
      "determine the SER system's trustworthiness using deep \n",
      "learning techniques. Trustworthiness evaluates privacy \n",
      "(gender in formation, speaker demographics), safety, \n",
      "fairness, sustainability, and emotions (sad, angry, happy, \n",
      "neutral). The architecture of the TrustSET framework uses \n",
      "CNN encoder and Transformer encoder models. \n",
      "Trustworthy profiles under the Federated Learning \n",
      "scenario might improve privacy, fairness, and safety.  \n",
      "A hybrid meta -heuristic ensemble -based classification [15] \n",
      "helps to detect speech emotions. Raw speech samples are \n",
      "filtered using the Butterworth filter; then, spectrogram \n",
      "speech features are extracted from each frame to create a \n",
      "hybrid feature vector. The ensemble -based classification is \n",
      "applied to hybrid feature vectors to classify em otions. The \n",
      "ensemble -based classification contains a Recurrent Neural \n",
      "Network (RNN), a Deep Belief Network (DBN), and an \n",
      "Artificial Neural Network (ANN).  \n",
      "The above -related work shows that it is crucial to identify \n",
      "gender to segregate emotions in a speech. Male and female \n",
      "emotions are different based on situations. Whispered \n",
      "speech is an essential concept of emotion recognition. \n",
      "There is an opportunity to experiment further based on \n",
      "other datasets of whispered speech. So, this is a motivation \n",
      "to work on emot ion recognition in whispered speech, as \n",
      "this is a less explored area of research .  \n",
      "4- System Model  \n",
      "The analysis of e motion recognition from the whispered \n",
      "speech segregates into two parts. One is gender detection, \n",
      "and the second is emotion detection as shown in Fig 1.  \n",
      " \n",
      " \n",
      "Fig. 1 Emotion recognition from whispered speech  \n",
      " \n",
      "As part of gender detection, pre -processing of the speech \n",
      "samples is performed, and then pitch and SDC are \n",
      "extracted as part of prosodic and spectral features, \n",
      "respectively. Both the features are fused to get a single \n",
      "feature set with the help of multifeature fusion. PCA helps \n",
      "to reduce the dimensions of extracted features and is used \n",
      "as input to Bidirectional Long short -term memory \n",
      "(BiLSTM) to classify genders , as shown in Fig 2.  \n",
      " \n",
      " \n",
      "Fig .2 Gender detection from whispered speech  \n",
      " \n",
      "Following  Gender detection, the augmentation of speech \n",
      "data helps to get more realistic data. Then the speech features such as Chromatogram, Zero -Crossing Rate, \n",
      "Spectral Central, MFCC, Spectral Flux , and Mel \n",
      "Spectrog rams are extracted. All the extracted features \n",
      "combine to get a single feature using multifeature fusion. \n",
      "Then the dimensions are reduced to get the optimal data \n",
      "points. The data points are inserted into the Deep \n",
      "Convolutional Neural Network (DCNN) to identify the \n",
      "emotions , as shown in Fig 3.  \n",
      "5- System  Design  \n",
      "The implementation of system design happens in two parts. \n",
      "The first part identifies the male and female gender from \n",
      "the whispered speech samples. Then the determination of \n",
      "different emotions like sadn ess, happiness, fear, anger, \n",
      "surprise, disgust, and neutral are detected in gender -\n",
      "segregated speech samples.  \n",
      " \n",
      "Fig. 3 Emotion Recognition from Male and Female whispered speech.  \n",
      "5-1- Whispered Speech Data Corpus  \n",
      "wTIMIT [16] is a whispered voice data corpus ha ving 450 \n",
      "phonetically balanced sentences with 29 speakers. There \n",
      "are 11,324 utterances with a normal voice , which  can be \n",
      "used for normal and whispered training. This data is \n",
      "available in two parts - train and test divisions. The \n",
      "samples contain an 8 kHz sampling rate with a high -\n",
      "quality  and low-pass filter.  \n",
      "5-2-    Data Augmentation  \n",
      "Data augmentation [17 ] is the method of getting more \n",
      "realistic data from the existing data, which helps to add \n",
      "more training data to the model, reduce overfitting, and \n",
      "increase the model’s generalization ability. Below are the \n",
      "steps to generate synthetic data.  \n",
      " Shifting Time : Shifting time is a simple concept \n",
      "where the audio shifts to the left or right with a \n",
      "haphazard second. If the audio shifts to the left \n",
      "\n",
      " \n",
      "Journal of Information Systems and Telecommunication, Vol.12, No.2,  April -June 2024  \n",
      " \n",
      " \n",
      "155 \n",
      "with z seconds, then the first z seconds are \n",
      "marked as silence. Similarly, if the audio shifts to \n",
      "the right with z second s, the last z seconds are \n",
      "marked as silence.  \n",
      " Changing Pitch: Pitch change is adjusting the \n",
      "pitch randomly without upsetting the speediness \n",
      "of the audio file.  \n",
      " Changing Speed: Speed audio changes by \n",
      "stretching the time series data by a fixed rate.  \n",
      "5-3- Pre-processing  \n",
      "After the data collection and data augmentation, pre -\n",
      "processing [3] is the initial  phase  in training SER. \n",
      "Framing, Windowing, Voice activity detection, \n",
      "Normalization, and Noise reduction are pre -\n",
      "processing steps.   \n",
      "Framing:  Speech signals are quasip eriodic and vary \n",
      "over time. Hence, information and related emotions \n",
      "also fluctuate over time. The segregation of the \n",
      "signals happens in a shorter period to make the \n",
      "speech signal invariant. Twenty milliseconds to thirty \n",
      "milliseconds helps to make the speec h signal \n",
      "invariant, and five milliseconds of overlap of the \n",
      "frames avoid data leakage between the frames .  \n",
      "Windowing:  Windowing on each frame helps to diminish \n",
      "data leakage during Fast Fourier Transformation (FFT) \n",
      "after framing. The Hamming window allows  this step \n",
      "where the window size is N for the frames     , used in \n",
      "equation (1).  \n",
      " \n",
      "                  (   \n",
      "   )       (1) \n",
      "            \n",
      " \n",
      "Voice activity detection (VAD):  An utterance has three \n",
      "portions of speech activities : voiced, unvoiced, and silent. \n",
      "Zero Crossing Rate (ZCR) speech feature helps to detect \n",
      "VAD. ZCR represents the frequency of signal transitions \n",
      "between positive and negative values within a specific \n",
      "frame . Due to high energy, the ZCR value is low for \n",
      "voiced speech and high for unvoiced s peech because of \n",
      "low energy.  \n",
      "Normalization:  Normalization helps to reduce  speaker \n",
      "and recording inconsistency without affecting the features’ \n",
      "strength and enhancing the features’ generalization \n",
      "capability. The Z-normalization method is used more and \n",
      "represented as  \n",
      "       \n",
      "      (2) \n",
      "Where   is the speech signal,   and   are the mean and \n",
      "standard deviation of the data, respectively.  \n",
      "Noise Reduction:  The environmental noise captured while \n",
      "recording a speech signal  affects the recognition rate . \n",
      "Minimum mean square error (MMSE) and log -spectral \n",
      "amplitude MMSE (LogMMSE) reduce the noise from the speech signals. Noise reduction helps to get more accuracy \n",
      "in SER evaluation.  \n",
      "5-4- Feature Extraction  \n",
      "Specific emotions are present in prosodic and spectral \n",
      "features of speech. The extraction of prosodic and spectral \n",
      "features is a vital characteristic of emotion recognition \n",
      "after pre -processing the speech signals.  \n",
      "5-4-1-Prosodic Features  \n",
      "Prosodic features deal with the audio qualities of a speech \n",
      "when connected speeches use sounds as input. The \n",
      "production of the speech deals with the amount of energy, \n",
      "frequency, period, loudness, pitch, and duration. Speech \n",
      "signal com munication depends on intonation, stress, and \n",
      "rhythm, which prosodic features can detect. The prosodic \n",
      "features used in the implementation are : \n",
      "Zero -Crossing Rate (ZCR):  ZCR [17] measures the \n",
      "frequency of signal transitions between positive and \n",
      "negative values in every audio frame  and defined as  \n",
      "    \n",
      "    ∑       [ ]         [      \n",
      "    (3) \n",
      "Where        is the sign function . \n",
      "    [     ]  {        \n",
      "       \n",
      "Fundamental Frequency (Pitch):  Fundamental \n",
      "Frequency (F0) [18] is the minimum freque ncy of the \n",
      "periodic waveform. F0 is the significant parameter to \n",
      "differentiate male speech from female speech. Pitch also \n",
      "determines the voiced and unvoiced portion of the speech \n",
      "signal. This analysis uses pitch parameters like pitch mean \n",
      "value and pitch r ange.  \n",
      "The pitch range determines the number of octaves a speech  \n",
      "sample  can cover, from the lowest to the highest. F0 value \n",
      "varies from 85Hz to 180 HZ for the voiced speech of adult \n",
      "males.  \n",
      " \n",
      " \n",
      "Fig. 4 Representation of Pitch data  \n",
      " \n",
      "Similarly, the F0 value for adult females goes from 165Hz \n",
      "to 255Hz, as shown in Fig 4.  \n",
      "\n",
      " \n",
      "Mohanty, & Cherukuri, Whispered Speech Emotion  Recognition with Gender Detection using BiLSTM and DCNN  \n",
      " \n",
      " \n",
      " \n",
      "156 \n",
      "5-4-2-Spectral features  \n",
      "The representation of spectral features happens by \n",
      "converting the time -domain speech signal into the \n",
      "frequency domain with the help of the Fast Fourier \n",
      "Transform (FFT) which  benefits  to represent the \n",
      "characteristics of the Human Vocal Tract. The  spectral \n",
      "features are  \n",
      "Spectral Centroid (SC):  The Spectral Centroid [19] is the \n",
      "most used speech feature where the positioning of the \n",
      "―center of mass‖ of the audio signal spectr um defines the \n",
      "weight of the spectrum. The center of mass measures the \n",
      "weighted average of the frequency component located in \n",
      "the audio signal, defined as  \n",
      " \n",
      "      ∑            \n",
      "   \n",
      "∑       \n",
      "       (4) \n",
      "Where      represents the magnitude of bin numb er   and \n",
      "      represents the central frequency of the bin.  \n",
      "Chroma STFT:  Chroma STFT [20] is obtained using FFT \n",
      "on speech samples and the resulting spectrums a re a \n",
      "chromatogram in a vertical axis. This feature captures the \n",
      "harmonic feature of the speech s ignals.  \n",
      "Mel-scale Spectrogram:  Mel-scale Spectrogram [ 21] is a \n",
      "spectrogram in which frequencies convert to the Mel scale, \n",
      "which helps to differentiate the range of frequencies. Mel -\n",
      "spectrogram helps to understand emotions in a better way \n",
      "as humans can perceive sound on a logarithmic scale.  \n",
      "Mel Frequency Cepstral Coefficient (MFCC ): MFCC \n",
      "[17] i s SER’s  standard feature extraction technique. The \n",
      "vocal cord s, tongue, and teeth filter the sound and make it \n",
      "unique for each speaker in the Human Speech production \n",
      "system. Mel scale represents MFCC, where the frequency \n",
      "bands are equally spaced and close to the Human Auditory \n",
      "System’s response, as shown in Fig 5.  \n",
      " \n",
      "Fig 5 MFCC speech feature extraction  \n",
      " \n",
      "Shifted Delta Cepstrum (SDC):  Time derivatives apply \n",
      "to the cepstral coefficients obtained from the MFCC and \n",
      "combined with the delta coefficient to get SDC, as shown \n",
      "in Fig 6.       and   are the four parameters used in \n",
      "SDC [ 22].  M denotes the cepstral coefficient for each \n",
      "frame.   indicates added frames.   are the frames that \n",
      "append delta features from the new feature vector .   \n",
      "represents the delta values difference. So, the coefficient \n",
      "vectors represent as   \n",
      "     [                  ]   (5) \n",
      "   are MFCC coefficients and   is the coefficient index. \n",
      "For a given time,   an intermediate calculation is done to \n",
      "obtain the   coefficients.  \n",
      "                                 (6) \n",
      "Finally, the SDC coefficients vectors of   dimensions are \n",
      "obtained as  \n",
      "       [          (                )] (7) \n",
      " \n",
      " \n",
      "Fig. 6 Extraction of SDC speech feature  \n",
      " \n",
      "This speech feature helps to identify genders for a long -\n",
      "range dynamic appearance in speech signals.  \n",
      "Spectral Flux:  Spectral flux [ 20] measures the change in \n",
      "speed of the signal’s power spectrum compared to the \n",
      "previous frame. This feature estimates the speech signal’s \n",
      "power spectrum about  the power spectrum of one frame \n",
      "with others.  \n",
      "5-5- Feature Selection and Dimension Reduction  \n",
      "Feature s election [23] is selecting features from a large set \n",
      "of extracted features to eliminate redundant and unused \n",
      "information and decrease processing time. For this work, \n",
      "the extracted pitch contains lots of information. The global \n",
      "features, like the range of p itch values and mean pitch \n",
      "values, are selected. Removing redundant and unused \n",
      "information, such as additional zeroes, duplicate values, or \n",
      "frames, is part of MFCC feature extraction. The exact \n",
      "process removes the silent speech frames.  \n",
      "Fundamental frequen cy and SDC are two speech features \n",
      "to identify gender. Similarly, Spectral centroid, Chroma \n",
      "STFT, MFCC, and Spectral flux features are used for \n",
      "emotion identification to create a single set of data points \n",
      "from multiple speech features using multifeature fu sion \n",
      "technique [24]. \n",
      "Multifeature Fusion:  The extracted speech features hav e \n",
      "different dimensions, so feature proximity usages  average \n",
      "dimensional spacing between the vectors to denote the \n",
      "proximity between the diverse features  [24]. The average \n",
      "dimensional spacing between the vectors is computed as  \n",
      "          \n",
      "    √∑∑              (8) \n",
      "   =  \n",
      "     √∑∑              (9) \n",
      "\n",
      " \n",
      "Journal of Information Systems and Telecommunication, Vol.12, No.2,  April -June 2024  \n",
      " \n",
      " \n",
      "157 \n",
      "        and     are the mean and variance interval of \n",
      "average dimensions of the feature vector s.    and    are \n",
      "the mean value of   and   type of sound features.     and    \n",
      "are the mean value of   and   type of sound features . Then \n",
      "the subsequent dimensionality reduction [25] method \n",
      "follows once the feature selection process is complete. \n",
      "High data variance is present in the extracted features \n",
      "containing more information. Dimension reduction \n",
      "techniques reduce the dimensions from extracted feature \n",
      "vectors.  \n",
      " \n",
      "5-5-1- Principal Component Analysis  (PCA)  \n",
      "PCA [25] is an approach for redu cing the dimensionality \n",
      "of extensive datasets, transforming them into more \n",
      "compact representations while retaining crucial \n",
      "information . These reductions in the number of variables \n",
      "help to get more accurate results. A reduced dataset makes \n",
      "it easier and fas ter to visualize and analyze the data. \n",
      "Following steps followed to explore PCA .  \n",
      " Standardization  creates a normalized dataset \n",
      "when there is a significant difference in the range \n",
      "of initial variables or a larger range of datasets \n",
      "dominates the smaller range. It can be done by  \n",
      "             \n",
      "                     (10) \n",
      " Covariance matrix computation  helps  to know  \n",
      "how the input  dataset  varies  from  the mean value \n",
      "to each data point.  \n",
      " Eigenvectors and eigenvalues  of the covariance \n",
      "matrix help to identify the principal component.  \n",
      "5-6- Modelling  \n",
      "This implementation uses two deep l earning models. \n",
      "Bidirectional Long Short -Term Memory (BiLSTM) \n",
      "classifies the genders in whispered speech, and Deep \n",
      "Convolutional Neural Network (DCNN) identifies \n",
      "emotions.  \n",
      "BiLSTM  [26] combines  two Recurrent Neural Networks \n",
      "(RNN) that are placed independently and can traverse \n",
      "backward and forward directions at each time step, as \n",
      "shown in Fig 7. There are two ways to deal with data in \n",
      "this model - The first involves processing data from the \n",
      "past to the future , and the second operates in the opposite \n",
      "direction, from future  to past , where two hidden layers of \n",
      "neurons help to preserve the information in both \n",
      "directions. This approach helps to improve the information \n",
      "available in the algorithm.  \n",
      "For B iLSTM, the sequence of inputs is \n",
      "                   for a traditional RNN, which \n",
      "computes the hidden vector sequences \n",
      "                  and results in the output \n",
      "sequences                    for the iteration \n",
      "    to                            (11) \n",
      "                (12) \n",
      "  are the weight matrices;   is the bias vector, and   is \n",
      "the hidden layer function.  \n",
      "DCNN  [27] is a Convolutional Neural Network (CNN) \n",
      "type that helps to identify emotions from whispered \n",
      "speech . The input to the model is the speech data that \n",
      "traverses through many stages, such as the convolution \n",
      "layer, the pooling layer, Activation, Fully Connected, \n",
      "Batch normalization, and dropout layers, as shown in Fig \n",
      "8.  \n",
      " \n",
      " \n",
      " \n",
      "Fig 7. Structure of BiLSTM  \n",
      " \n",
      "To avoid overfitting, Leaky ReLU is used as an activation \n",
      "function in this implementation and evaluated as  \n",
      "{                      \n",
      "                        (13) \n",
      "The fully connected layer is a loss layer, measuring the \n",
      "inconsistency between the desired and actual outputs . Root \n",
      "The Mean Squared Propagation (RMSProp) optimization \n",
      "algorithm enhances the loss function, which varies in \n",
      "vertical directions.   \n",
      " \n",
      "Fig. 8 Structure of Deep Convolution Neural Network  \n",
      "5-7- Emotions  \n",
      "Deep learning techniques are applied to identify the \n",
      "emotions of human speech. Discrete Emotion [3] \n",
      "theory uses seven vital emotions in Human activities.  \n",
      " Sadness:  This is the feeling of dissatisfaction, \n",
      "sorrow, or fruitlessness.  \n",
      " Happiness:  This is the emotional state that elicits \n",
      "satisfaction and pleasure.  \n",
      " Fear:  This feeling triggers a fright response.  \n",
      " Anger:  This emotional state leads to feelings of \n",
      "hostility and frustration.  \n",
      " Surprise:  It is the state of mind that expresses \n",
      "either positive or negative following something \n",
      "unexpected.  \n",
      "\n",
      " \n",
      "Mohanty, & Cherukuri, Whispered Speech Emotion  Recognition with Gender Detection using BiLSTM and DCNN  \n",
      " \n",
      " \n",
      " \n",
      "158 \n",
      " Disgust:  A strong emotion that results in feeling \n",
      "repulsed.  \n",
      " Neutral:  This is the feeling of lack of particular \n",
      "preference.  \n",
      " \n",
      "5-8- Algorithm for Emotion Recognition  \n",
      "The algorithm to detect emotions from the whispered \n",
      "speech is in two parts  \n",
      " Gender Detection  \n",
      " Emotion Identification   \n",
      " \n",
      "Algorithm 1:  Geneder Detection  \n",
      "INPUT: Text  File (Whispered Speech Samples)  \n",
      "OUTPUT: Emotions (Sad, Happy, Fear, Anger, Surprise, \n",
      "Disgust, Neutral)  \n",
      " \n",
      "1: Begin:  \n",
      "2: Read speech  samples from Corpus   \n",
      "3: Pre-processing  the speech  samples:  \n",
      "4: Processed_speech=Pre_processing (Speech_Sample)   \n",
      "5: Extracted  SDC,  Pitch  features:  \n",
      "6: SDC, Pitch=Feature_Extraction(Processed_speech)   \n",
      "7: Multifeature  fusion:  \n",
      "8: multi _feature = multifeatured _fusion(SDC,  Pitch)   \n",
      "9: Dimension  reduction:  \n",
      "10: Dimension _R = Dimension _Reduction(multi _feature)  \n",
      "11: Determine  gender:  \n",
      "12: Gender = BiLSTM  (Dimension _R) \n",
      "13: Placed  in folders:  \n",
      "14: Gender=  (Male,  Female)   \n",
      "15: End:  \n",
      "Algorithm 2 Emotion Identification  \n",
      " \n",
      "1:  Begin:   \n",
      "2:  Speech Samples = (Male,  Female)  \n",
      "3:  Data Augmentation:  \n",
      "4:  Augemented _samples = Data Augmentation  (Speech          \n",
      "Samples)  \n",
      "5:  Pre-processed  the speech  samples:   \n",
      "6:  Processed _speech=Preprocessing  (Augemented _sam \n",
      "ples)  \n",
      "7:  Extracted  Spectral  Centroid,  ChromaSTFT,  MFCC,  \n",
      "Mel spectrogram  features:   \n",
      "8:  Spectral _Centroid,  Chroma _STFT,  MFCC,  \n",
      "Mel_spectrogram = Feature Extraction  (Processed _spe \n",
      "ech)  \n",
      "9:  Multifeature  fusion:   \n",
      "10:  multi _feature = \n",
      "multifeatured _fusion(Spectral _Centroid,  Chroma _STFT,  \n",
      "MFCC,  Mel_spectrogr am) \n",
      "11:  Dimension  reduction:  \n",
      "12:  Dimension_R = Dimension_Reduction(multi_feature)  \n",
      "13:  Emotion  detection:  14:  Emotions = DCNN(Dimension_R)   \n",
      "15:  OUTPUT Emotions  (Sad, Happy, Fear, Anger, \n",
      "Surprise, Disgust, Neutral)  \n",
      "16:  End:  \n",
      "6- Experiment and Result Analysis  \n",
      "The proposed SER has been implemented utilizing Python \n",
      "programming language and fortified by various machine \n",
      "learning libraries and additional supporting libraries . The \n",
      "experiment uses Python (Python 3.6.3rc1) and Librosa \n",
      "(Librosa 0.8.0) for audio processing. Graph plotting uses \n",
      "Seaborn and Matplotlib libraries, which help to analyze the \n",
      "speech data. BiLSTM and DCNN models implement the \n",
      "model using Keras (Keras - 2.6.0), TensorFlow \n",
      "(TensorFlow -2.6.0), and Scikit -learn libraries .  \n",
      "wTIMIT dataset is collected and divided into the train, \n",
      "cross -validation, and test samples. Subsequently, the \n",
      "speech processing takes place during implementation, \n",
      "incorporating data -augmentation techniques like shifting \n",
      "time, changing pitch, and changing speed . The pre -\n",
      "processing of speech content from each sample helps to \n",
      "analyze it and extract its features. The dimensionally \n",
      "reduced extracted features align with the data points in the \n",
      "BiLSTM model. The training dataset makes the model \n",
      "learn the features or data. Epochs continuously learn \n",
      "hidden features when the dataset completes backward and \n",
      "forward iterations. Then the cross -validation dataset is \n",
      "used to estimate the model’s performance on each epoch \n",
      "and prevent the model from being overfit ted. The test \n",
      "dataset helps to evaluate the trained model unbiasedly \n",
      "using performance metrics like precision, recall, f1 score, \n",
      "and confusion Matrix .  \n",
      "The parameters used in BiLSTM models are as in Table 1.  \n",
      "Table 1: Parameters of the BiLSTM model  \n",
      "Layers  Shape of output  Parameters#  \n",
      "Embedding  \n",
      "(Embedding)  (None, 216, 256)  \n",
      " 524544  \n",
      " \n",
      "Bidirectional  \n",
      " (None, 128)  \n",
      " 98816  \n",
      "batch normalization  (None, 128)  \n",
      " 512 \n",
      "Dense (Dense)  (None, 128)  \n",
      " 10512  \n",
      "dropout (Dropout)  (None, 128)  \n",
      " 0 \n",
      "flatten (Flatten)  (None, 128)  \n",
      " 1024  \n",
      "dense1(Dense)  \n",
      " (None, 12)  \n",
      " 0 \n",
      "dense2(Dense)  \n",
      " (None, 12)  \n",
      " 455 \n",
      " \n",
      " \n",
      "Journal of Information Systems and Telecommunication, Vol.12, No.2,  April -June 2024  \n",
      " \n",
      " \n",
      "159 \n",
      "The BiLSTM model helps to segregate the speech samples \n",
      "into male and female. The segregated speech samples \n",
      "move to their respective male and female folders.  \n",
      "Table 2: Parameters of the DCNN model  \n",
      "Layers  Shape of output  Parameters#  \n",
      "conv1d (Conv1D)  \n",
      " (None, 216, 256)  \n",
      " 2304  \n",
      "Activation  \n",
      "(Activation)  (None, 216, 256)  \n",
      " 0 \n",
      "conv1d 1 (Conv1D)  \n",
      " (None, 216, 256)  524544  \n",
      "batch normalization  \n",
      " (None, 216, 256)  1024 \n",
      "activation 1  \n",
      "(Activation)  \n",
      " (None, 216, 256) 0 \n",
      "dropout (Dropout)  (None, 216, 256)  \n",
      " 0 \n",
      "max pooling1d \n",
      "(MaxPooling1D)  \n",
      " (None, 13, 256)  \n",
      " 0 \n",
      "conv1d 2 (Conv1D)  \n",
      " (None, 13, 128)  \n",
      " 262272  \n",
      " \n",
      "activation 2 \n",
      "(Activation)  \n",
      " (None, 13, 128)  \n",
      " 0 \n",
      "conv1d 3 (Conv1D)  \n",
      " (None, 13, 128)  \n",
      " 131200  \n",
      " \n",
      "activation 3 \n",
      "(Activation)  \n",
      " (None, 13, 128)  \n",
      " 0 \n",
      "conv1d 4 (Conv1D)  \n",
      " (None, 13, 128)  \n",
      " 131200  \n",
      " \n",
      "batch normalization  \n",
      " (None, 13, 128)  \n",
      " 512 \n",
      "activation 4 \n",
      "(Activation)  \n",
      " (None, 13, 128)  \n",
      " 0 \n",
      "conv1d 5 (Conv1D)  \n",
      " (None, 13, 128)  \n",
      " 131200  \n",
      " \n",
      "batch normalization \n",
      "1 \n",
      " (None, 13, 128)  \n",
      " 512 \n",
      "activation 5 \n",
      "(Activation)  \n",
      " (None, 13, 128)  \n",
      " 0 \n",
      "dropout 1 (Dropout)  \n",
      " (None, 13, 128)  \n",
      " 0 \n",
      "max pooling1d 1 \n",
      "(MaxPooling1D)  \n",
      " (None, 1, 128)  \n",
      " 0 \n",
      "conv1d 6 (Conv1D)  \n",
      " (None, 1, 64)  \n",
      " 65600  \n",
      "activation 6 \n",
      "(Activation)  \n",
      " (None, 1, 64)  \n",
      " 0 \n",
      "conv1d 7 (Conv1D)  \n",
      " (None, 1, 64)  \n",
      " 32832  \n",
      "activation 7 \n",
      "(Activation)  \n",
      " (None, 1, 64)  \n",
      " 0 \n",
      "flatten (Flatten)  \n",
      " (None, 64)  \n",
      " 0 dense (Dense)  \n",
      " (None, 14)  \n",
      " 910 \n",
      "activation 8 \n",
      "(Activation)  \n",
      " (None, 14)  \n",
      " 0 \n",
      " \n",
      "After separating the speech samples into genders, Chroma \n",
      "STFT, Spectral centroid, Mel-scale spectrogram, and \n",
      "Spectral Flux feature s extraction happen ed. Then the \n",
      "created data points are dimensionally reduced to fit into \n",
      "the DCNN deep learning model to identify emotions. \n",
      "Metrics like precision, recall, F1 score, and the confusion \n",
      "matrix aid in assessing the model's performance.  Table 2 \n",
      "mentions the parameters of the DCNN model.  \n",
      "6-1- Result Analysis  \n",
      "The fusion of Fundamental frequency and SDC \n",
      "detect s the gender of speech samples. The initial \n",
      "stage involves pre -processing the speech samples, \n",
      "followed by feature extraction, which includes SDC \n",
      "and fundamental frequencies. The combination of \n",
      "SDC and Fundamental speech features create a \n",
      "multifeature fusion resulting in a single set of data \n",
      "points. The next step is to reduce the dimensions of \n",
      "data p oints to use them as input to the BiLSTM \n",
      "Model. The BiLSTM model predicts the data in the \n",
      "dataset as male and female, placed in separate \n",
      "folders. Of these, 3342 speech samples are available, \n",
      "and 3294 are correctly classified. So, the accuracy of \n",
      "the model prediction is 99.59%. The precision, recall, \n",
      "and f1 -score values, along with the confusion matrix, \n",
      "are shown in Fig s 9 and 10. The predicted model \n",
      "identifies the female speech samples more accurately \n",
      "than the male.   \n",
      " \n",
      "Fig. 9 Confusion Matrix for Gender detection by BiLSTM.  \n",
      " \n",
      "The graph in Fig 9 shows the gender detection from the \n",
      "speech sample. The X -axis represents the true label, and \n",
      "the Y-axis shows the predicted label.  This graph gives the \n",
      "\n",
      " \n",
      "Mohanty, & Cherukuri, Whispered Speech Emotion  Recognition with Gender Detection using BiLSTM and DCNN  \n",
      " \n",
      " \n",
      " \n",
      "160 \n",
      "count of detected gender spe ech that the Bi -LSTM model \n",
      "correctly and incorrectly detects.  \n",
      " \n",
      "Fig. 10 Accuracy measures of Gender Detection.  \n",
      " \n",
      "2031 female and 1263 male speech are correctly detected, \n",
      "whereas 13 females and 35 males are incorrectly detected. \n",
      "Pitch values identify the emotions after detecting genders \n",
      "from the speech samples.  \n",
      "The extraction of MFCC, Mel -scale spectrogram, Chroma \n",
      "STFT, Spectral Flux, and Spectral Centroid speech \n",
      "features happens from speech samples. Then, all five \n",
      "speech features are fused into a single feature and \n",
      "dimensionally reduced. Finally, the DCNN model is \n",
      "applied to predict emotions.  \n",
      "Individual emotions are detected based on gender as \n",
      "shown in Fig 11. The deviation in the male speech \n",
      "emotions is less than the female . Female fear and female \n",
      "neutral  emotions show divergent results from other \n",
      "emotions. The accuracy of the model is 98.54%.   \n",
      " \n",
      " \n",
      "Fig.11 Emotions based on gender.  \n",
      "6-2- Comparison Analysis  \n",
      "This experiment evaluates the current implementation \n",
      "using the wTIMIT  dataset, resulting in an impressive \n",
      "accuracy of 98.54%. The performance compares with \n",
      "three -layered Long short -term memory Bidirectional \n",
      "RNNs (LSTM BiRNN) that use No -Attention, Feed \n",
      "Forward Attention (FFA), and Improved Feed Forward \n",
      "Attention Mechanism ( IFFA) to evaluate the emotions in \n",
      "the dataset. This model uses 35 hidden states to train the dataset. The models are trained and validated with the help \n",
      "of the wTIMIT dataset [28] and tested with the help of the \n",
      "CHAINS dataset,  as shown in Table 3. This ex periment is \n",
      "a new hypothesis for  emotion detection with gender \n",
      "identification on whispered speech.  Hence, there are fewer \n",
      "references available for similar investigations.  \n",
      "Table 3: Comparison of the Implementation  \n",
      "  Sl# Model Implementation  Accuracy  \n",
      "1 FFA, IFFA, LSTM, Bi_RNN [2 8] 97.6 %  \n",
      "2 Proposed Model  98.54 %  \n",
      "The proposed method improves the performance of \n",
      "emotions after segregating the speech samples into genders.  \n",
      "By separating the approach into two models, this \n",
      "implementation excels in capturing natural and \n",
      "spontaneous expressions of emotions with low latency \n",
      "manner .  \n",
      "7- Conclusion  and Future Work  \n",
      "Identification of gender from whispered speech and \n",
      "recognizing emotions is a complicated task. This \n",
      "implementation initially detects the gender from the \n",
      "speech samples before identifying the emotions. Emotions \n",
      "might vary based on gender in the same situation. Speech \n",
      "features such as Fu ndamental Frequency and SDC help \n",
      "with gender identification. MFCC, Mel -scale spectrogram, \n",
      "Spectral flux, Spectral Centroid, and Chroma STFT play a \n",
      "vital role in detecting emotions. Multifeature fusion helps \n",
      "to combine speech features into a single set of d ata points . \n",
      "The concept verifies with the publicly available dataset \n",
      "wTIMIT with an accuracy of 98.54%. This approach helps \n",
      "to identify nearly inaudible emotions and is used to figure \n",
      "out the strategy. The proposed methodology can be \n",
      "improved using other s peech features, machine learning, \n",
      "and deep learning concepts.  \n",
      " \n",
      "References  \n",
      "[1]  ST Jovicic , and Z  Saric , ―Acoustic analysis of  consonants in \n",
      "whispered speech,‖ Journal of voice , vol 22, no. 3, pp. 263–\n",
      "274, 2008 . \n",
      "[2]  M Kumari , and I  Ali, ―An efficient algorithm for gender \n",
      "detection using voice samples,‖ 2015 Communication, \n",
      "Control and Intelligent Systems (CCIS),  2015 , Mathura, \n",
      "Utter Pradesh,  pp. 221–226, doi: \n",
      "10.1109/CCIntelS.20 15.7437912 . \n",
      "[3] S Motamed , S Setayeshi, A Rabiee , and A Sharifi, ―Speech \n",
      "Emotion Recognition Based on Fusion Method,‖ Journal of \n",
      "Information Systems and Telecommunication (JIST), vol. 3, \n",
      "pp. 50--56, 2017, doi: 10.7508/jist.2017.17.007 .  \n",
      "[4] JS Li, CC Huang , ST Sheu , and MW  Lin, ―Speech emotion \n",
      "recognition and its applications,‖ Proc. of Taiwan Institute of \n",
      "Kansei Conference , 2010  Paris , France , pp. 187–192.  \n",
      "\n",
      " \n",
      "Journal of Information Systems and Telecommunication, Vol.12, No.2,  April -June 2024  \n",
      " \n",
      " \n",
      "161 \n",
      "[5] A Guerrieri , E Braccili , F Sgro , and GN Meldolesi  ―Gender \n",
      "identification in a two -level hierarchical speech emotion \n",
      "recognition system for an Italian Social Robot,‖ Sensors, vol. \n",
      "22, no. 5, pp. 1714, 2022 , doi: 10.3390/s22051714 . \n",
      "[6] M Sarria -Paja, TH Falk, and D  O’Shaughnessy , ―Whispered \n",
      "speaker verification and gender detection using weighted \n",
      "instantaneous frequencies,‖ 2013 IEEE International \n",
      "Conference on Acoustics, Speech and Signal Processing , \n",
      "2013  (May 26 -31), Vancouver , Canada, pp. 7209 –7213 , doi: \n",
      "10.1109/ICASSP.2013.6639062 .  \n",
      "[7] J Deng , S Fruhholz, Z Zhang , and Bojrn Schuller , \n",
      "―Recognizing emotions from whispered speech based on \n",
      "acoustic feature transfer learning,‖ IEEE Access , vol.  5, pp. \n",
      "5235 –5246 , 2017 .  \n",
      "[8] M Cotescu , T Drugman, G Huybrechts, J Lorenzo -Trueba, \n",
      "and A Moinet , ―Voice conversion for whispered speech \n",
      "synthesis,‖  IEEE Signal Processing Letters , vol. 27, pp. 186–\n",
      "190, 2019 , doi: 10.1109/LSP.2019.2961213 .  \n",
      "[9] P Mishra , and R Sharma, ―Gender differentiated \n",
      "convolutional neural networks for speech emotion \n",
      "recognition,‖ 2020 12th International Congress on Ultra -\n",
      "Modern Telecommuni - cations and Control Systems and \n",
      "Workshops (ICUMT),  2020  (October 5 -7), Brno , Czech \n",
      "Republic,  pp. 142–148, doi: \n",
      "10.1109/ICUMT51630.2020.9222412 . \n",
      "[10] Mustaqeem S Kwon, ―Optimal feature selection based \n",
      "speech emotion recognition using two -stream deep \n",
      "convolutional neural network,‖ International Journal of \n",
      "Intelligent Systems , vol. 36, no. 9, pp. 5116 – 5135 , 2021 , \n",
      "doi:  10.1002/int.22505 . \n",
      "[11] J. Ancilin and A. Milton, ―Improved speech emotion \n",
      "recognition with mel frequency magnitude coefficient ,‖ \n",
      "Applied Acoustics , vol. 179, pp. 108046 , 2021 , doi: \n",
      "10.1016/j.apacoust.2021.108046 .  \n",
      "[12] S. Jothimani and K. Premalatha, ―Mff -saug: Multi feature \n",
      "fusion with spectrogram augmentatio n of speech emotion \n",
      "recognition using convolution neural network ,‖ Chaos, \n",
      "Solitons & Fractals , vol. 162, pp. 112512 , 2022 , doi: \n",
      "10.1016/j.chaos.2022.112512 .  \n",
      "[13] B Yalamanchili, SK Samayamantula, and KR Anne, ―Neural \n",
      "network -based blended ensemble learning for speech \n",
      "emotion recognition,‖ Multidimensional Systems and Signal \n",
      "Processing, vol. 33, no. 4, pp. 1323 --1348 , 2022 , doi: \n",
      "10.1007/s11045 -022-00845 -9.  \n",
      "[14] T Feng , R Hebbar , and S Narayanan, ―Trustser: On the \n",
      "trustworthiness of fine -tuning pre -trained speech embeddings \n",
      "for speech emotion recognition,‖ arXiv preprint \n",
      "arXiv:2305.11229, 2023 , doi:  10.48550/arXiv.2305.11229  \n",
      "[15] RV Darekar and M Chavan , S Sharanyaa,  and NR Ranjan , \n",
      "―A hybrid meta -heuristic ensemble based classification \n",
      "technique speech emotion recognition,‖ Advances in \n",
      "Engineering Software, vol. 180, pp. 103412 , 2023 . \n",
      "[16] J Rekimot o, ―Dualvoice: A speech interaction method using \n",
      "whisper -voice as commands,‖ CHI Conference on Human \n",
      "Factors in Computing Systems Extended Abstracts,  pp. 1–6, \n",
      "2022 , doi: 10.1145/3491101.3519700 . \n",
      "[17]H Dolka,  AX VM , and S Juliet,―  Speech  emotion r ecognition  \n",
      "using  ANN on MFCC  features,‖ 2021 3rd international \n",
      "conference on signal processing and communication (ICPSC) , \n",
      "2021, Coimbatore, India, pp. 431–435, doi: \n",
      "10.1109/ICSPC51351.2021.9451810 . [18] MK Reddy and KS Rao, ―Robust pitch extraction method \n",
      "for the hmm -based speech synthesis system,‖ IEEE signal \n",
      "processing letters , vol.  24, no. 8, pp. 1133 –1137 , 2017 , doi: \n",
      "10.1109/LSP.2017. 2712646 .  \n",
      "[19] J Chatterjee, V Mukesh, HH Hsu , G Vyas , and Z Liu , \n",
      "―Speech emotion recognition using cross - correlation and \n",
      "acoustic features,‖ 2018 IEEE 16th Intl Conf on Dependable, \n",
      "Autonomic and Secure Computing, 16th Intl Conf on \n",
      "Pervasive Intelligence and Computing, 4th Intl Conf on Big \n",
      "Data Intelligence and Computing and Cyber Science and \n",
      "Technology Congress , 2018  (August 12-15), Athens, Greece , \n",
      "pp. 243–249, doi: 10.1109/DASC/PiCom/DataCom/CyberSci  \n",
      " Tec.2018.00050 .  \n",
      "[20] S Rajesh and NJ Nalini, ―Musical instrument emotion \n",
      "recognition using deep recurrent neural network,‖ Procedia \n",
      "Computer Science, vol. 167, pp. 16 --25, 2020, doi: \n",
      "10.1016/j.procs.2020.03.178 . \n",
      "[21] M Aly , and NS Alotaibi, ―A novel deep learning model to \n",
      "detect covid -19 based on wavelet features extracted from \n",
      "mel- scale spectrogram of patients’ cough and breathing \n",
      "sounds,‖ Informatics in Medicine Unlocked , vol.  32, pp.  \n",
      "101049 , 2022, doi: 10.1016/j.imu.2022.101049 . \n",
      "[22] Z Qawaqneh,  AA Mallouh,  and BD Barkana, ― Age and \n",
      "gender  classification  from  speech and face images by jointly \n",
      "fine-tuned deep neural networks,‖ Expert Systems with \n",
      "Applications , vol. 85, pp. 76–86, 2017 , doi: \n",
      "10.1016/j.eswa.2017.05.037 .  \n",
      "[23] A Koduru, HB Valiveti and AK Budati, ―Feature extraction \n",
      "algorithms to improve the speech emotion recognition rate,‖ \n",
      "International Journal of Speech Technology , vol. 23, no. 1, \n",
      "pp. 45–55, 2020 , doi: 10.1007/s10772 -020-09672 -4.  \n",
      "[24] S Zhang , and C Li, ―Research on feature fusion speech \n",
      "emotion recognition technology for smart teaching,‖ Mobile \n",
      "Information Systems , vol. 2022 , 2022 , doi:  \n",
      "10.1155/2022/7785929 . \n",
      "[25] GB, Prasanna, SV Bhat, C Naik , and HN Champa, ―An \n",
      "Efficient Method for Handwritten Kannada Digit \n",
      "Recognition based on PCA and SVM Classifier,‖ Journal of \n",
      "Information Systems and Telecommunication (JIST), vol. 3, \n",
      "no. 35, pp. 169 2021, doi: 20.1001.1.23221437.2021.9.35.3.2 . \n",
      "[26] A Graves, N Jaitly, and A Mohamed, ―Hybrid speech \n",
      "recognition wit h deep bidirectional lstm,‖ 2013 IEEE \n",
      "workshop on automatic speech recognition and \n",
      "understanding , 2013, Olomouc, Czech Republic, pp.  273–\n",
      "278, doi: 10.1109/ASRU.2013.6707742 .  \n",
      "[27] N Aloysius and M Geetha, ―A review on deep convolutional \n",
      "neural networks,‖ 2017 international conference on \n",
      "communication and signal processing (ICCSP), 2017, pp. \n",
      "0588 –0592 , IEEE, doi: 10.1109/ICCSP.2017.8286426 .  \n",
      "[28] SBC Gutha, MAB Shaik, T Udayakumar , and AA \n",
      "Saunshikhar , ―Improved feed forward attention mechanism in \n",
      "bidirectional recurrent neural networks for robust sequence \n",
      "classification,‖ 2020 International Conference on Signal \n",
      "Processing and Communications (SPCOM) , 2020 , IISc, \n",
      "Bangalore.  IEEE , pp. 1—5, doi: \n",
      "0.9179606 10.1109/SPCOM50965.202  \n",
      "\n",
      "\n",
      "Reading: d:\\PythonCode\\document_portal\\notebook\\data\\mohanty-2020-ijca-920840.pdf\n",
      " \n",
      " \n",
      " \n",
      "Journal of Information Systems and Telecommunication  \n",
      "Vol.12, No.2,  April -June 2024, 152-161 \n",
      " \n",
      "http://jist.acecr.org  \n",
      "ISSN 2322 -1437  / EISSN:2345 -2773  \n",
      " \n",
      "Whispered Speech Emotion Recognition with Gender Detection using \n",
      "BiLSTM and DCNN  \n",
      "Aniruddha Mohanty1*, Ravindranath C Cherukuri1 \n",
      " \n",
      "1.Department of Computer  science and Engineering, Christ (Deemed to be University), Bangalore, India  \n",
      " \n",
      " \n",
      "Received: 22 Aug 2023 / Revised: 04 Apr 2024 / Accepted: 21 May 2024  \n",
      " \n",
      " \n",
      "Abstract   \n",
      "Emotions are human mental states at a particular instance in time concerning one’s circumstances, mood, and relationships \n",
      "with others . Identifying emotions from the whispered speech is complicated as the conversation might be confidential. The \n",
      "representation of the speech relies on the magnitude of its information. Whispered speech is intelligible, a low -intensi ty \n",
      "signal, and varies from normal speech. Emotion identification is quite tricky from whispered speech. Both prosodic and \n",
      "spectral speech features help to identify emotions. The emotion identification in a whispered speech happens using prosodic \n",
      "speech fea tures such as zero -crossing rate  (ZCR) , pitch, and spectral features that include spectral centroid, chroma STFT, \n",
      "Mel scale spectrogram, Mel -frequency cepstral coefficient (MFCC), Shifted Delta Cepstrum (SDC), and Spectral Flux. \n",
      "There are two parts to the proposed implementation. Bidirectional Long  Short -Term Memory (BiLSTM) helps to identify \n",
      "the gender from the speech sample in the first step with SDC and pitch. The Deep Convolutional Neural Network (DCNN) \n",
      "model helps to identify the emotions in the second  step. This implementation is evaluated using the  wTIMIT data corpus \n",
      "and gives 98.54% accuracy. Emotions have a dynamic effect on genders, so this implementation performs better than \n",
      "traditional approaches. This approach helps to design online learning management systems, different applications for \n",
      "mobile devices, checking cyber -criminal activities, emotion detection for older people, automatic speaker identification and \n",
      "authentication, forensics, and surveillance . \n",
      " \n",
      "Keywords:  Whispered Speech; Emotion Re cognition; Speech Features; Data Corpus; BiLSTM;  DCNN . \n",
      " \n",
      "1- Introduction  \n",
      "Emotions are the humans’ short -lived feeling s that affects \n",
      "thinking, actions, relationships, and social interactions. \n",
      "Emotions express humans’ physiological and emotional \n",
      "states with facial expressions and body language. \n",
      "Whispered speech is a form of speec h that expresses \n",
      "emotions. Whispered speech is a type of communication \n",
      "produced with breath without any noise and excitation of \n",
      "voice. The whispered speech structure changes \n",
      "significantly because of the lack of periodic excitation in \n",
      "the voice. This result s in missing speech and reduced \n",
      "transparency in communication.  \n",
      "The difference between normal and whispered speech is \n",
      "the absence of vocal tract vibrations due to the vocal \n",
      "tract’s physiological blocking. The strength of whispered \n",
      "speech is minute and with out voice compared to phoned \n",
      "speech. The spectral and prosodic features help to detect \n",
      "the whispered speech. Prosodic features of speech vary \n",
      "over time. Spectral features of whispered speech are highly accurate over unvoiced consonants, voiced consonants, \n",
      "and formants in vowels [1]. \n",
      "The impacts on Speech Emotion Recognition ( SER) are \n",
      "due to various acoustic conditions such as compressed, \n",
      "noisy, telephonic conversation s and imitator speeches. \n",
      "Other environmental conditions,  such as stress, rhythm, \n",
      "and intonation, can affect  SER.  Whispered speech is also \n",
      "affected by similar acoustic and environmental conditions. \n",
      "SER depends on acoustic characteristics and gender in \n",
      "some scenarios. Hence gender plays a vital role in SER.  \n",
      "Nowadays, several whispered speech  samples of male and \n",
      "female voices are available. The effectiveness of the SER \n",
      "is more when the implementation is in two parts. The \n",
      "identification of the gender [2] happens initially using \n",
      "Bidirectional Long Short -term Memory (BiLSTM)  with \n",
      "speech features . The next step is detecting emotions using \n",
      "various speech features [3] and Deep Convolutional \n",
      "Neural Network (DCNN).   \n",
      "The structure of the paper includes various sections. \n",
      "Section 2 describes Human Emotions and their \n",
      "Applications. Details of the Related Work are in Section 3. \n",
      "Section 4 describes the System Model, which is the black \n",
      "box view of the implementation. Section 5 is the Model \n",
      " \n",
      "Journal of Information Systems and Telecommunication, Vol.12, No.2,  April -June 2024  \n",
      " \n",
      " \n",
      "153 \n",
      "Design that describes the details of the speech features and \n",
      "the deep learning models used in this implementation. \n",
      "Section 6 gives the Experiment and Result Analysis. \n",
      "Section 7 brief ly discuss  the Conclusion and Future Work.   \n",
      "2- Human  Emotions  and their  Applications  \n",
      "Human emotions are the mental state caused by countless \n",
      "associated views , feeling s, behavioral replies, and the \n",
      "degree of pressure and annoyance. It is often associated \n",
      "with attitude , temperament, behavior , disposition, and \n",
      "creativi ty [4]. Emotion recognition helps to detect a \n",
      "humans ’ emotional mood , which lasts hours and days. \n",
      "Speech conveys emotions such as anger, disgust, fear, \n",
      "happiness, neutrality, sadness, and surprise. The machine \n",
      "automatically detects various emotions from speech with \n",
      "the help of different algorithms. Emotion Recognition \n",
      "helps to : \n",
      " \n",
      "•  Detect customers ’ intentions based on the teleconference. \n",
      "•  Detect cybercrime.  \n",
      "•  Students ’ attention and teachers ’ content adjustment.  \n",
      "•  Disability assistance  \n",
      "•  Customer satisfaction  \n",
      "•  Stress monitoring  \n",
      "•  Social media analysis  \n",
      "•  Suspicious activity  \n",
      "•  Human -machine interaction and so on.   \n",
      "3- Related Work  \n",
      "Over time, numerous studies have detected emotions from \n",
      "whispered speech . The vario us deep learning models \n",
      "detect emotions based on the speech features extracted \n",
      "from the collected whispered speech data corpus . The SER \n",
      "for normal and whispered speech is diverse because of \n",
      "vocal excitation. Emotions vary with many factors; gender \n",
      "is among  the most influential factors  [5]. Identifying \n",
      "gender in the first step improves emotion detection from \n",
      "whispered speech. So, the related work explored is on \n",
      "gender detection and emotion recognition from whispered \n",
      "speech.  \n",
      "MFCC obtained by the Hilbert enve lope approach and \n",
      "weighted instantaneous frequencies (WIFs) obtained by \n",
      "the coherent demodulation help to detect gender in \n",
      "whispered speech  samples [6]. There is an opportunity to \n",
      "explore gender detection in noisy speech conditions using \n",
      "these approaches.  \n",
      "Autoencoder -enabled features  in the transfer learning \n",
      "framework propose to practice  phonated data to identify \n",
      "emotions from Whispered speech [7]. The feature \n",
      "extraction is from the Geneva Whispered Emotion Corpus \n",
      "(GeWEC) and Berlin Emotional Speech Databa se (EMO -\n",
      "DB) data corpus. The acoustic features such as Mel-frequency cepstral coefficient (MFCC), root mean square \n",
      "(RMS), frame energy, zero -crossing -rate (ZCR), pitch \n",
      "frequency (F0), probability of voicing autocorrelation \n",
      "function are the inputs to evaluate the framework. \n",
      "Implementing deep learning concepts on spontaneous data \n",
      "gives more accuracy than the current framework .  \n",
      "Gender is detected to target an anonymous speaker. The \n",
      "Deep Neural Network (DNN) model is used to generalize \n",
      "gender by using the MFCC speech feature. This \n",
      "implementation is applied to the wTIMIT dataset to verify \n",
      "the gender and recognize the speaker  [8]. The DNN model \n",
      "cannot generalize gender; evaluation can happen with \n",
      "other datasets.   \n",
      "MFCC and CNN, with the fully  connected network, detect \n",
      "gender and emotions like anger, disgust, fear, happiness, \n",
      "sadness, surprise, and a neutral state  [9]. The concept \n",
      "verification happens on RAVDESS, CREMA -D, SAVEE, \n",
      "and TESS datasets, having an accuracy of 92.283%, which \n",
      "is better than the tr aditional model.  \n",
      "The final prediction of the implemented model is to learn \n",
      "the mutually spatial -spectral features happens by a two -\n",
      "stream deep convolutional neural network with an iterative \n",
      "neighborhood  component analysis (INCA) and the most \n",
      "discriminatory optimal features  [10]. The concept \n",
      "verification happens on EMO -DB, SAVEE, and \n",
      "RAVDESS emotional speech corpora which perform with \n",
      "95%, 82%, and 85% accuracy rates. Real -time applications \n",
      "with natural and huge data corpora can help to extend this \n",
      "concept to identify emotions.  \n",
      "Mel Frequency Magnitude Coefficient (MFMC) and three \n",
      "spectral features, namely MFCC, log frequency power \n",
      "coefficient, and linear prediction cepstral coefficient, are \n",
      "used with the help of Support Vector Machine (SVM) \n",
      "modeling  [11]. The performance evaluation uses this \n",
      "concept  on Berlin, RAVDESS, SAVEE, EMOVO, and \n",
      "eNTERFACE data corpus . Feature selection, feature \n",
      "fusion, and multiple classification schemes improve the \n",
      "performance of MFMC.  \n",
      "The proposed MFF -SAug research in  which noise removal \n",
      "improves emotion. White Noise Injection, Pitch Tuning \n",
      "techniques, MFCC, ZCR, RMS speech features, and \n",
      "Convolutional Neural Network (CNN) modeling [12] \n",
      "detect emotions. Emotion detection during the interaction \n",
      "between people can extend the MFF -SAug approach .  \n",
      "The proposed Neural Network -based Blended Ensemble \n",
      "Learning (NNBEL) model is composed of a 1 -dimensional \n",
      "Convolution Neural Network (1D -CNN), Long Short -\n",
      "Term Memory (LSTM), and CapsuleNets. LSTM receives \n",
      "input from the Log Mel -spect rogram speech features, \n",
      "while 1D -CNN and CapsuleNets receive input from the \n",
      "MFCC.  Each model’s output is fed to Multi -Layer \n",
      "Perceptron (MLP) and predicts the final emotion s [13]. \n",
      "This model shows 95.3% and 94% accuracy on \n",
      "RAVDESS and IEMOCAP datasets, respectively.  \n",
      " \n",
      "Mohanty, & Cherukuri, Whispered Speech Emotion  Recognition with Gender Detection using BiLSTM and DCNN  \n",
      " \n",
      " \n",
      " \n",
      "154 \n",
      "TrustSER [14] implemented a general framework to \n",
      "determine the SER system's trustworthiness using deep \n",
      "learning techniques. Trustworthiness evaluates privacy \n",
      "(gender in formation, speaker demographics), safety, \n",
      "fairness, sustainability, and emotions (sad, angry, happy, \n",
      "neutral). The architecture of the TrustSET framework uses \n",
      "CNN encoder and Transformer encoder models. \n",
      "Trustworthy profiles under the Federated Learning \n",
      "scenario might improve privacy, fairness, and safety.  \n",
      "A hybrid meta -heuristic ensemble -based classification [15] \n",
      "helps to detect speech emotions. Raw speech samples are \n",
      "filtered using the Butterworth filter; then, spectrogram \n",
      "speech features are extracted from each frame to create a \n",
      "hybrid feature vector. The ensemble -based classification is \n",
      "applied to hybrid feature vectors to classify em otions. The \n",
      "ensemble -based classification contains a Recurrent Neural \n",
      "Network (RNN), a Deep Belief Network (DBN), and an \n",
      "Artificial Neural Network (ANN).  \n",
      "The above -related work shows that it is crucial to identify \n",
      "gender to segregate emotions in a speech. Male and female \n",
      "emotions are different based on situations. Whispered \n",
      "speech is an essential concept of emotion recognition. \n",
      "There is an opportunity to experiment further based on \n",
      "other datasets of whispered speech. So, this is a motivation \n",
      "to work on emot ion recognition in whispered speech, as \n",
      "this is a less explored area of research .  \n",
      "4- System Model  \n",
      "The analysis of e motion recognition from the whispered \n",
      "speech segregates into two parts. One is gender detection, \n",
      "and the second is emotion detection as shown in Fig 1.  \n",
      " \n",
      " \n",
      "Fig. 1 Emotion recognition from whispered speech  \n",
      " \n",
      "As part of gender detection, pre -processing of the speech \n",
      "samples is performed, and then pitch and SDC are \n",
      "extracted as part of prosodic and spectral features, \n",
      "respectively. Both the features are fused to get a single \n",
      "feature set with the help of multifeature fusion. PCA helps \n",
      "to reduce the dimensions of extracted features and is used \n",
      "as input to Bidirectional Long short -term memory \n",
      "(BiLSTM) to classify genders , as shown in Fig 2.  \n",
      " \n",
      " \n",
      "Fig .2 Gender detection from whispered speech  \n",
      " \n",
      "Following  Gender detection, the augmentation of speech \n",
      "data helps to get more realistic data. Then the speech features such as Chromatogram, Zero -Crossing Rate, \n",
      "Spectral Central, MFCC, Spectral Flux , and Mel \n",
      "Spectrog rams are extracted. All the extracted features \n",
      "combine to get a single feature using multifeature fusion. \n",
      "Then the dimensions are reduced to get the optimal data \n",
      "points. The data points are inserted into the Deep \n",
      "Convolutional Neural Network (DCNN) to identify the \n",
      "emotions , as shown in Fig 3.  \n",
      "5- System  Design  \n",
      "The implementation of system design happens in two parts. \n",
      "The first part identifies the male and female gender from \n",
      "the whispered speech samples. Then the determination of \n",
      "different emotions like sadn ess, happiness, fear, anger, \n",
      "surprise, disgust, and neutral are detected in gender -\n",
      "segregated speech samples.  \n",
      " \n",
      "Fig. 3 Emotion Recognition from Male and Female whispered speech.  \n",
      "5-1- Whispered Speech Data Corpus  \n",
      "wTIMIT [16] is a whispered voice data corpus ha ving 450 \n",
      "phonetically balanced sentences with 29 speakers. There \n",
      "are 11,324 utterances with a normal voice , which  can be \n",
      "used for normal and whispered training. This data is \n",
      "available in two parts - train and test divisions. The \n",
      "samples contain an 8 kHz sampling rate with a high -\n",
      "quality  and low-pass filter.  \n",
      "5-2-    Data Augmentation  \n",
      "Data augmentation [17 ] is the method of getting more \n",
      "realistic data from the existing data, which helps to add \n",
      "more training data to the model, reduce overfitting, and \n",
      "increase the model’s generalization ability. Below are the \n",
      "steps to generate synthetic data.  \n",
      " Shifting Time : Shifting time is a simple concept \n",
      "where the audio shifts to the left or right with a \n",
      "haphazard second. If the audio shifts to the left \n",
      "\n",
      " \n",
      "Journal of Information Systems and Telecommunication, Vol.12, No.2,  April -June 2024  \n",
      " \n",
      " \n",
      "155 \n",
      "with z seconds, then the first z seconds are \n",
      "marked as silence. Similarly, if the audio shifts to \n",
      "the right with z second s, the last z seconds are \n",
      "marked as silence.  \n",
      " Changing Pitch: Pitch change is adjusting the \n",
      "pitch randomly without upsetting the speediness \n",
      "of the audio file.  \n",
      " Changing Speed: Speed audio changes by \n",
      "stretching the time series data by a fixed rate.  \n",
      "5-3- Pre-processing  \n",
      "After the data collection and data augmentation, pre -\n",
      "processing [3] is the initial  phase  in training SER. \n",
      "Framing, Windowing, Voice activity detection, \n",
      "Normalization, and Noise reduction are pre -\n",
      "processing steps.   \n",
      "Framing:  Speech signals are quasip eriodic and vary \n",
      "over time. Hence, information and related emotions \n",
      "also fluctuate over time. The segregation of the \n",
      "signals happens in a shorter period to make the \n",
      "speech signal invariant. Twenty milliseconds to thirty \n",
      "milliseconds helps to make the speec h signal \n",
      "invariant, and five milliseconds of overlap of the \n",
      "frames avoid data leakage between the frames .  \n",
      "Windowing:  Windowing on each frame helps to diminish \n",
      "data leakage during Fast Fourier Transformation (FFT) \n",
      "after framing. The Hamming window allows  this step \n",
      "where the window size is N for the frames     , used in \n",
      "equation (1).  \n",
      " \n",
      "                  (   \n",
      "   )       (1) \n",
      "            \n",
      " \n",
      "Voice activity detection (VAD):  An utterance has three \n",
      "portions of speech activities : voiced, unvoiced, and silent. \n",
      "Zero Crossing Rate (ZCR) speech feature helps to detect \n",
      "VAD. ZCR represents the frequency of signal transitions \n",
      "between positive and negative values within a specific \n",
      "frame . Due to high energy, the ZCR value is low for \n",
      "voiced speech and high for unvoiced s peech because of \n",
      "low energy.  \n",
      "Normalization:  Normalization helps to reduce  speaker \n",
      "and recording inconsistency without affecting the features’ \n",
      "strength and enhancing the features’ generalization \n",
      "capability. The Z-normalization method is used more and \n",
      "represented as  \n",
      "       \n",
      "      (2) \n",
      "Where   is the speech signal,   and   are the mean and \n",
      "standard deviation of the data, respectively.  \n",
      "Noise Reduction:  The environmental noise captured while \n",
      "recording a speech signal  affects the recognition rate . \n",
      "Minimum mean square error (MMSE) and log -spectral \n",
      "amplitude MMSE (LogMMSE) reduce the noise from the speech signals. Noise reduction helps to get more accuracy \n",
      "in SER evaluation.  \n",
      "5-4- Feature Extraction  \n",
      "Specific emotions are present in prosodic and spectral \n",
      "features of speech. The extraction of prosodic and spectral \n",
      "features is a vital characteristic of emotion recognition \n",
      "after pre -processing the speech signals.  \n",
      "5-4-1-Prosodic Features  \n",
      "Prosodic features deal with the audio qualities of a speech \n",
      "when connected speeches use sounds as input. The \n",
      "production of the speech deals with the amount of energy, \n",
      "frequency, period, loudness, pitch, and duration. Speech \n",
      "signal com munication depends on intonation, stress, and \n",
      "rhythm, which prosodic features can detect. The prosodic \n",
      "features used in the implementation are : \n",
      "Zero -Crossing Rate (ZCR):  ZCR [17] measures the \n",
      "frequency of signal transitions between positive and \n",
      "negative values in every audio frame  and defined as  \n",
      "    \n",
      "    ∑       [ ]         [      \n",
      "    (3) \n",
      "Where        is the sign function . \n",
      "    [     ]  {        \n",
      "       \n",
      "Fundamental Frequency (Pitch):  Fundamental \n",
      "Frequency (F0) [18] is the minimum freque ncy of the \n",
      "periodic waveform. F0 is the significant parameter to \n",
      "differentiate male speech from female speech. Pitch also \n",
      "determines the voiced and unvoiced portion of the speech \n",
      "signal. This analysis uses pitch parameters like pitch mean \n",
      "value and pitch r ange.  \n",
      "The pitch range determines the number of octaves a speech  \n",
      "sample  can cover, from the lowest to the highest. F0 value \n",
      "varies from 85Hz to 180 HZ for the voiced speech of adult \n",
      "males.  \n",
      " \n",
      " \n",
      "Fig. 4 Representation of Pitch data  \n",
      " \n",
      "Similarly, the F0 value for adult females goes from 165Hz \n",
      "to 255Hz, as shown in Fig 4.  \n",
      "\n",
      " \n",
      "Mohanty, & Cherukuri, Whispered Speech Emotion  Recognition with Gender Detection using BiLSTM and DCNN  \n",
      " \n",
      " \n",
      " \n",
      "156 \n",
      "5-4-2-Spectral features  \n",
      "The representation of spectral features happens by \n",
      "converting the time -domain speech signal into the \n",
      "frequency domain with the help of the Fast Fourier \n",
      "Transform (FFT) which  benefits  to represent the \n",
      "characteristics of the Human Vocal Tract. The  spectral \n",
      "features are  \n",
      "Spectral Centroid (SC):  The Spectral Centroid [19] is the \n",
      "most used speech feature where the positioning of the \n",
      "―center of mass‖ of the audio signal spectr um defines the \n",
      "weight of the spectrum. The center of mass measures the \n",
      "weighted average of the frequency component located in \n",
      "the audio signal, defined as  \n",
      " \n",
      "      ∑            \n",
      "   \n",
      "∑       \n",
      "       (4) \n",
      "Where      represents the magnitude of bin numb er   and \n",
      "      represents the central frequency of the bin.  \n",
      "Chroma STFT:  Chroma STFT [20] is obtained using FFT \n",
      "on speech samples and the resulting spectrums a re a \n",
      "chromatogram in a vertical axis. This feature captures the \n",
      "harmonic feature of the speech s ignals.  \n",
      "Mel-scale Spectrogram:  Mel-scale Spectrogram [ 21] is a \n",
      "spectrogram in which frequencies convert to the Mel scale, \n",
      "which helps to differentiate the range of frequencies. Mel -\n",
      "spectrogram helps to understand emotions in a better way \n",
      "as humans can perceive sound on a logarithmic scale.  \n",
      "Mel Frequency Cepstral Coefficient (MFCC ): MFCC \n",
      "[17] i s SER’s  standard feature extraction technique. The \n",
      "vocal cord s, tongue, and teeth filter the sound and make it \n",
      "unique for each speaker in the Human Speech production \n",
      "system. Mel scale represents MFCC, where the frequency \n",
      "bands are equally spaced and close to the Human Auditory \n",
      "System’s response, as shown in Fig 5.  \n",
      " \n",
      "Fig 5 MFCC speech feature extraction  \n",
      " \n",
      "Shifted Delta Cepstrum (SDC):  Time derivatives apply \n",
      "to the cepstral coefficients obtained from the MFCC and \n",
      "combined with the delta coefficient to get SDC, as shown \n",
      "in Fig 6.       and   are the four parameters used in \n",
      "SDC [ 22].  M denotes the cepstral coefficient for each \n",
      "frame.   indicates added frames.   are the frames that \n",
      "append delta features from the new feature vector .   \n",
      "represents the delta values difference. So, the coefficient \n",
      "vectors represent as   \n",
      "     [                  ]   (5) \n",
      "   are MFCC coefficients and   is the coefficient index. \n",
      "For a given time,   an intermediate calculation is done to \n",
      "obtain the   coefficients.  \n",
      "                                 (6) \n",
      "Finally, the SDC coefficients vectors of   dimensions are \n",
      "obtained as  \n",
      "       [          (                )] (7) \n",
      " \n",
      " \n",
      "Fig. 6 Extraction of SDC speech feature  \n",
      " \n",
      "This speech feature helps to identify genders for a long -\n",
      "range dynamic appearance in speech signals.  \n",
      "Spectral Flux:  Spectral flux [ 20] measures the change in \n",
      "speed of the signal’s power spectrum compared to the \n",
      "previous frame. This feature estimates the speech signal’s \n",
      "power spectrum about  the power spectrum of one frame \n",
      "with others.  \n",
      "5-5- Feature Selection and Dimension Reduction  \n",
      "Feature s election [23] is selecting features from a large set \n",
      "of extracted features to eliminate redundant and unused \n",
      "information and decrease processing time. For this work, \n",
      "the extracted pitch contains lots of information. The global \n",
      "features, like the range of p itch values and mean pitch \n",
      "values, are selected. Removing redundant and unused \n",
      "information, such as additional zeroes, duplicate values, or \n",
      "frames, is part of MFCC feature extraction. The exact \n",
      "process removes the silent speech frames.  \n",
      "Fundamental frequen cy and SDC are two speech features \n",
      "to identify gender. Similarly, Spectral centroid, Chroma \n",
      "STFT, MFCC, and Spectral flux features are used for \n",
      "emotion identification to create a single set of data points \n",
      "from multiple speech features using multifeature fu sion \n",
      "technique [24]. \n",
      "Multifeature Fusion:  The extracted speech features hav e \n",
      "different dimensions, so feature proximity usages  average \n",
      "dimensional spacing between the vectors to denote the \n",
      "proximity between the diverse features  [24]. The average \n",
      "dimensional spacing between the vectors is computed as  \n",
      "          \n",
      "    √∑∑              (8) \n",
      "   =  \n",
      "     √∑∑              (9) \n",
      "\n",
      " \n",
      "Journal of Information Systems and Telecommunication, Vol.12, No.2,  April -June 2024  \n",
      " \n",
      " \n",
      "157 \n",
      "        and     are the mean and variance interval of \n",
      "average dimensions of the feature vector s.    and    are \n",
      "the mean value of   and   type of sound features.     and    \n",
      "are the mean value of   and   type of sound features . Then \n",
      "the subsequent dimensionality reduction [25] method \n",
      "follows once the feature selection process is complete. \n",
      "High data variance is present in the extracted features \n",
      "containing more information. Dimension reduction \n",
      "techniques reduce the dimensions from extracted feature \n",
      "vectors.  \n",
      " \n",
      "5-5-1- Principal Component Analysis  (PCA)  \n",
      "PCA [25] is an approach for redu cing the dimensionality \n",
      "of extensive datasets, transforming them into more \n",
      "compact representations while retaining crucial \n",
      "information . These reductions in the number of variables \n",
      "help to get more accurate results. A reduced dataset makes \n",
      "it easier and fas ter to visualize and analyze the data. \n",
      "Following steps followed to explore PCA .  \n",
      " Standardization  creates a normalized dataset \n",
      "when there is a significant difference in the range \n",
      "of initial variables or a larger range of datasets \n",
      "dominates the smaller range. It can be done by  \n",
      "             \n",
      "                     (10) \n",
      " Covariance matrix computation  helps  to know  \n",
      "how the input  dataset  varies  from  the mean value \n",
      "to each data point.  \n",
      " Eigenvectors and eigenvalues  of the covariance \n",
      "matrix help to identify the principal component.  \n",
      "5-6- Modelling  \n",
      "This implementation uses two deep l earning models. \n",
      "Bidirectional Long Short -Term Memory (BiLSTM) \n",
      "classifies the genders in whispered speech, and Deep \n",
      "Convolutional Neural Network (DCNN) identifies \n",
      "emotions.  \n",
      "BiLSTM  [26] combines  two Recurrent Neural Networks \n",
      "(RNN) that are placed independently and can traverse \n",
      "backward and forward directions at each time step, as \n",
      "shown in Fig 7. There are two ways to deal with data in \n",
      "this model - The first involves processing data from the \n",
      "past to the future , and the second operates in the opposite \n",
      "direction, from future  to past , where two hidden layers of \n",
      "neurons help to preserve the information in both \n",
      "directions. This approach helps to improve the information \n",
      "available in the algorithm.  \n",
      "For B iLSTM, the sequence of inputs is \n",
      "                   for a traditional RNN, which \n",
      "computes the hidden vector sequences \n",
      "                  and results in the output \n",
      "sequences                    for the iteration \n",
      "    to                            (11) \n",
      "                (12) \n",
      "  are the weight matrices;   is the bias vector, and   is \n",
      "the hidden layer function.  \n",
      "DCNN  [27] is a Convolutional Neural Network (CNN) \n",
      "type that helps to identify emotions from whispered \n",
      "speech . The input to the model is the speech data that \n",
      "traverses through many stages, such as the convolution \n",
      "layer, the pooling layer, Activation, Fully Connected, \n",
      "Batch normalization, and dropout layers, as shown in Fig \n",
      "8.  \n",
      " \n",
      " \n",
      " \n",
      "Fig 7. Structure of BiLSTM  \n",
      " \n",
      "To avoid overfitting, Leaky ReLU is used as an activation \n",
      "function in this implementation and evaluated as  \n",
      "{                      \n",
      "                        (13) \n",
      "The fully connected layer is a loss layer, measuring the \n",
      "inconsistency between the desired and actual outputs . Root \n",
      "The Mean Squared Propagation (RMSProp) optimization \n",
      "algorithm enhances the loss function, which varies in \n",
      "vertical directions.   \n",
      " \n",
      "Fig. 8 Structure of Deep Convolution Neural Network  \n",
      "5-7- Emotions  \n",
      "Deep learning techniques are applied to identify the \n",
      "emotions of human speech. Discrete Emotion [3] \n",
      "theory uses seven vital emotions in Human activities.  \n",
      " Sadness:  This is the feeling of dissatisfaction, \n",
      "sorrow, or fruitlessness.  \n",
      " Happiness:  This is the emotional state that elicits \n",
      "satisfaction and pleasure.  \n",
      " Fear:  This feeling triggers a fright response.  \n",
      " Anger:  This emotional state leads to feelings of \n",
      "hostility and frustration.  \n",
      " Surprise:  It is the state of mind that expresses \n",
      "either positive or negative following something \n",
      "unexpected.  \n",
      "\n",
      " \n",
      "Mohanty, & Cherukuri, Whispered Speech Emotion  Recognition with Gender Detection using BiLSTM and DCNN  \n",
      " \n",
      " \n",
      " \n",
      "158 \n",
      " Disgust:  A strong emotion that results in feeling \n",
      "repulsed.  \n",
      " Neutral:  This is the feeling of lack of particular \n",
      "preference.  \n",
      " \n",
      "5-8- Algorithm for Emotion Recognition  \n",
      "The algorithm to detect emotions from the whispered \n",
      "speech is in two parts  \n",
      " Gender Detection  \n",
      " Emotion Identification   \n",
      " \n",
      "Algorithm 1:  Geneder Detection  \n",
      "INPUT: Text  File (Whispered Speech Samples)  \n",
      "OUTPUT: Emotions (Sad, Happy, Fear, Anger, Surprise, \n",
      "Disgust, Neutral)  \n",
      " \n",
      "1: Begin:  \n",
      "2: Read speech  samples from Corpus   \n",
      "3: Pre-processing  the speech  samples:  \n",
      "4: Processed_speech=Pre_processing (Speech_Sample)   \n",
      "5: Extracted  SDC,  Pitch  features:  \n",
      "6: SDC, Pitch=Feature_Extraction(Processed_speech)   \n",
      "7: Multifeature  fusion:  \n",
      "8: multi _feature = multifeatured _fusion(SDC,  Pitch)   \n",
      "9: Dimension  reduction:  \n",
      "10: Dimension _R = Dimension _Reduction(multi _feature)  \n",
      "11: Determine  gender:  \n",
      "12: Gender = BiLSTM  (Dimension _R) \n",
      "13: Placed  in folders:  \n",
      "14: Gender=  (Male,  Female)   \n",
      "15: End:  \n",
      "Algorithm 2 Emotion Identification  \n",
      " \n",
      "1:  Begin:   \n",
      "2:  Speech Samples = (Male,  Female)  \n",
      "3:  Data Augmentation:  \n",
      "4:  Augemented _samples = Data Augmentation  (Speech          \n",
      "Samples)  \n",
      "5:  Pre-processed  the speech  samples:   \n",
      "6:  Processed _speech=Preprocessing  (Augemented _sam \n",
      "ples)  \n",
      "7:  Extracted  Spectral  Centroid,  ChromaSTFT,  MFCC,  \n",
      "Mel spectrogram  features:   \n",
      "8:  Spectral _Centroid,  Chroma _STFT,  MFCC,  \n",
      "Mel_spectrogram = Feature Extraction  (Processed _spe \n",
      "ech)  \n",
      "9:  Multifeature  fusion:   \n",
      "10:  multi _feature = \n",
      "multifeatured _fusion(Spectral _Centroid,  Chroma _STFT,  \n",
      "MFCC,  Mel_spectrogr am) \n",
      "11:  Dimension  reduction:  \n",
      "12:  Dimension_R = Dimension_Reduction(multi_feature)  \n",
      "13:  Emotion  detection:  14:  Emotions = DCNN(Dimension_R)   \n",
      "15:  OUTPUT Emotions  (Sad, Happy, Fear, Anger, \n",
      "Surprise, Disgust, Neutral)  \n",
      "16:  End:  \n",
      "6- Experiment and Result Analysis  \n",
      "The proposed SER has been implemented utilizing Python \n",
      "programming language and fortified by various machine \n",
      "learning libraries and additional supporting libraries . The \n",
      "experiment uses Python (Python 3.6.3rc1) and Librosa \n",
      "(Librosa 0.8.0) for audio processing. Graph plotting uses \n",
      "Seaborn and Matplotlib libraries, which help to analyze the \n",
      "speech data. BiLSTM and DCNN models implement the \n",
      "model using Keras (Keras - 2.6.0), TensorFlow \n",
      "(TensorFlow -2.6.0), and Scikit -learn libraries .  \n",
      "wTIMIT dataset is collected and divided into the train, \n",
      "cross -validation, and test samples. Subsequently, the \n",
      "speech processing takes place during implementation, \n",
      "incorporating data -augmentation techniques like shifting \n",
      "time, changing pitch, and changing speed . The pre -\n",
      "processing of speech content from each sample helps to \n",
      "analyze it and extract its features. The dimensionally \n",
      "reduced extracted features align with the data points in the \n",
      "BiLSTM model. The training dataset makes the model \n",
      "learn the features or data. Epochs continuously learn \n",
      "hidden features when the dataset completes backward and \n",
      "forward iterations. Then the cross -validation dataset is \n",
      "used to estimate the model’s performance on each epoch \n",
      "and prevent the model from being overfit ted. The test \n",
      "dataset helps to evaluate the trained model unbiasedly \n",
      "using performance metrics like precision, recall, f1 score, \n",
      "and confusion Matrix .  \n",
      "The parameters used in BiLSTM models are as in Table 1.  \n",
      "Table 1: Parameters of the BiLSTM model  \n",
      "Layers  Shape of output  Parameters#  \n",
      "Embedding  \n",
      "(Embedding)  (None, 216, 256)  \n",
      " 524544  \n",
      " \n",
      "Bidirectional  \n",
      " (None, 128)  \n",
      " 98816  \n",
      "batch normalization  (None, 128)  \n",
      " 512 \n",
      "Dense (Dense)  (None, 128)  \n",
      " 10512  \n",
      "dropout (Dropout)  (None, 128)  \n",
      " 0 \n",
      "flatten (Flatten)  (None, 128)  \n",
      " 1024  \n",
      "dense1(Dense)  \n",
      " (None, 12)  \n",
      " 0 \n",
      "dense2(Dense)  \n",
      " (None, 12)  \n",
      " 455 \n",
      " \n",
      " \n",
      "Journal of Information Systems and Telecommunication, Vol.12, No.2,  April -June 2024  \n",
      " \n",
      " \n",
      "159 \n",
      "The BiLSTM model helps to segregate the speech samples \n",
      "into male and female. The segregated speech samples \n",
      "move to their respective male and female folders.  \n",
      "Table 2: Parameters of the DCNN model  \n",
      "Layers  Shape of output  Parameters#  \n",
      "conv1d (Conv1D)  \n",
      " (None, 216, 256)  \n",
      " 2304  \n",
      "Activation  \n",
      "(Activation)  (None, 216, 256)  \n",
      " 0 \n",
      "conv1d 1 (Conv1D)  \n",
      " (None, 216, 256)  524544  \n",
      "batch normalization  \n",
      " (None, 216, 256)  1024 \n",
      "activation 1  \n",
      "(Activation)  \n",
      " (None, 216, 256) 0 \n",
      "dropout (Dropout)  (None, 216, 256)  \n",
      " 0 \n",
      "max pooling1d \n",
      "(MaxPooling1D)  \n",
      " (None, 13, 256)  \n",
      " 0 \n",
      "conv1d 2 (Conv1D)  \n",
      " (None, 13, 128)  \n",
      " 262272  \n",
      " \n",
      "activation 2 \n",
      "(Activation)  \n",
      " (None, 13, 128)  \n",
      " 0 \n",
      "conv1d 3 (Conv1D)  \n",
      " (None, 13, 128)  \n",
      " 131200  \n",
      " \n",
      "activation 3 \n",
      "(Activation)  \n",
      " (None, 13, 128)  \n",
      " 0 \n",
      "conv1d 4 (Conv1D)  \n",
      " (None, 13, 128)  \n",
      " 131200  \n",
      " \n",
      "batch normalization  \n",
      " (None, 13, 128)  \n",
      " 512 \n",
      "activation 4 \n",
      "(Activation)  \n",
      " (None, 13, 128)  \n",
      " 0 \n",
      "conv1d 5 (Conv1D)  \n",
      " (None, 13, 128)  \n",
      " 131200  \n",
      " \n",
      "batch normalization \n",
      "1 \n",
      " (None, 13, 128)  \n",
      " 512 \n",
      "activation 5 \n",
      "(Activation)  \n",
      " (None, 13, 128)  \n",
      " 0 \n",
      "dropout 1 (Dropout)  \n",
      " (None, 13, 128)  \n",
      " 0 \n",
      "max pooling1d 1 \n",
      "(MaxPooling1D)  \n",
      " (None, 1, 128)  \n",
      " 0 \n",
      "conv1d 6 (Conv1D)  \n",
      " (None, 1, 64)  \n",
      " 65600  \n",
      "activation 6 \n",
      "(Activation)  \n",
      " (None, 1, 64)  \n",
      " 0 \n",
      "conv1d 7 (Conv1D)  \n",
      " (None, 1, 64)  \n",
      " 32832  \n",
      "activation 7 \n",
      "(Activation)  \n",
      " (None, 1, 64)  \n",
      " 0 \n",
      "flatten (Flatten)  \n",
      " (None, 64)  \n",
      " 0 dense (Dense)  \n",
      " (None, 14)  \n",
      " 910 \n",
      "activation 8 \n",
      "(Activation)  \n",
      " (None, 14)  \n",
      " 0 \n",
      " \n",
      "After separating the speech samples into genders, Chroma \n",
      "STFT, Spectral centroid, Mel-scale spectrogram, and \n",
      "Spectral Flux feature s extraction happen ed. Then the \n",
      "created data points are dimensionally reduced to fit into \n",
      "the DCNN deep learning model to identify emotions. \n",
      "Metrics like precision, recall, F1 score, and the confusion \n",
      "matrix aid in assessing the model's performance.  Table 2 \n",
      "mentions the parameters of the DCNN model.  \n",
      "6-1- Result Analysis  \n",
      "The fusion of Fundamental frequency and SDC \n",
      "detect s the gender of speech samples. The initial \n",
      "stage involves pre -processing the speech samples, \n",
      "followed by feature extraction, which includes SDC \n",
      "and fundamental frequencies. The combination of \n",
      "SDC and Fundamental speech features create a \n",
      "multifeature fusion resulting in a single set of data \n",
      "points. The next step is to reduce the dimensions of \n",
      "data p oints to use them as input to the BiLSTM \n",
      "Model. The BiLSTM model predicts the data in the \n",
      "dataset as male and female, placed in separate \n",
      "folders. Of these, 3342 speech samples are available, \n",
      "and 3294 are correctly classified. So, the accuracy of \n",
      "the model prediction is 99.59%. The precision, recall, \n",
      "and f1 -score values, along with the confusion matrix, \n",
      "are shown in Fig s 9 and 10. The predicted model \n",
      "identifies the female speech samples more accurately \n",
      "than the male.   \n",
      " \n",
      "Fig. 9 Confusion Matrix for Gender detection by BiLSTM.  \n",
      " \n",
      "The graph in Fig 9 shows the gender detection from the \n",
      "speech sample. The X -axis represents the true label, and \n",
      "the Y-axis shows the predicted label.  This graph gives the \n",
      "\n",
      " \n",
      "Mohanty, & Cherukuri, Whispered Speech Emotion  Recognition with Gender Detection using BiLSTM and DCNN  \n",
      " \n",
      " \n",
      " \n",
      "160 \n",
      "count of detected gender spe ech that the Bi -LSTM model \n",
      "correctly and incorrectly detects.  \n",
      " \n",
      "Fig. 10 Accuracy measures of Gender Detection.  \n",
      " \n",
      "2031 female and 1263 male speech are correctly detected, \n",
      "whereas 13 females and 35 males are incorrectly detected. \n",
      "Pitch values identify the emotions after detecting genders \n",
      "from the speech samples.  \n",
      "The extraction of MFCC, Mel -scale spectrogram, Chroma \n",
      "STFT, Spectral Flux, and Spectral Centroid speech \n",
      "features happens from speech samples. Then, all five \n",
      "speech features are fused into a single feature and \n",
      "dimensionally reduced. Finally, the DCNN model is \n",
      "applied to predict emotions.  \n",
      "Individual emotions are detected based on gender as \n",
      "shown in Fig 11. The deviation in the male speech \n",
      "emotions is less than the female . Female fear and female \n",
      "neutral  emotions show divergent results from other \n",
      "emotions. The accuracy of the model is 98.54%.   \n",
      " \n",
      " \n",
      "Fig.11 Emotions based on gender.  \n",
      "6-2- Comparison Analysis  \n",
      "This experiment evaluates the current implementation \n",
      "using the wTIMIT  dataset, resulting in an impressive \n",
      "accuracy of 98.54%. The performance compares with \n",
      "three -layered Long short -term memory Bidirectional \n",
      "RNNs (LSTM BiRNN) that use No -Attention, Feed \n",
      "Forward Attention (FFA), and Improved Feed Forward \n",
      "Attention Mechanism ( IFFA) to evaluate the emotions in \n",
      "the dataset. This model uses 35 hidden states to train the dataset. The models are trained and validated with the help \n",
      "of the wTIMIT dataset [28] and tested with the help of the \n",
      "CHAINS dataset,  as shown in Table 3. This ex periment is \n",
      "a new hypothesis for  emotion detection with gender \n",
      "identification on whispered speech.  Hence, there are fewer \n",
      "references available for similar investigations.  \n",
      "Table 3: Comparison of the Implementation  \n",
      "  Sl# Model Implementation  Accuracy  \n",
      "1 FFA, IFFA, LSTM, Bi_RNN [2 8] 97.6 %  \n",
      "2 Proposed Model  98.54 %  \n",
      "The proposed method improves the performance of \n",
      "emotions after segregating the speech samples into genders.  \n",
      "By separating the approach into two models, this \n",
      "implementation excels in capturing natural and \n",
      "spontaneous expressions of emotions with low latency \n",
      "manner .  \n",
      "7- Conclusion  and Future Work  \n",
      "Identification of gender from whispered speech and \n",
      "recognizing emotions is a complicated task. This \n",
      "implementation initially detects the gender from the \n",
      "speech samples before identifying the emotions. Emotions \n",
      "might vary based on gender in the same situation. Speech \n",
      "features such as Fu ndamental Frequency and SDC help \n",
      "with gender identification. MFCC, Mel -scale spectrogram, \n",
      "Spectral flux, Spectral Centroid, and Chroma STFT play a \n",
      "vital role in detecting emotions. Multifeature fusion helps \n",
      "to combine speech features into a single set of d ata points . \n",
      "The concept verifies with the publicly available dataset \n",
      "wTIMIT with an accuracy of 98.54%. This approach helps \n",
      "to identify nearly inaudible emotions and is used to figure \n",
      "out the strategy. The proposed methodology can be \n",
      "improved using other s peech features, machine learning, \n",
      "and deep learning concepts.  \n",
      " \n",
      "References  \n",
      "[1]  ST Jovicic , and Z  Saric , ―Acoustic analysis of  consonants in \n",
      "whispered speech,‖ Journal of voice , vol 22, no. 3, pp. 263–\n",
      "274, 2008 . \n",
      "[2]  M Kumari , and I  Ali, ―An efficient algorithm for gender \n",
      "detection using voice samples,‖ 2015 Communication, \n",
      "Control and Intelligent Systems (CCIS),  2015 , Mathura, \n",
      "Utter Pradesh,  pp. 221–226, doi: \n",
      "10.1109/CCIntelS.20 15.7437912 . \n",
      "[3] S Motamed , S Setayeshi, A Rabiee , and A Sharifi, ―Speech \n",
      "Emotion Recognition Based on Fusion Method,‖ Journal of \n",
      "Information Systems and Telecommunication (JIST), vol. 3, \n",
      "pp. 50--56, 2017, doi: 10.7508/jist.2017.17.007 .  \n",
      "[4] JS Li, CC Huang , ST Sheu , and MW  Lin, ―Speech emotion \n",
      "recognition and its applications,‖ Proc. of Taiwan Institute of \n",
      "Kansei Conference , 2010  Paris , France , pp. 187–192.  \n",
      "\n",
      " \n",
      "Journal of Information Systems and Telecommunication, Vol.12, No.2,  April -June 2024  \n",
      " \n",
      " \n",
      "161 \n",
      "[5] A Guerrieri , E Braccili , F Sgro , and GN Meldolesi  ―Gender \n",
      "identification in a two -level hierarchical speech emotion \n",
      "recognition system for an Italian Social Robot,‖ Sensors, vol. \n",
      "22, no. 5, pp. 1714, 2022 , doi: 10.3390/s22051714 . \n",
      "[6] M Sarria -Paja, TH Falk, and D  O’Shaughnessy , ―Whispered \n",
      "speaker verification and gender detection using weighted \n",
      "instantaneous frequencies,‖ 2013 IEEE International \n",
      "Conference on Acoustics, Speech and Signal Processing , \n",
      "2013  (May 26 -31), Vancouver , Canada, pp. 7209 –7213 , doi: \n",
      "10.1109/ICASSP.2013.6639062 .  \n",
      "[7] J Deng , S Fruhholz, Z Zhang , and Bojrn Schuller , \n",
      "―Recognizing emotions from whispered speech based on \n",
      "acoustic feature transfer learning,‖ IEEE Access , vol.  5, pp. \n",
      "5235 –5246 , 2017 .  \n",
      "[8] M Cotescu , T Drugman, G Huybrechts, J Lorenzo -Trueba, \n",
      "and A Moinet , ―Voice conversion for whispered speech \n",
      "synthesis,‖  IEEE Signal Processing Letters , vol. 27, pp. 186–\n",
      "190, 2019 , doi: 10.1109/LSP.2019.2961213 .  \n",
      "[9] P Mishra , and R Sharma, ―Gender differentiated \n",
      "convolutional neural networks for speech emotion \n",
      "recognition,‖ 2020 12th International Congress on Ultra -\n",
      "Modern Telecommuni - cations and Control Systems and \n",
      "Workshops (ICUMT),  2020  (October 5 -7), Brno , Czech \n",
      "Republic,  pp. 142–148, doi: \n",
      "10.1109/ICUMT51630.2020.9222412 . \n",
      "[10] Mustaqeem S Kwon, ―Optimal feature selection based \n",
      "speech emotion recognition using two -stream deep \n",
      "convolutional neural network,‖ International Journal of \n",
      "Intelligent Systems , vol. 36, no. 9, pp. 5116 – 5135 , 2021 , \n",
      "doi:  10.1002/int.22505 . \n",
      "[11] J. Ancilin and A. Milton, ―Improved speech emotion \n",
      "recognition with mel frequency magnitude coefficient ,‖ \n",
      "Applied Acoustics , vol. 179, pp. 108046 , 2021 , doi: \n",
      "10.1016/j.apacoust.2021.108046 .  \n",
      "[12] S. Jothimani and K. Premalatha, ―Mff -saug: Multi feature \n",
      "fusion with spectrogram augmentatio n of speech emotion \n",
      "recognition using convolution neural network ,‖ Chaos, \n",
      "Solitons & Fractals , vol. 162, pp. 112512 , 2022 , doi: \n",
      "10.1016/j.chaos.2022.112512 .  \n",
      "[13] B Yalamanchili, SK Samayamantula, and KR Anne, ―Neural \n",
      "network -based blended ensemble learning for speech \n",
      "emotion recognition,‖ Multidimensional Systems and Signal \n",
      "Processing, vol. 33, no. 4, pp. 1323 --1348 , 2022 , doi: \n",
      "10.1007/s11045 -022-00845 -9.  \n",
      "[14] T Feng , R Hebbar , and S Narayanan, ―Trustser: On the \n",
      "trustworthiness of fine -tuning pre -trained speech embeddings \n",
      "for speech emotion recognition,‖ arXiv preprint \n",
      "arXiv:2305.11229, 2023 , doi:  10.48550/arXiv.2305.11229  \n",
      "[15] RV Darekar and M Chavan , S Sharanyaa,  and NR Ranjan , \n",
      "―A hybrid meta -heuristic ensemble based classification \n",
      "technique speech emotion recognition,‖ Advances in \n",
      "Engineering Software, vol. 180, pp. 103412 , 2023 . \n",
      "[16] J Rekimot o, ―Dualvoice: A speech interaction method using \n",
      "whisper -voice as commands,‖ CHI Conference on Human \n",
      "Factors in Computing Systems Extended Abstracts,  pp. 1–6, \n",
      "2022 , doi: 10.1145/3491101.3519700 . \n",
      "[17]H Dolka,  AX VM , and S Juliet,―  Speech  emotion r ecognition  \n",
      "using  ANN on MFCC  features,‖ 2021 3rd international \n",
      "conference on signal processing and communication (ICPSC) , \n",
      "2021, Coimbatore, India, pp. 431–435, doi: \n",
      "10.1109/ICSPC51351.2021.9451810 . [18] MK Reddy and KS Rao, ―Robust pitch extraction method \n",
      "for the hmm -based speech synthesis system,‖ IEEE signal \n",
      "processing letters , vol.  24, no. 8, pp. 1133 –1137 , 2017 , doi: \n",
      "10.1109/LSP.2017. 2712646 .  \n",
      "[19] J Chatterjee, V Mukesh, HH Hsu , G Vyas , and Z Liu , \n",
      "―Speech emotion recognition using cross - correlation and \n",
      "acoustic features,‖ 2018 IEEE 16th Intl Conf on Dependable, \n",
      "Autonomic and Secure Computing, 16th Intl Conf on \n",
      "Pervasive Intelligence and Computing, 4th Intl Conf on Big \n",
      "Data Intelligence and Computing and Cyber Science and \n",
      "Technology Congress , 2018  (August 12-15), Athens, Greece , \n",
      "pp. 243–249, doi: 10.1109/DASC/PiCom/DataCom/CyberSci  \n",
      " Tec.2018.00050 .  \n",
      "[20] S Rajesh and NJ Nalini, ―Musical instrument emotion \n",
      "recognition using deep recurrent neural network,‖ Procedia \n",
      "Computer Science, vol. 167, pp. 16 --25, 2020, doi: \n",
      "10.1016/j.procs.2020.03.178 . \n",
      "[21] M Aly , and NS Alotaibi, ―A novel deep learning model to \n",
      "detect covid -19 based on wavelet features extracted from \n",
      "mel- scale spectrogram of patients’ cough and breathing \n",
      "sounds,‖ Informatics in Medicine Unlocked , vol.  32, pp.  \n",
      "101049 , 2022, doi: 10.1016/j.imu.2022.101049 . \n",
      "[22] Z Qawaqneh,  AA Mallouh,  and BD Barkana, ― Age and \n",
      "gender  classification  from  speech and face images by jointly \n",
      "fine-tuned deep neural networks,‖ Expert Systems with \n",
      "Applications , vol. 85, pp. 76–86, 2017 , doi: \n",
      "10.1016/j.eswa.2017.05.037 .  \n",
      "[23] A Koduru, HB Valiveti and AK Budati, ―Feature extraction \n",
      "algorithms to improve the speech emotion recognition rate,‖ \n",
      "International Journal of Speech Technology , vol. 23, no. 1, \n",
      "pp. 45–55, 2020 , doi: 10.1007/s10772 -020-09672 -4.  \n",
      "[24] S Zhang , and C Li, ―Research on feature fusion speech \n",
      "emotion recognition technology for smart teaching,‖ Mobile \n",
      "Information Systems , vol. 2022 , 2022 , doi:  \n",
      "10.1155/2022/7785929 . \n",
      "[25] GB, Prasanna, SV Bhat, C Naik , and HN Champa, ―An \n",
      "Efficient Method for Handwritten Kannada Digit \n",
      "Recognition based on PCA and SVM Classifier,‖ Journal of \n",
      "Information Systems and Telecommunication (JIST), vol. 3, \n",
      "no. 35, pp. 169 2021, doi: 20.1001.1.23221437.2021.9.35.3.2 . \n",
      "[26] A Graves, N Jaitly, and A Mohamed, ―Hybrid speech \n",
      "recognition wit h deep bidirectional lstm,‖ 2013 IEEE \n",
      "workshop on automatic speech recognition and \n",
      "understanding , 2013, Olomouc, Czech Republic, pp.  273–\n",
      "278, doi: 10.1109/ASRU.2013.6707742 .  \n",
      "[27] N Aloysius and M Geetha, ―A review on deep convolutional \n",
      "neural networks,‖ 2017 international conference on \n",
      "communication and signal processing (ICCSP), 2017, pp. \n",
      "0588 –0592 , IEEE, doi: 10.1109/ICCSP.2017.8286426 .  \n",
      "[28] SBC Gutha, MAB Shaik, T Udayakumar , and AA \n",
      "Saunshikhar , ―Improved feed forward attention mechanism in \n",
      "bidirectional recurrent neural networks for robust sequence \n",
      "classification,‖ 2020 International Conference on Signal \n",
      "Processing and Communications (SPCOM) , 2020 , IISc, \n",
      "Bangalore.  IEEE , pp. 1—5, doi: \n",
      "0.9179606 10.1109/SPCOM50965.202  \n",
      "\n",
      "\n",
      "Reading: d:\\PythonCode\\document_portal\\notebook\\data\\Paper_94-Sentiment_Analysis_on_Banking_Feedback_and_News_Data.pdf\n",
      " \n",
      " \n",
      " \n",
      "Journal of Information Systems and Telecommunication  \n",
      "Vol.12, No.2,  April -June 2024, 152-161 \n",
      " \n",
      "http://jist.acecr.org  \n",
      "ISSN 2322 -1437  / EISSN:2345 -2773  \n",
      " \n",
      "Whispered Speech Emotion Recognition with Gender Detection using \n",
      "BiLSTM and DCNN  \n",
      "Aniruddha Mohanty1*, Ravindranath C Cherukuri1 \n",
      " \n",
      "1.Department of Computer  science and Engineering, Christ (Deemed to be University), Bangalore, India  \n",
      " \n",
      " \n",
      "Received: 22 Aug 2023 / Revised: 04 Apr 2024 / Accepted: 21 May 2024  \n",
      " \n",
      " \n",
      "Abstract   \n",
      "Emotions are human mental states at a particular instance in time concerning one’s circumstances, mood, and relationships \n",
      "with others . Identifying emotions from the whispered speech is complicated as the conversation might be confidential. The \n",
      "representation of the speech relies on the magnitude of its information. Whispered speech is intelligible, a low -intensi ty \n",
      "signal, and varies from normal speech. Emotion identification is quite tricky from whispered speech. Both prosodic and \n",
      "spectral speech features help to identify emotions. The emotion identification in a whispered speech happens using prosodic \n",
      "speech fea tures such as zero -crossing rate  (ZCR) , pitch, and spectral features that include spectral centroid, chroma STFT, \n",
      "Mel scale spectrogram, Mel -frequency cepstral coefficient (MFCC), Shifted Delta Cepstrum (SDC), and Spectral Flux. \n",
      "There are two parts to the proposed implementation. Bidirectional Long  Short -Term Memory (BiLSTM) helps to identify \n",
      "the gender from the speech sample in the first step with SDC and pitch. The Deep Convolutional Neural Network (DCNN) \n",
      "model helps to identify the emotions in the second  step. This implementation is evaluated using the  wTIMIT data corpus \n",
      "and gives 98.54% accuracy. Emotions have a dynamic effect on genders, so this implementation performs better than \n",
      "traditional approaches. This approach helps to design online learning management systems, different applications for \n",
      "mobile devices, checking cyber -criminal activities, emotion detection for older people, automatic speaker identification and \n",
      "authentication, forensics, and surveillance . \n",
      " \n",
      "Keywords:  Whispered Speech; Emotion Re cognition; Speech Features; Data Corpus; BiLSTM;  DCNN . \n",
      " \n",
      "1- Introduction  \n",
      "Emotions are the humans’ short -lived feeling s that affects \n",
      "thinking, actions, relationships, and social interactions. \n",
      "Emotions express humans’ physiological and emotional \n",
      "states with facial expressions and body language. \n",
      "Whispered speech is a form of speec h that expresses \n",
      "emotions. Whispered speech is a type of communication \n",
      "produced with breath without any noise and excitation of \n",
      "voice. The whispered speech structure changes \n",
      "significantly because of the lack of periodic excitation in \n",
      "the voice. This result s in missing speech and reduced \n",
      "transparency in communication.  \n",
      "The difference between normal and whispered speech is \n",
      "the absence of vocal tract vibrations due to the vocal \n",
      "tract’s physiological blocking. The strength of whispered \n",
      "speech is minute and with out voice compared to phoned \n",
      "speech. The spectral and prosodic features help to detect \n",
      "the whispered speech. Prosodic features of speech vary \n",
      "over time. Spectral features of whispered speech are highly accurate over unvoiced consonants, voiced consonants, \n",
      "and formants in vowels [1]. \n",
      "The impacts on Speech Emotion Recognition ( SER) are \n",
      "due to various acoustic conditions such as compressed, \n",
      "noisy, telephonic conversation s and imitator speeches. \n",
      "Other environmental conditions,  such as stress, rhythm, \n",
      "and intonation, can affect  SER.  Whispered speech is also \n",
      "affected by similar acoustic and environmental conditions. \n",
      "SER depends on acoustic characteristics and gender in \n",
      "some scenarios. Hence gender plays a vital role in SER.  \n",
      "Nowadays, several whispered speech  samples of male and \n",
      "female voices are available. The effectiveness of the SER \n",
      "is more when the implementation is in two parts. The \n",
      "identification of the gender [2] happens initially using \n",
      "Bidirectional Long Short -term Memory (BiLSTM)  with \n",
      "speech features . The next step is detecting emotions using \n",
      "various speech features [3] and Deep Convolutional \n",
      "Neural Network (DCNN).   \n",
      "The structure of the paper includes various sections. \n",
      "Section 2 describes Human Emotions and their \n",
      "Applications. Details of the Related Work are in Section 3. \n",
      "Section 4 describes the System Model, which is the black \n",
      "box view of the implementation. Section 5 is the Model \n",
      " \n",
      "Journal of Information Systems and Telecommunication, Vol.12, No.2,  April -June 2024  \n",
      " \n",
      " \n",
      "153 \n",
      "Design that describes the details of the speech features and \n",
      "the deep learning models used in this implementation. \n",
      "Section 6 gives the Experiment and Result Analysis. \n",
      "Section 7 brief ly discuss  the Conclusion and Future Work.   \n",
      "2- Human  Emotions  and their  Applications  \n",
      "Human emotions are the mental state caused by countless \n",
      "associated views , feeling s, behavioral replies, and the \n",
      "degree of pressure and annoyance. It is often associated \n",
      "with attitude , temperament, behavior , disposition, and \n",
      "creativi ty [4]. Emotion recognition helps to detect a \n",
      "humans ’ emotional mood , which lasts hours and days. \n",
      "Speech conveys emotions such as anger, disgust, fear, \n",
      "happiness, neutrality, sadness, and surprise. The machine \n",
      "automatically detects various emotions from speech with \n",
      "the help of different algorithms. Emotion Recognition \n",
      "helps to : \n",
      " \n",
      "•  Detect customers ’ intentions based on the teleconference. \n",
      "•  Detect cybercrime.  \n",
      "•  Students ’ attention and teachers ’ content adjustment.  \n",
      "•  Disability assistance  \n",
      "•  Customer satisfaction  \n",
      "•  Stress monitoring  \n",
      "•  Social media analysis  \n",
      "•  Suspicious activity  \n",
      "•  Human -machine interaction and so on.   \n",
      "3- Related Work  \n",
      "Over time, numerous studies have detected emotions from \n",
      "whispered speech . The vario us deep learning models \n",
      "detect emotions based on the speech features extracted \n",
      "from the collected whispered speech data corpus . The SER \n",
      "for normal and whispered speech is diverse because of \n",
      "vocal excitation. Emotions vary with many factors; gender \n",
      "is among  the most influential factors  [5]. Identifying \n",
      "gender in the first step improves emotion detection from \n",
      "whispered speech. So, the related work explored is on \n",
      "gender detection and emotion recognition from whispered \n",
      "speech.  \n",
      "MFCC obtained by the Hilbert enve lope approach and \n",
      "weighted instantaneous frequencies (WIFs) obtained by \n",
      "the coherent demodulation help to detect gender in \n",
      "whispered speech  samples [6]. There is an opportunity to \n",
      "explore gender detection in noisy speech conditions using \n",
      "these approaches.  \n",
      "Autoencoder -enabled features  in the transfer learning \n",
      "framework propose to practice  phonated data to identify \n",
      "emotions from Whispered speech [7]. The feature \n",
      "extraction is from the Geneva Whispered Emotion Corpus \n",
      "(GeWEC) and Berlin Emotional Speech Databa se (EMO -\n",
      "DB) data corpus. The acoustic features such as Mel-frequency cepstral coefficient (MFCC), root mean square \n",
      "(RMS), frame energy, zero -crossing -rate (ZCR), pitch \n",
      "frequency (F0), probability of voicing autocorrelation \n",
      "function are the inputs to evaluate the framework. \n",
      "Implementing deep learning concepts on spontaneous data \n",
      "gives more accuracy than the current framework .  \n",
      "Gender is detected to target an anonymous speaker. The \n",
      "Deep Neural Network (DNN) model is used to generalize \n",
      "gender by using the MFCC speech feature. This \n",
      "implementation is applied to the wTIMIT dataset to verify \n",
      "the gender and recognize the speaker  [8]. The DNN model \n",
      "cannot generalize gender; evaluation can happen with \n",
      "other datasets.   \n",
      "MFCC and CNN, with the fully  connected network, detect \n",
      "gender and emotions like anger, disgust, fear, happiness, \n",
      "sadness, surprise, and a neutral state  [9]. The concept \n",
      "verification happens on RAVDESS, CREMA -D, SAVEE, \n",
      "and TESS datasets, having an accuracy of 92.283%, which \n",
      "is better than the tr aditional model.  \n",
      "The final prediction of the implemented model is to learn \n",
      "the mutually spatial -spectral features happens by a two -\n",
      "stream deep convolutional neural network with an iterative \n",
      "neighborhood  component analysis (INCA) and the most \n",
      "discriminatory optimal features  [10]. The concept \n",
      "verification happens on EMO -DB, SAVEE, and \n",
      "RAVDESS emotional speech corpora which perform with \n",
      "95%, 82%, and 85% accuracy rates. Real -time applications \n",
      "with natural and huge data corpora can help to extend this \n",
      "concept to identify emotions.  \n",
      "Mel Frequency Magnitude Coefficient (MFMC) and three \n",
      "spectral features, namely MFCC, log frequency power \n",
      "coefficient, and linear prediction cepstral coefficient, are \n",
      "used with the help of Support Vector Machine (SVM) \n",
      "modeling  [11]. The performance evaluation uses this \n",
      "concept  on Berlin, RAVDESS, SAVEE, EMOVO, and \n",
      "eNTERFACE data corpus . Feature selection, feature \n",
      "fusion, and multiple classification schemes improve the \n",
      "performance of MFMC.  \n",
      "The proposed MFF -SAug research in  which noise removal \n",
      "improves emotion. White Noise Injection, Pitch Tuning \n",
      "techniques, MFCC, ZCR, RMS speech features, and \n",
      "Convolutional Neural Network (CNN) modeling [12] \n",
      "detect emotions. Emotion detection during the interaction \n",
      "between people can extend the MFF -SAug approach .  \n",
      "The proposed Neural Network -based Blended Ensemble \n",
      "Learning (NNBEL) model is composed of a 1 -dimensional \n",
      "Convolution Neural Network (1D -CNN), Long Short -\n",
      "Term Memory (LSTM), and CapsuleNets. LSTM receives \n",
      "input from the Log Mel -spect rogram speech features, \n",
      "while 1D -CNN and CapsuleNets receive input from the \n",
      "MFCC.  Each model’s output is fed to Multi -Layer \n",
      "Perceptron (MLP) and predicts the final emotion s [13]. \n",
      "This model shows 95.3% and 94% accuracy on \n",
      "RAVDESS and IEMOCAP datasets, respectively.  \n",
      " \n",
      "Mohanty, & Cherukuri, Whispered Speech Emotion  Recognition with Gender Detection using BiLSTM and DCNN  \n",
      " \n",
      " \n",
      " \n",
      "154 \n",
      "TrustSER [14] implemented a general framework to \n",
      "determine the SER system's trustworthiness using deep \n",
      "learning techniques. Trustworthiness evaluates privacy \n",
      "(gender in formation, speaker demographics), safety, \n",
      "fairness, sustainability, and emotions (sad, angry, happy, \n",
      "neutral). The architecture of the TrustSET framework uses \n",
      "CNN encoder and Transformer encoder models. \n",
      "Trustworthy profiles under the Federated Learning \n",
      "scenario might improve privacy, fairness, and safety.  \n",
      "A hybrid meta -heuristic ensemble -based classification [15] \n",
      "helps to detect speech emotions. Raw speech samples are \n",
      "filtered using the Butterworth filter; then, spectrogram \n",
      "speech features are extracted from each frame to create a \n",
      "hybrid feature vector. The ensemble -based classification is \n",
      "applied to hybrid feature vectors to classify em otions. The \n",
      "ensemble -based classification contains a Recurrent Neural \n",
      "Network (RNN), a Deep Belief Network (DBN), and an \n",
      "Artificial Neural Network (ANN).  \n",
      "The above -related work shows that it is crucial to identify \n",
      "gender to segregate emotions in a speech. Male and female \n",
      "emotions are different based on situations. Whispered \n",
      "speech is an essential concept of emotion recognition. \n",
      "There is an opportunity to experiment further based on \n",
      "other datasets of whispered speech. So, this is a motivation \n",
      "to work on emot ion recognition in whispered speech, as \n",
      "this is a less explored area of research .  \n",
      "4- System Model  \n",
      "The analysis of e motion recognition from the whispered \n",
      "speech segregates into two parts. One is gender detection, \n",
      "and the second is emotion detection as shown in Fig 1.  \n",
      " \n",
      " \n",
      "Fig. 1 Emotion recognition from whispered speech  \n",
      " \n",
      "As part of gender detection, pre -processing of the speech \n",
      "samples is performed, and then pitch and SDC are \n",
      "extracted as part of prosodic and spectral features, \n",
      "respectively. Both the features are fused to get a single \n",
      "feature set with the help of multifeature fusion. PCA helps \n",
      "to reduce the dimensions of extracted features and is used \n",
      "as input to Bidirectional Long short -term memory \n",
      "(BiLSTM) to classify genders , as shown in Fig 2.  \n",
      " \n",
      " \n",
      "Fig .2 Gender detection from whispered speech  \n",
      " \n",
      "Following  Gender detection, the augmentation of speech \n",
      "data helps to get more realistic data. Then the speech features such as Chromatogram, Zero -Crossing Rate, \n",
      "Spectral Central, MFCC, Spectral Flux , and Mel \n",
      "Spectrog rams are extracted. All the extracted features \n",
      "combine to get a single feature using multifeature fusion. \n",
      "Then the dimensions are reduced to get the optimal data \n",
      "points. The data points are inserted into the Deep \n",
      "Convolutional Neural Network (DCNN) to identify the \n",
      "emotions , as shown in Fig 3.  \n",
      "5- System  Design  \n",
      "The implementation of system design happens in two parts. \n",
      "The first part identifies the male and female gender from \n",
      "the whispered speech samples. Then the determination of \n",
      "different emotions like sadn ess, happiness, fear, anger, \n",
      "surprise, disgust, and neutral are detected in gender -\n",
      "segregated speech samples.  \n",
      " \n",
      "Fig. 3 Emotion Recognition from Male and Female whispered speech.  \n",
      "5-1- Whispered Speech Data Corpus  \n",
      "wTIMIT [16] is a whispered voice data corpus ha ving 450 \n",
      "phonetically balanced sentences with 29 speakers. There \n",
      "are 11,324 utterances with a normal voice , which  can be \n",
      "used for normal and whispered training. This data is \n",
      "available in two parts - train and test divisions. The \n",
      "samples contain an 8 kHz sampling rate with a high -\n",
      "quality  and low-pass filter.  \n",
      "5-2-    Data Augmentation  \n",
      "Data augmentation [17 ] is the method of getting more \n",
      "realistic data from the existing data, which helps to add \n",
      "more training data to the model, reduce overfitting, and \n",
      "increase the model’s generalization ability. Below are the \n",
      "steps to generate synthetic data.  \n",
      " Shifting Time : Shifting time is a simple concept \n",
      "where the audio shifts to the left or right with a \n",
      "haphazard second. If the audio shifts to the left \n",
      "\n",
      " \n",
      "Journal of Information Systems and Telecommunication, Vol.12, No.2,  April -June 2024  \n",
      " \n",
      " \n",
      "155 \n",
      "with z seconds, then the first z seconds are \n",
      "marked as silence. Similarly, if the audio shifts to \n",
      "the right with z second s, the last z seconds are \n",
      "marked as silence.  \n",
      " Changing Pitch: Pitch change is adjusting the \n",
      "pitch randomly without upsetting the speediness \n",
      "of the audio file.  \n",
      " Changing Speed: Speed audio changes by \n",
      "stretching the time series data by a fixed rate.  \n",
      "5-3- Pre-processing  \n",
      "After the data collection and data augmentation, pre -\n",
      "processing [3] is the initial  phase  in training SER. \n",
      "Framing, Windowing, Voice activity detection, \n",
      "Normalization, and Noise reduction are pre -\n",
      "processing steps.   \n",
      "Framing:  Speech signals are quasip eriodic and vary \n",
      "over time. Hence, information and related emotions \n",
      "also fluctuate over time. The segregation of the \n",
      "signals happens in a shorter period to make the \n",
      "speech signal invariant. Twenty milliseconds to thirty \n",
      "milliseconds helps to make the speec h signal \n",
      "invariant, and five milliseconds of overlap of the \n",
      "frames avoid data leakage between the frames .  \n",
      "Windowing:  Windowing on each frame helps to diminish \n",
      "data leakage during Fast Fourier Transformation (FFT) \n",
      "after framing. The Hamming window allows  this step \n",
      "where the window size is N for the frames     , used in \n",
      "equation (1).  \n",
      " \n",
      "                  (   \n",
      "   )       (1) \n",
      "            \n",
      " \n",
      "Voice activity detection (VAD):  An utterance has three \n",
      "portions of speech activities : voiced, unvoiced, and silent. \n",
      "Zero Crossing Rate (ZCR) speech feature helps to detect \n",
      "VAD. ZCR represents the frequency of signal transitions \n",
      "between positive and negative values within a specific \n",
      "frame . Due to high energy, the ZCR value is low for \n",
      "voiced speech and high for unvoiced s peech because of \n",
      "low energy.  \n",
      "Normalization:  Normalization helps to reduce  speaker \n",
      "and recording inconsistency without affecting the features’ \n",
      "strength and enhancing the features’ generalization \n",
      "capability. The Z-normalization method is used more and \n",
      "represented as  \n",
      "       \n",
      "      (2) \n",
      "Where   is the speech signal,   and   are the mean and \n",
      "standard deviation of the data, respectively.  \n",
      "Noise Reduction:  The environmental noise captured while \n",
      "recording a speech signal  affects the recognition rate . \n",
      "Minimum mean square error (MMSE) and log -spectral \n",
      "amplitude MMSE (LogMMSE) reduce the noise from the speech signals. Noise reduction helps to get more accuracy \n",
      "in SER evaluation.  \n",
      "5-4- Feature Extraction  \n",
      "Specific emotions are present in prosodic and spectral \n",
      "features of speech. The extraction of prosodic and spectral \n",
      "features is a vital characteristic of emotion recognition \n",
      "after pre -processing the speech signals.  \n",
      "5-4-1-Prosodic Features  \n",
      "Prosodic features deal with the audio qualities of a speech \n",
      "when connected speeches use sounds as input. The \n",
      "production of the speech deals with the amount of energy, \n",
      "frequency, period, loudness, pitch, and duration. Speech \n",
      "signal com munication depends on intonation, stress, and \n",
      "rhythm, which prosodic features can detect. The prosodic \n",
      "features used in the implementation are : \n",
      "Zero -Crossing Rate (ZCR):  ZCR [17] measures the \n",
      "frequency of signal transitions between positive and \n",
      "negative values in every audio frame  and defined as  \n",
      "    \n",
      "    ∑       [ ]         [      \n",
      "    (3) \n",
      "Where        is the sign function . \n",
      "    [     ]  {        \n",
      "       \n",
      "Fundamental Frequency (Pitch):  Fundamental \n",
      "Frequency (F0) [18] is the minimum freque ncy of the \n",
      "periodic waveform. F0 is the significant parameter to \n",
      "differentiate male speech from female speech. Pitch also \n",
      "determines the voiced and unvoiced portion of the speech \n",
      "signal. This analysis uses pitch parameters like pitch mean \n",
      "value and pitch r ange.  \n",
      "The pitch range determines the number of octaves a speech  \n",
      "sample  can cover, from the lowest to the highest. F0 value \n",
      "varies from 85Hz to 180 HZ for the voiced speech of adult \n",
      "males.  \n",
      " \n",
      " \n",
      "Fig. 4 Representation of Pitch data  \n",
      " \n",
      "Similarly, the F0 value for adult females goes from 165Hz \n",
      "to 255Hz, as shown in Fig 4.  \n",
      "\n",
      " \n",
      "Mohanty, & Cherukuri, Whispered Speech Emotion  Recognition with Gender Detection using BiLSTM and DCNN  \n",
      " \n",
      " \n",
      " \n",
      "156 \n",
      "5-4-2-Spectral features  \n",
      "The representation of spectral features happens by \n",
      "converting the time -domain speech signal into the \n",
      "frequency domain with the help of the Fast Fourier \n",
      "Transform (FFT) which  benefits  to represent the \n",
      "characteristics of the Human Vocal Tract. The  spectral \n",
      "features are  \n",
      "Spectral Centroid (SC):  The Spectral Centroid [19] is the \n",
      "most used speech feature where the positioning of the \n",
      "―center of mass‖ of the audio signal spectr um defines the \n",
      "weight of the spectrum. The center of mass measures the \n",
      "weighted average of the frequency component located in \n",
      "the audio signal, defined as  \n",
      " \n",
      "      ∑            \n",
      "   \n",
      "∑       \n",
      "       (4) \n",
      "Where      represents the magnitude of bin numb er   and \n",
      "      represents the central frequency of the bin.  \n",
      "Chroma STFT:  Chroma STFT [20] is obtained using FFT \n",
      "on speech samples and the resulting spectrums a re a \n",
      "chromatogram in a vertical axis. This feature captures the \n",
      "harmonic feature of the speech s ignals.  \n",
      "Mel-scale Spectrogram:  Mel-scale Spectrogram [ 21] is a \n",
      "spectrogram in which frequencies convert to the Mel scale, \n",
      "which helps to differentiate the range of frequencies. Mel -\n",
      "spectrogram helps to understand emotions in a better way \n",
      "as humans can perceive sound on a logarithmic scale.  \n",
      "Mel Frequency Cepstral Coefficient (MFCC ): MFCC \n",
      "[17] i s SER’s  standard feature extraction technique. The \n",
      "vocal cord s, tongue, and teeth filter the sound and make it \n",
      "unique for each speaker in the Human Speech production \n",
      "system. Mel scale represents MFCC, where the frequency \n",
      "bands are equally spaced and close to the Human Auditory \n",
      "System’s response, as shown in Fig 5.  \n",
      " \n",
      "Fig 5 MFCC speech feature extraction  \n",
      " \n",
      "Shifted Delta Cepstrum (SDC):  Time derivatives apply \n",
      "to the cepstral coefficients obtained from the MFCC and \n",
      "combined with the delta coefficient to get SDC, as shown \n",
      "in Fig 6.       and   are the four parameters used in \n",
      "SDC [ 22].  M denotes the cepstral coefficient for each \n",
      "frame.   indicates added frames.   are the frames that \n",
      "append delta features from the new feature vector .   \n",
      "represents the delta values difference. So, the coefficient \n",
      "vectors represent as   \n",
      "     [                  ]   (5) \n",
      "   are MFCC coefficients and   is the coefficient index. \n",
      "For a given time,   an intermediate calculation is done to \n",
      "obtain the   coefficients.  \n",
      "                                 (6) \n",
      "Finally, the SDC coefficients vectors of   dimensions are \n",
      "obtained as  \n",
      "       [          (                )] (7) \n",
      " \n",
      " \n",
      "Fig. 6 Extraction of SDC speech feature  \n",
      " \n",
      "This speech feature helps to identify genders for a long -\n",
      "range dynamic appearance in speech signals.  \n",
      "Spectral Flux:  Spectral flux [ 20] measures the change in \n",
      "speed of the signal’s power spectrum compared to the \n",
      "previous frame. This feature estimates the speech signal’s \n",
      "power spectrum about  the power spectrum of one frame \n",
      "with others.  \n",
      "5-5- Feature Selection and Dimension Reduction  \n",
      "Feature s election [23] is selecting features from a large set \n",
      "of extracted features to eliminate redundant and unused \n",
      "information and decrease processing time. For this work, \n",
      "the extracted pitch contains lots of information. The global \n",
      "features, like the range of p itch values and mean pitch \n",
      "values, are selected. Removing redundant and unused \n",
      "information, such as additional zeroes, duplicate values, or \n",
      "frames, is part of MFCC feature extraction. The exact \n",
      "process removes the silent speech frames.  \n",
      "Fundamental frequen cy and SDC are two speech features \n",
      "to identify gender. Similarly, Spectral centroid, Chroma \n",
      "STFT, MFCC, and Spectral flux features are used for \n",
      "emotion identification to create a single set of data points \n",
      "from multiple speech features using multifeature fu sion \n",
      "technique [24]. \n",
      "Multifeature Fusion:  The extracted speech features hav e \n",
      "different dimensions, so feature proximity usages  average \n",
      "dimensional spacing between the vectors to denote the \n",
      "proximity between the diverse features  [24]. The average \n",
      "dimensional spacing between the vectors is computed as  \n",
      "          \n",
      "    √∑∑              (8) \n",
      "   =  \n",
      "     √∑∑              (9) \n",
      "\n",
      " \n",
      "Journal of Information Systems and Telecommunication, Vol.12, No.2,  April -June 2024  \n",
      " \n",
      " \n",
      "157 \n",
      "        and     are the mean and variance interval of \n",
      "average dimensions of the feature vector s.    and    are \n",
      "the mean value of   and   type of sound features.     and    \n",
      "are the mean value of   and   type of sound features . Then \n",
      "the subsequent dimensionality reduction [25] method \n",
      "follows once the feature selection process is complete. \n",
      "High data variance is present in the extracted features \n",
      "containing more information. Dimension reduction \n",
      "techniques reduce the dimensions from extracted feature \n",
      "vectors.  \n",
      " \n",
      "5-5-1- Principal Component Analysis  (PCA)  \n",
      "PCA [25] is an approach for redu cing the dimensionality \n",
      "of extensive datasets, transforming them into more \n",
      "compact representations while retaining crucial \n",
      "information . These reductions in the number of variables \n",
      "help to get more accurate results. A reduced dataset makes \n",
      "it easier and fas ter to visualize and analyze the data. \n",
      "Following steps followed to explore PCA .  \n",
      " Standardization  creates a normalized dataset \n",
      "when there is a significant difference in the range \n",
      "of initial variables or a larger range of datasets \n",
      "dominates the smaller range. It can be done by  \n",
      "             \n",
      "                     (10) \n",
      " Covariance matrix computation  helps  to know  \n",
      "how the input  dataset  varies  from  the mean value \n",
      "to each data point.  \n",
      " Eigenvectors and eigenvalues  of the covariance \n",
      "matrix help to identify the principal component.  \n",
      "5-6- Modelling  \n",
      "This implementation uses two deep l earning models. \n",
      "Bidirectional Long Short -Term Memory (BiLSTM) \n",
      "classifies the genders in whispered speech, and Deep \n",
      "Convolutional Neural Network (DCNN) identifies \n",
      "emotions.  \n",
      "BiLSTM  [26] combines  two Recurrent Neural Networks \n",
      "(RNN) that are placed independently and can traverse \n",
      "backward and forward directions at each time step, as \n",
      "shown in Fig 7. There are two ways to deal with data in \n",
      "this model - The first involves processing data from the \n",
      "past to the future , and the second operates in the opposite \n",
      "direction, from future  to past , where two hidden layers of \n",
      "neurons help to preserve the information in both \n",
      "directions. This approach helps to improve the information \n",
      "available in the algorithm.  \n",
      "For B iLSTM, the sequence of inputs is \n",
      "                   for a traditional RNN, which \n",
      "computes the hidden vector sequences \n",
      "                  and results in the output \n",
      "sequences                    for the iteration \n",
      "    to                            (11) \n",
      "                (12) \n",
      "  are the weight matrices;   is the bias vector, and   is \n",
      "the hidden layer function.  \n",
      "DCNN  [27] is a Convolutional Neural Network (CNN) \n",
      "type that helps to identify emotions from whispered \n",
      "speech . The input to the model is the speech data that \n",
      "traverses through many stages, such as the convolution \n",
      "layer, the pooling layer, Activation, Fully Connected, \n",
      "Batch normalization, and dropout layers, as shown in Fig \n",
      "8.  \n",
      " \n",
      " \n",
      " \n",
      "Fig 7. Structure of BiLSTM  \n",
      " \n",
      "To avoid overfitting, Leaky ReLU is used as an activation \n",
      "function in this implementation and evaluated as  \n",
      "{                      \n",
      "                        (13) \n",
      "The fully connected layer is a loss layer, measuring the \n",
      "inconsistency between the desired and actual outputs . Root \n",
      "The Mean Squared Propagation (RMSProp) optimization \n",
      "algorithm enhances the loss function, which varies in \n",
      "vertical directions.   \n",
      " \n",
      "Fig. 8 Structure of Deep Convolution Neural Network  \n",
      "5-7- Emotions  \n",
      "Deep learning techniques are applied to identify the \n",
      "emotions of human speech. Discrete Emotion [3] \n",
      "theory uses seven vital emotions in Human activities.  \n",
      " Sadness:  This is the feeling of dissatisfaction, \n",
      "sorrow, or fruitlessness.  \n",
      " Happiness:  This is the emotional state that elicits \n",
      "satisfaction and pleasure.  \n",
      " Fear:  This feeling triggers a fright response.  \n",
      " Anger:  This emotional state leads to feelings of \n",
      "hostility and frustration.  \n",
      " Surprise:  It is the state of mind that expresses \n",
      "either positive or negative following something \n",
      "unexpected.  \n",
      "\n",
      " \n",
      "Mohanty, & Cherukuri, Whispered Speech Emotion  Recognition with Gender Detection using BiLSTM and DCNN  \n",
      " \n",
      " \n",
      " \n",
      "158 \n",
      " Disgust:  A strong emotion that results in feeling \n",
      "repulsed.  \n",
      " Neutral:  This is the feeling of lack of particular \n",
      "preference.  \n",
      " \n",
      "5-8- Algorithm for Emotion Recognition  \n",
      "The algorithm to detect emotions from the whispered \n",
      "speech is in two parts  \n",
      " Gender Detection  \n",
      " Emotion Identification   \n",
      " \n",
      "Algorithm 1:  Geneder Detection  \n",
      "INPUT: Text  File (Whispered Speech Samples)  \n",
      "OUTPUT: Emotions (Sad, Happy, Fear, Anger, Surprise, \n",
      "Disgust, Neutral)  \n",
      " \n",
      "1: Begin:  \n",
      "2: Read speech  samples from Corpus   \n",
      "3: Pre-processing  the speech  samples:  \n",
      "4: Processed_speech=Pre_processing (Speech_Sample)   \n",
      "5: Extracted  SDC,  Pitch  features:  \n",
      "6: SDC, Pitch=Feature_Extraction(Processed_speech)   \n",
      "7: Multifeature  fusion:  \n",
      "8: multi _feature = multifeatured _fusion(SDC,  Pitch)   \n",
      "9: Dimension  reduction:  \n",
      "10: Dimension _R = Dimension _Reduction(multi _feature)  \n",
      "11: Determine  gender:  \n",
      "12: Gender = BiLSTM  (Dimension _R) \n",
      "13: Placed  in folders:  \n",
      "14: Gender=  (Male,  Female)   \n",
      "15: End:  \n",
      "Algorithm 2 Emotion Identification  \n",
      " \n",
      "1:  Begin:   \n",
      "2:  Speech Samples = (Male,  Female)  \n",
      "3:  Data Augmentation:  \n",
      "4:  Augemented _samples = Data Augmentation  (Speech          \n",
      "Samples)  \n",
      "5:  Pre-processed  the speech  samples:   \n",
      "6:  Processed _speech=Preprocessing  (Augemented _sam \n",
      "ples)  \n",
      "7:  Extracted  Spectral  Centroid,  ChromaSTFT,  MFCC,  \n",
      "Mel spectrogram  features:   \n",
      "8:  Spectral _Centroid,  Chroma _STFT,  MFCC,  \n",
      "Mel_spectrogram = Feature Extraction  (Processed _spe \n",
      "ech)  \n",
      "9:  Multifeature  fusion:   \n",
      "10:  multi _feature = \n",
      "multifeatured _fusion(Spectral _Centroid,  Chroma _STFT,  \n",
      "MFCC,  Mel_spectrogr am) \n",
      "11:  Dimension  reduction:  \n",
      "12:  Dimension_R = Dimension_Reduction(multi_feature)  \n",
      "13:  Emotion  detection:  14:  Emotions = DCNN(Dimension_R)   \n",
      "15:  OUTPUT Emotions  (Sad, Happy, Fear, Anger, \n",
      "Surprise, Disgust, Neutral)  \n",
      "16:  End:  \n",
      "6- Experiment and Result Analysis  \n",
      "The proposed SER has been implemented utilizing Python \n",
      "programming language and fortified by various machine \n",
      "learning libraries and additional supporting libraries . The \n",
      "experiment uses Python (Python 3.6.3rc1) and Librosa \n",
      "(Librosa 0.8.0) for audio processing. Graph plotting uses \n",
      "Seaborn and Matplotlib libraries, which help to analyze the \n",
      "speech data. BiLSTM and DCNN models implement the \n",
      "model using Keras (Keras - 2.6.0), TensorFlow \n",
      "(TensorFlow -2.6.0), and Scikit -learn libraries .  \n",
      "wTIMIT dataset is collected and divided into the train, \n",
      "cross -validation, and test samples. Subsequently, the \n",
      "speech processing takes place during implementation, \n",
      "incorporating data -augmentation techniques like shifting \n",
      "time, changing pitch, and changing speed . The pre -\n",
      "processing of speech content from each sample helps to \n",
      "analyze it and extract its features. The dimensionally \n",
      "reduced extracted features align with the data points in the \n",
      "BiLSTM model. The training dataset makes the model \n",
      "learn the features or data. Epochs continuously learn \n",
      "hidden features when the dataset completes backward and \n",
      "forward iterations. Then the cross -validation dataset is \n",
      "used to estimate the model’s performance on each epoch \n",
      "and prevent the model from being overfit ted. The test \n",
      "dataset helps to evaluate the trained model unbiasedly \n",
      "using performance metrics like precision, recall, f1 score, \n",
      "and confusion Matrix .  \n",
      "The parameters used in BiLSTM models are as in Table 1.  \n",
      "Table 1: Parameters of the BiLSTM model  \n",
      "Layers  Shape of output  Parameters#  \n",
      "Embedding  \n",
      "(Embedding)  (None, 216, 256)  \n",
      " 524544  \n",
      " \n",
      "Bidirectional  \n",
      " (None, 128)  \n",
      " 98816  \n",
      "batch normalization  (None, 128)  \n",
      " 512 \n",
      "Dense (Dense)  (None, 128)  \n",
      " 10512  \n",
      "dropout (Dropout)  (None, 128)  \n",
      " 0 \n",
      "flatten (Flatten)  (None, 128)  \n",
      " 1024  \n",
      "dense1(Dense)  \n",
      " (None, 12)  \n",
      " 0 \n",
      "dense2(Dense)  \n",
      " (None, 12)  \n",
      " 455 \n",
      " \n",
      " \n",
      "Journal of Information Systems and Telecommunication, Vol.12, No.2,  April -June 2024  \n",
      " \n",
      " \n",
      "159 \n",
      "The BiLSTM model helps to segregate the speech samples \n",
      "into male and female. The segregated speech samples \n",
      "move to their respective male and female folders.  \n",
      "Table 2: Parameters of the DCNN model  \n",
      "Layers  Shape of output  Parameters#  \n",
      "conv1d (Conv1D)  \n",
      " (None, 216, 256)  \n",
      " 2304  \n",
      "Activation  \n",
      "(Activation)  (None, 216, 256)  \n",
      " 0 \n",
      "conv1d 1 (Conv1D)  \n",
      " (None, 216, 256)  524544  \n",
      "batch normalization  \n",
      " (None, 216, 256)  1024 \n",
      "activation 1  \n",
      "(Activation)  \n",
      " (None, 216, 256) 0 \n",
      "dropout (Dropout)  (None, 216, 256)  \n",
      " 0 \n",
      "max pooling1d \n",
      "(MaxPooling1D)  \n",
      " (None, 13, 256)  \n",
      " 0 \n",
      "conv1d 2 (Conv1D)  \n",
      " (None, 13, 128)  \n",
      " 262272  \n",
      " \n",
      "activation 2 \n",
      "(Activation)  \n",
      " (None, 13, 128)  \n",
      " 0 \n",
      "conv1d 3 (Conv1D)  \n",
      " (None, 13, 128)  \n",
      " 131200  \n",
      " \n",
      "activation 3 \n",
      "(Activation)  \n",
      " (None, 13, 128)  \n",
      " 0 \n",
      "conv1d 4 (Conv1D)  \n",
      " (None, 13, 128)  \n",
      " 131200  \n",
      " \n",
      "batch normalization  \n",
      " (None, 13, 128)  \n",
      " 512 \n",
      "activation 4 \n",
      "(Activation)  \n",
      " (None, 13, 128)  \n",
      " 0 \n",
      "conv1d 5 (Conv1D)  \n",
      " (None, 13, 128)  \n",
      " 131200  \n",
      " \n",
      "batch normalization \n",
      "1 \n",
      " (None, 13, 128)  \n",
      " 512 \n",
      "activation 5 \n",
      "(Activation)  \n",
      " (None, 13, 128)  \n",
      " 0 \n",
      "dropout 1 (Dropout)  \n",
      " (None, 13, 128)  \n",
      " 0 \n",
      "max pooling1d 1 \n",
      "(MaxPooling1D)  \n",
      " (None, 1, 128)  \n",
      " 0 \n",
      "conv1d 6 (Conv1D)  \n",
      " (None, 1, 64)  \n",
      " 65600  \n",
      "activation 6 \n",
      "(Activation)  \n",
      " (None, 1, 64)  \n",
      " 0 \n",
      "conv1d 7 (Conv1D)  \n",
      " (None, 1, 64)  \n",
      " 32832  \n",
      "activation 7 \n",
      "(Activation)  \n",
      " (None, 1, 64)  \n",
      " 0 \n",
      "flatten (Flatten)  \n",
      " (None, 64)  \n",
      " 0 dense (Dense)  \n",
      " (None, 14)  \n",
      " 910 \n",
      "activation 8 \n",
      "(Activation)  \n",
      " (None, 14)  \n",
      " 0 \n",
      " \n",
      "After separating the speech samples into genders, Chroma \n",
      "STFT, Spectral centroid, Mel-scale spectrogram, and \n",
      "Spectral Flux feature s extraction happen ed. Then the \n",
      "created data points are dimensionally reduced to fit into \n",
      "the DCNN deep learning model to identify emotions. \n",
      "Metrics like precision, recall, F1 score, and the confusion \n",
      "matrix aid in assessing the model's performance.  Table 2 \n",
      "mentions the parameters of the DCNN model.  \n",
      "6-1- Result Analysis  \n",
      "The fusion of Fundamental frequency and SDC \n",
      "detect s the gender of speech samples. The initial \n",
      "stage involves pre -processing the speech samples, \n",
      "followed by feature extraction, which includes SDC \n",
      "and fundamental frequencies. The combination of \n",
      "SDC and Fundamental speech features create a \n",
      "multifeature fusion resulting in a single set of data \n",
      "points. The next step is to reduce the dimensions of \n",
      "data p oints to use them as input to the BiLSTM \n",
      "Model. The BiLSTM model predicts the data in the \n",
      "dataset as male and female, placed in separate \n",
      "folders. Of these, 3342 speech samples are available, \n",
      "and 3294 are correctly classified. So, the accuracy of \n",
      "the model prediction is 99.59%. The precision, recall, \n",
      "and f1 -score values, along with the confusion matrix, \n",
      "are shown in Fig s 9 and 10. The predicted model \n",
      "identifies the female speech samples more accurately \n",
      "than the male.   \n",
      " \n",
      "Fig. 9 Confusion Matrix for Gender detection by BiLSTM.  \n",
      " \n",
      "The graph in Fig 9 shows the gender detection from the \n",
      "speech sample. The X -axis represents the true label, and \n",
      "the Y-axis shows the predicted label.  This graph gives the \n",
      "\n",
      " \n",
      "Mohanty, & Cherukuri, Whispered Speech Emotion  Recognition with Gender Detection using BiLSTM and DCNN  \n",
      " \n",
      " \n",
      " \n",
      "160 \n",
      "count of detected gender spe ech that the Bi -LSTM model \n",
      "correctly and incorrectly detects.  \n",
      " \n",
      "Fig. 10 Accuracy measures of Gender Detection.  \n",
      " \n",
      "2031 female and 1263 male speech are correctly detected, \n",
      "whereas 13 females and 35 males are incorrectly detected. \n",
      "Pitch values identify the emotions after detecting genders \n",
      "from the speech samples.  \n",
      "The extraction of MFCC, Mel -scale spectrogram, Chroma \n",
      "STFT, Spectral Flux, and Spectral Centroid speech \n",
      "features happens from speech samples. Then, all five \n",
      "speech features are fused into a single feature and \n",
      "dimensionally reduced. Finally, the DCNN model is \n",
      "applied to predict emotions.  \n",
      "Individual emotions are detected based on gender as \n",
      "shown in Fig 11. The deviation in the male speech \n",
      "emotions is less than the female . Female fear and female \n",
      "neutral  emotions show divergent results from other \n",
      "emotions. The accuracy of the model is 98.54%.   \n",
      " \n",
      " \n",
      "Fig.11 Emotions based on gender.  \n",
      "6-2- Comparison Analysis  \n",
      "This experiment evaluates the current implementation \n",
      "using the wTIMIT  dataset, resulting in an impressive \n",
      "accuracy of 98.54%. The performance compares with \n",
      "three -layered Long short -term memory Bidirectional \n",
      "RNNs (LSTM BiRNN) that use No -Attention, Feed \n",
      "Forward Attention (FFA), and Improved Feed Forward \n",
      "Attention Mechanism ( IFFA) to evaluate the emotions in \n",
      "the dataset. This model uses 35 hidden states to train the dataset. The models are trained and validated with the help \n",
      "of the wTIMIT dataset [28] and tested with the help of the \n",
      "CHAINS dataset,  as shown in Table 3. This ex periment is \n",
      "a new hypothesis for  emotion detection with gender \n",
      "identification on whispered speech.  Hence, there are fewer \n",
      "references available for similar investigations.  \n",
      "Table 3: Comparison of the Implementation  \n",
      "  Sl# Model Implementation  Accuracy  \n",
      "1 FFA, IFFA, LSTM, Bi_RNN [2 8] 97.6 %  \n",
      "2 Proposed Model  98.54 %  \n",
      "The proposed method improves the performance of \n",
      "emotions after segregating the speech samples into genders.  \n",
      "By separating the approach into two models, this \n",
      "implementation excels in capturing natural and \n",
      "spontaneous expressions of emotions with low latency \n",
      "manner .  \n",
      "7- Conclusion  and Future Work  \n",
      "Identification of gender from whispered speech and \n",
      "recognizing emotions is a complicated task. This \n",
      "implementation initially detects the gender from the \n",
      "speech samples before identifying the emotions. Emotions \n",
      "might vary based on gender in the same situation. Speech \n",
      "features such as Fu ndamental Frequency and SDC help \n",
      "with gender identification. MFCC, Mel -scale spectrogram, \n",
      "Spectral flux, Spectral Centroid, and Chroma STFT play a \n",
      "vital role in detecting emotions. Multifeature fusion helps \n",
      "to combine speech features into a single set of d ata points . \n",
      "The concept verifies with the publicly available dataset \n",
      "wTIMIT with an accuracy of 98.54%. This approach helps \n",
      "to identify nearly inaudible emotions and is used to figure \n",
      "out the strategy. The proposed methodology can be \n",
      "improved using other s peech features, machine learning, \n",
      "and deep learning concepts.  \n",
      " \n",
      "References  \n",
      "[1]  ST Jovicic , and Z  Saric , ―Acoustic analysis of  consonants in \n",
      "whispered speech,‖ Journal of voice , vol 22, no. 3, pp. 263–\n",
      "274, 2008 . \n",
      "[2]  M Kumari , and I  Ali, ―An efficient algorithm for gender \n",
      "detection using voice samples,‖ 2015 Communication, \n",
      "Control and Intelligent Systems (CCIS),  2015 , Mathura, \n",
      "Utter Pradesh,  pp. 221–226, doi: \n",
      "10.1109/CCIntelS.20 15.7437912 . \n",
      "[3] S Motamed , S Setayeshi, A Rabiee , and A Sharifi, ―Speech \n",
      "Emotion Recognition Based on Fusion Method,‖ Journal of \n",
      "Information Systems and Telecommunication (JIST), vol. 3, \n",
      "pp. 50--56, 2017, doi: 10.7508/jist.2017.17.007 .  \n",
      "[4] JS Li, CC Huang , ST Sheu , and MW  Lin, ―Speech emotion \n",
      "recognition and its applications,‖ Proc. of Taiwan Institute of \n",
      "Kansei Conference , 2010  Paris , France , pp. 187–192.  \n",
      "\n",
      " \n",
      "Journal of Information Systems and Telecommunication, Vol.12, No.2,  April -June 2024  \n",
      " \n",
      " \n",
      "161 \n",
      "[5] A Guerrieri , E Braccili , F Sgro , and GN Meldolesi  ―Gender \n",
      "identification in a two -level hierarchical speech emotion \n",
      "recognition system for an Italian Social Robot,‖ Sensors, vol. \n",
      "22, no. 5, pp. 1714, 2022 , doi: 10.3390/s22051714 . \n",
      "[6] M Sarria -Paja, TH Falk, and D  O’Shaughnessy , ―Whispered \n",
      "speaker verification and gender detection using weighted \n",
      "instantaneous frequencies,‖ 2013 IEEE International \n",
      "Conference on Acoustics, Speech and Signal Processing , \n",
      "2013  (May 26 -31), Vancouver , Canada, pp. 7209 –7213 , doi: \n",
      "10.1109/ICASSP.2013.6639062 .  \n",
      "[7] J Deng , S Fruhholz, Z Zhang , and Bojrn Schuller , \n",
      "―Recognizing emotions from whispered speech based on \n",
      "acoustic feature transfer learning,‖ IEEE Access , vol.  5, pp. \n",
      "5235 –5246 , 2017 .  \n",
      "[8] M Cotescu , T Drugman, G Huybrechts, J Lorenzo -Trueba, \n",
      "and A Moinet , ―Voice conversion for whispered speech \n",
      "synthesis,‖  IEEE Signal Processing Letters , vol. 27, pp. 186–\n",
      "190, 2019 , doi: 10.1109/LSP.2019.2961213 .  \n",
      "[9] P Mishra , and R Sharma, ―Gender differentiated \n",
      "convolutional neural networks for speech emotion \n",
      "recognition,‖ 2020 12th International Congress on Ultra -\n",
      "Modern Telecommuni - cations and Control Systems and \n",
      "Workshops (ICUMT),  2020  (October 5 -7), Brno , Czech \n",
      "Republic,  pp. 142–148, doi: \n",
      "10.1109/ICUMT51630.2020.9222412 . \n",
      "[10] Mustaqeem S Kwon, ―Optimal feature selection based \n",
      "speech emotion recognition using two -stream deep \n",
      "convolutional neural network,‖ International Journal of \n",
      "Intelligent Systems , vol. 36, no. 9, pp. 5116 – 5135 , 2021 , \n",
      "doi:  10.1002/int.22505 . \n",
      "[11] J. Ancilin and A. Milton, ―Improved speech emotion \n",
      "recognition with mel frequency magnitude coefficient ,‖ \n",
      "Applied Acoustics , vol. 179, pp. 108046 , 2021 , doi: \n",
      "10.1016/j.apacoust.2021.108046 .  \n",
      "[12] S. Jothimani and K. Premalatha, ―Mff -saug: Multi feature \n",
      "fusion with spectrogram augmentatio n of speech emotion \n",
      "recognition using convolution neural network ,‖ Chaos, \n",
      "Solitons & Fractals , vol. 162, pp. 112512 , 2022 , doi: \n",
      "10.1016/j.chaos.2022.112512 .  \n",
      "[13] B Yalamanchili, SK Samayamantula, and KR Anne, ―Neural \n",
      "network -based blended ensemble learning for speech \n",
      "emotion recognition,‖ Multidimensional Systems and Signal \n",
      "Processing, vol. 33, no. 4, pp. 1323 --1348 , 2022 , doi: \n",
      "10.1007/s11045 -022-00845 -9.  \n",
      "[14] T Feng , R Hebbar , and S Narayanan, ―Trustser: On the \n",
      "trustworthiness of fine -tuning pre -trained speech embeddings \n",
      "for speech emotion recognition,‖ arXiv preprint \n",
      "arXiv:2305.11229, 2023 , doi:  10.48550/arXiv.2305.11229  \n",
      "[15] RV Darekar and M Chavan , S Sharanyaa,  and NR Ranjan , \n",
      "―A hybrid meta -heuristic ensemble based classification \n",
      "technique speech emotion recognition,‖ Advances in \n",
      "Engineering Software, vol. 180, pp. 103412 , 2023 . \n",
      "[16] J Rekimot o, ―Dualvoice: A speech interaction method using \n",
      "whisper -voice as commands,‖ CHI Conference on Human \n",
      "Factors in Computing Systems Extended Abstracts,  pp. 1–6, \n",
      "2022 , doi: 10.1145/3491101.3519700 . \n",
      "[17]H Dolka,  AX VM , and S Juliet,―  Speech  emotion r ecognition  \n",
      "using  ANN on MFCC  features,‖ 2021 3rd international \n",
      "conference on signal processing and communication (ICPSC) , \n",
      "2021, Coimbatore, India, pp. 431–435, doi: \n",
      "10.1109/ICSPC51351.2021.9451810 . [18] MK Reddy and KS Rao, ―Robust pitch extraction method \n",
      "for the hmm -based speech synthesis system,‖ IEEE signal \n",
      "processing letters , vol.  24, no. 8, pp. 1133 –1137 , 2017 , doi: \n",
      "10.1109/LSP.2017. 2712646 .  \n",
      "[19] J Chatterjee, V Mukesh, HH Hsu , G Vyas , and Z Liu , \n",
      "―Speech emotion recognition using cross - correlation and \n",
      "acoustic features,‖ 2018 IEEE 16th Intl Conf on Dependable, \n",
      "Autonomic and Secure Computing, 16th Intl Conf on \n",
      "Pervasive Intelligence and Computing, 4th Intl Conf on Big \n",
      "Data Intelligence and Computing and Cyber Science and \n",
      "Technology Congress , 2018  (August 12-15), Athens, Greece , \n",
      "pp. 243–249, doi: 10.1109/DASC/PiCom/DataCom/CyberSci  \n",
      " Tec.2018.00050 .  \n",
      "[20] S Rajesh and NJ Nalini, ―Musical instrument emotion \n",
      "recognition using deep recurrent neural network,‖ Procedia \n",
      "Computer Science, vol. 167, pp. 16 --25, 2020, doi: \n",
      "10.1016/j.procs.2020.03.178 . \n",
      "[21] M Aly , and NS Alotaibi, ―A novel deep learning model to \n",
      "detect covid -19 based on wavelet features extracted from \n",
      "mel- scale spectrogram of patients’ cough and breathing \n",
      "sounds,‖ Informatics in Medicine Unlocked , vol.  32, pp.  \n",
      "101049 , 2022, doi: 10.1016/j.imu.2022.101049 . \n",
      "[22] Z Qawaqneh,  AA Mallouh,  and BD Barkana, ― Age and \n",
      "gender  classification  from  speech and face images by jointly \n",
      "fine-tuned deep neural networks,‖ Expert Systems with \n",
      "Applications , vol. 85, pp. 76–86, 2017 , doi: \n",
      "10.1016/j.eswa.2017.05.037 .  \n",
      "[23] A Koduru, HB Valiveti and AK Budati, ―Feature extraction \n",
      "algorithms to improve the speech emotion recognition rate,‖ \n",
      "International Journal of Speech Technology , vol. 23, no. 1, \n",
      "pp. 45–55, 2020 , doi: 10.1007/s10772 -020-09672 -4.  \n",
      "[24] S Zhang , and C Li, ―Research on feature fusion speech \n",
      "emotion recognition technology for smart teaching,‖ Mobile \n",
      "Information Systems , vol. 2022 , 2022 , doi:  \n",
      "10.1155/2022/7785929 . \n",
      "[25] GB, Prasanna, SV Bhat, C Naik , and HN Champa, ―An \n",
      "Efficient Method for Handwritten Kannada Digit \n",
      "Recognition based on PCA and SVM Classifier,‖ Journal of \n",
      "Information Systems and Telecommunication (JIST), vol. 3, \n",
      "no. 35, pp. 169 2021, doi: 20.1001.1.23221437.2021.9.35.3.2 . \n",
      "[26] A Graves, N Jaitly, and A Mohamed, ―Hybrid speech \n",
      "recognition wit h deep bidirectional lstm,‖ 2013 IEEE \n",
      "workshop on automatic speech recognition and \n",
      "understanding , 2013, Olomouc, Czech Republic, pp.  273–\n",
      "278, doi: 10.1109/ASRU.2013.6707742 .  \n",
      "[27] N Aloysius and M Geetha, ―A review on deep convolutional \n",
      "neural networks,‖ 2017 international conference on \n",
      "communication and signal processing (ICCSP), 2017, pp. \n",
      "0588 –0592 , IEEE, doi: 10.1109/ICCSP.2017.8286426 .  \n",
      "[28] SBC Gutha, MAB Shaik, T Udayakumar , and AA \n",
      "Saunshikhar , ―Improved feed forward attention mechanism in \n",
      "bidirectional recurrent neural networks for robust sequence \n",
      "classification,‖ 2020 International Conference on Signal \n",
      "Processing and Communications (SPCOM) , 2020 , IISc, \n",
      "Bangalore.  IEEE , pp. 1—5, doi: \n",
      "0.9179606 10.1109/SPCOM50965.202  \n",
      "\n",
      "\n",
      "Reading: d:\\PythonCode\\document_portal\\notebook\\data\\Prospect-of-Low-Power-Sensor-Network-Technology-in-Disaster-Management-for-Sustainable-Future.pdf\n",
      " \n",
      " \n",
      " \n",
      "Journal of Information Systems and Telecommunication  \n",
      "Vol.12, No.2,  April -June 2024, 152-161 \n",
      " \n",
      "http://jist.acecr.org  \n",
      "ISSN 2322 -1437  / EISSN:2345 -2773  \n",
      " \n",
      "Whispered Speech Emotion Recognition with Gender Detection using \n",
      "BiLSTM and DCNN  \n",
      "Aniruddha Mohanty1*, Ravindranath C Cherukuri1 \n",
      " \n",
      "1.Department of Computer  science and Engineering, Christ (Deemed to be University), Bangalore, India  \n",
      " \n",
      " \n",
      "Received: 22 Aug 2023 / Revised: 04 Apr 2024 / Accepted: 21 May 2024  \n",
      " \n",
      " \n",
      "Abstract   \n",
      "Emotions are human mental states at a particular instance in time concerning one’s circumstances, mood, and relationships \n",
      "with others . Identifying emotions from the whispered speech is complicated as the conversation might be confidential. The \n",
      "representation of the speech relies on the magnitude of its information. Whispered speech is intelligible, a low -intensi ty \n",
      "signal, and varies from normal speech. Emotion identification is quite tricky from whispered speech. Both prosodic and \n",
      "spectral speech features help to identify emotions. The emotion identification in a whispered speech happens using prosodic \n",
      "speech fea tures such as zero -crossing rate  (ZCR) , pitch, and spectral features that include spectral centroid, chroma STFT, \n",
      "Mel scale spectrogram, Mel -frequency cepstral coefficient (MFCC), Shifted Delta Cepstrum (SDC), and Spectral Flux. \n",
      "There are two parts to the proposed implementation. Bidirectional Long  Short -Term Memory (BiLSTM) helps to identify \n",
      "the gender from the speech sample in the first step with SDC and pitch. The Deep Convolutional Neural Network (DCNN) \n",
      "model helps to identify the emotions in the second  step. This implementation is evaluated using the  wTIMIT data corpus \n",
      "and gives 98.54% accuracy. Emotions have a dynamic effect on genders, so this implementation performs better than \n",
      "traditional approaches. This approach helps to design online learning management systems, different applications for \n",
      "mobile devices, checking cyber -criminal activities, emotion detection for older people, automatic speaker identification and \n",
      "authentication, forensics, and surveillance . \n",
      " \n",
      "Keywords:  Whispered Speech; Emotion Re cognition; Speech Features; Data Corpus; BiLSTM;  DCNN . \n",
      " \n",
      "1- Introduction  \n",
      "Emotions are the humans’ short -lived feeling s that affects \n",
      "thinking, actions, relationships, and social interactions. \n",
      "Emotions express humans’ physiological and emotional \n",
      "states with facial expressions and body language. \n",
      "Whispered speech is a form of speec h that expresses \n",
      "emotions. Whispered speech is a type of communication \n",
      "produced with breath without any noise and excitation of \n",
      "voice. The whispered speech structure changes \n",
      "significantly because of the lack of periodic excitation in \n",
      "the voice. This result s in missing speech and reduced \n",
      "transparency in communication.  \n",
      "The difference between normal and whispered speech is \n",
      "the absence of vocal tract vibrations due to the vocal \n",
      "tract’s physiological blocking. The strength of whispered \n",
      "speech is minute and with out voice compared to phoned \n",
      "speech. The spectral and prosodic features help to detect \n",
      "the whispered speech. Prosodic features of speech vary \n",
      "over time. Spectral features of whispered speech are highly accurate over unvoiced consonants, voiced consonants, \n",
      "and formants in vowels [1]. \n",
      "The impacts on Speech Emotion Recognition ( SER) are \n",
      "due to various acoustic conditions such as compressed, \n",
      "noisy, telephonic conversation s and imitator speeches. \n",
      "Other environmental conditions,  such as stress, rhythm, \n",
      "and intonation, can affect  SER.  Whispered speech is also \n",
      "affected by similar acoustic and environmental conditions. \n",
      "SER depends on acoustic characteristics and gender in \n",
      "some scenarios. Hence gender plays a vital role in SER.  \n",
      "Nowadays, several whispered speech  samples of male and \n",
      "female voices are available. The effectiveness of the SER \n",
      "is more when the implementation is in two parts. The \n",
      "identification of the gender [2] happens initially using \n",
      "Bidirectional Long Short -term Memory (BiLSTM)  with \n",
      "speech features . The next step is detecting emotions using \n",
      "various speech features [3] and Deep Convolutional \n",
      "Neural Network (DCNN).   \n",
      "The structure of the paper includes various sections. \n",
      "Section 2 describes Human Emotions and their \n",
      "Applications. Details of the Related Work are in Section 3. \n",
      "Section 4 describes the System Model, which is the black \n",
      "box view of the implementation. Section 5 is the Model \n",
      " \n",
      "Journal of Information Systems and Telecommunication, Vol.12, No.2,  April -June 2024  \n",
      " \n",
      " \n",
      "153 \n",
      "Design that describes the details of the speech features and \n",
      "the deep learning models used in this implementation. \n",
      "Section 6 gives the Experiment and Result Analysis. \n",
      "Section 7 brief ly discuss  the Conclusion and Future Work.   \n",
      "2- Human  Emotions  and their  Applications  \n",
      "Human emotions are the mental state caused by countless \n",
      "associated views , feeling s, behavioral replies, and the \n",
      "degree of pressure and annoyance. It is often associated \n",
      "with attitude , temperament, behavior , disposition, and \n",
      "creativi ty [4]. Emotion recognition helps to detect a \n",
      "humans ’ emotional mood , which lasts hours and days. \n",
      "Speech conveys emotions such as anger, disgust, fear, \n",
      "happiness, neutrality, sadness, and surprise. The machine \n",
      "automatically detects various emotions from speech with \n",
      "the help of different algorithms. Emotion Recognition \n",
      "helps to : \n",
      " \n",
      "•  Detect customers ’ intentions based on the teleconference. \n",
      "•  Detect cybercrime.  \n",
      "•  Students ’ attention and teachers ’ content adjustment.  \n",
      "•  Disability assistance  \n",
      "•  Customer satisfaction  \n",
      "•  Stress monitoring  \n",
      "•  Social media analysis  \n",
      "•  Suspicious activity  \n",
      "•  Human -machine interaction and so on.   \n",
      "3- Related Work  \n",
      "Over time, numerous studies have detected emotions from \n",
      "whispered speech . The vario us deep learning models \n",
      "detect emotions based on the speech features extracted \n",
      "from the collected whispered speech data corpus . The SER \n",
      "for normal and whispered speech is diverse because of \n",
      "vocal excitation. Emotions vary with many factors; gender \n",
      "is among  the most influential factors  [5]. Identifying \n",
      "gender in the first step improves emotion detection from \n",
      "whispered speech. So, the related work explored is on \n",
      "gender detection and emotion recognition from whispered \n",
      "speech.  \n",
      "MFCC obtained by the Hilbert enve lope approach and \n",
      "weighted instantaneous frequencies (WIFs) obtained by \n",
      "the coherent demodulation help to detect gender in \n",
      "whispered speech  samples [6]. There is an opportunity to \n",
      "explore gender detection in noisy speech conditions using \n",
      "these approaches.  \n",
      "Autoencoder -enabled features  in the transfer learning \n",
      "framework propose to practice  phonated data to identify \n",
      "emotions from Whispered speech [7]. The feature \n",
      "extraction is from the Geneva Whispered Emotion Corpus \n",
      "(GeWEC) and Berlin Emotional Speech Databa se (EMO -\n",
      "DB) data corpus. The acoustic features such as Mel-frequency cepstral coefficient (MFCC), root mean square \n",
      "(RMS), frame energy, zero -crossing -rate (ZCR), pitch \n",
      "frequency (F0), probability of voicing autocorrelation \n",
      "function are the inputs to evaluate the framework. \n",
      "Implementing deep learning concepts on spontaneous data \n",
      "gives more accuracy than the current framework .  \n",
      "Gender is detected to target an anonymous speaker. The \n",
      "Deep Neural Network (DNN) model is used to generalize \n",
      "gender by using the MFCC speech feature. This \n",
      "implementation is applied to the wTIMIT dataset to verify \n",
      "the gender and recognize the speaker  [8]. The DNN model \n",
      "cannot generalize gender; evaluation can happen with \n",
      "other datasets.   \n",
      "MFCC and CNN, with the fully  connected network, detect \n",
      "gender and emotions like anger, disgust, fear, happiness, \n",
      "sadness, surprise, and a neutral state  [9]. The concept \n",
      "verification happens on RAVDESS, CREMA -D, SAVEE, \n",
      "and TESS datasets, having an accuracy of 92.283%, which \n",
      "is better than the tr aditional model.  \n",
      "The final prediction of the implemented model is to learn \n",
      "the mutually spatial -spectral features happens by a two -\n",
      "stream deep convolutional neural network with an iterative \n",
      "neighborhood  component analysis (INCA) and the most \n",
      "discriminatory optimal features  [10]. The concept \n",
      "verification happens on EMO -DB, SAVEE, and \n",
      "RAVDESS emotional speech corpora which perform with \n",
      "95%, 82%, and 85% accuracy rates. Real -time applications \n",
      "with natural and huge data corpora can help to extend this \n",
      "concept to identify emotions.  \n",
      "Mel Frequency Magnitude Coefficient (MFMC) and three \n",
      "spectral features, namely MFCC, log frequency power \n",
      "coefficient, and linear prediction cepstral coefficient, are \n",
      "used with the help of Support Vector Machine (SVM) \n",
      "modeling  [11]. The performance evaluation uses this \n",
      "concept  on Berlin, RAVDESS, SAVEE, EMOVO, and \n",
      "eNTERFACE data corpus . Feature selection, feature \n",
      "fusion, and multiple classification schemes improve the \n",
      "performance of MFMC.  \n",
      "The proposed MFF -SAug research in  which noise removal \n",
      "improves emotion. White Noise Injection, Pitch Tuning \n",
      "techniques, MFCC, ZCR, RMS speech features, and \n",
      "Convolutional Neural Network (CNN) modeling [12] \n",
      "detect emotions. Emotion detection during the interaction \n",
      "between people can extend the MFF -SAug approach .  \n",
      "The proposed Neural Network -based Blended Ensemble \n",
      "Learning (NNBEL) model is composed of a 1 -dimensional \n",
      "Convolution Neural Network (1D -CNN), Long Short -\n",
      "Term Memory (LSTM), and CapsuleNets. LSTM receives \n",
      "input from the Log Mel -spect rogram speech features, \n",
      "while 1D -CNN and CapsuleNets receive input from the \n",
      "MFCC.  Each model’s output is fed to Multi -Layer \n",
      "Perceptron (MLP) and predicts the final emotion s [13]. \n",
      "This model shows 95.3% and 94% accuracy on \n",
      "RAVDESS and IEMOCAP datasets, respectively.  \n",
      " \n",
      "Mohanty, & Cherukuri, Whispered Speech Emotion  Recognition with Gender Detection using BiLSTM and DCNN  \n",
      " \n",
      " \n",
      " \n",
      "154 \n",
      "TrustSER [14] implemented a general framework to \n",
      "determine the SER system's trustworthiness using deep \n",
      "learning techniques. Trustworthiness evaluates privacy \n",
      "(gender in formation, speaker demographics), safety, \n",
      "fairness, sustainability, and emotions (sad, angry, happy, \n",
      "neutral). The architecture of the TrustSET framework uses \n",
      "CNN encoder and Transformer encoder models. \n",
      "Trustworthy profiles under the Federated Learning \n",
      "scenario might improve privacy, fairness, and safety.  \n",
      "A hybrid meta -heuristic ensemble -based classification [15] \n",
      "helps to detect speech emotions. Raw speech samples are \n",
      "filtered using the Butterworth filter; then, spectrogram \n",
      "speech features are extracted from each frame to create a \n",
      "hybrid feature vector. The ensemble -based classification is \n",
      "applied to hybrid feature vectors to classify em otions. The \n",
      "ensemble -based classification contains a Recurrent Neural \n",
      "Network (RNN), a Deep Belief Network (DBN), and an \n",
      "Artificial Neural Network (ANN).  \n",
      "The above -related work shows that it is crucial to identify \n",
      "gender to segregate emotions in a speech. Male and female \n",
      "emotions are different based on situations. Whispered \n",
      "speech is an essential concept of emotion recognition. \n",
      "There is an opportunity to experiment further based on \n",
      "other datasets of whispered speech. So, this is a motivation \n",
      "to work on emot ion recognition in whispered speech, as \n",
      "this is a less explored area of research .  \n",
      "4- System Model  \n",
      "The analysis of e motion recognition from the whispered \n",
      "speech segregates into two parts. One is gender detection, \n",
      "and the second is emotion detection as shown in Fig 1.  \n",
      " \n",
      " \n",
      "Fig. 1 Emotion recognition from whispered speech  \n",
      " \n",
      "As part of gender detection, pre -processing of the speech \n",
      "samples is performed, and then pitch and SDC are \n",
      "extracted as part of prosodic and spectral features, \n",
      "respectively. Both the features are fused to get a single \n",
      "feature set with the help of multifeature fusion. PCA helps \n",
      "to reduce the dimensions of extracted features and is used \n",
      "as input to Bidirectional Long short -term memory \n",
      "(BiLSTM) to classify genders , as shown in Fig 2.  \n",
      " \n",
      " \n",
      "Fig .2 Gender detection from whispered speech  \n",
      " \n",
      "Following  Gender detection, the augmentation of speech \n",
      "data helps to get more realistic data. Then the speech features such as Chromatogram, Zero -Crossing Rate, \n",
      "Spectral Central, MFCC, Spectral Flux , and Mel \n",
      "Spectrog rams are extracted. All the extracted features \n",
      "combine to get a single feature using multifeature fusion. \n",
      "Then the dimensions are reduced to get the optimal data \n",
      "points. The data points are inserted into the Deep \n",
      "Convolutional Neural Network (DCNN) to identify the \n",
      "emotions , as shown in Fig 3.  \n",
      "5- System  Design  \n",
      "The implementation of system design happens in two parts. \n",
      "The first part identifies the male and female gender from \n",
      "the whispered speech samples. Then the determination of \n",
      "different emotions like sadn ess, happiness, fear, anger, \n",
      "surprise, disgust, and neutral are detected in gender -\n",
      "segregated speech samples.  \n",
      " \n",
      "Fig. 3 Emotion Recognition from Male and Female whispered speech.  \n",
      "5-1- Whispered Speech Data Corpus  \n",
      "wTIMIT [16] is a whispered voice data corpus ha ving 450 \n",
      "phonetically balanced sentences with 29 speakers. There \n",
      "are 11,324 utterances with a normal voice , which  can be \n",
      "used for normal and whispered training. This data is \n",
      "available in two parts - train and test divisions. The \n",
      "samples contain an 8 kHz sampling rate with a high -\n",
      "quality  and low-pass filter.  \n",
      "5-2-    Data Augmentation  \n",
      "Data augmentation [17 ] is the method of getting more \n",
      "realistic data from the existing data, which helps to add \n",
      "more training data to the model, reduce overfitting, and \n",
      "increase the model’s generalization ability. Below are the \n",
      "steps to generate synthetic data.  \n",
      " Shifting Time : Shifting time is a simple concept \n",
      "where the audio shifts to the left or right with a \n",
      "haphazard second. If the audio shifts to the left \n",
      "\n",
      " \n",
      "Journal of Information Systems and Telecommunication, Vol.12, No.2,  April -June 2024  \n",
      " \n",
      " \n",
      "155 \n",
      "with z seconds, then the first z seconds are \n",
      "marked as silence. Similarly, if the audio shifts to \n",
      "the right with z second s, the last z seconds are \n",
      "marked as silence.  \n",
      " Changing Pitch: Pitch change is adjusting the \n",
      "pitch randomly without upsetting the speediness \n",
      "of the audio file.  \n",
      " Changing Speed: Speed audio changes by \n",
      "stretching the time series data by a fixed rate.  \n",
      "5-3- Pre-processing  \n",
      "After the data collection and data augmentation, pre -\n",
      "processing [3] is the initial  phase  in training SER. \n",
      "Framing, Windowing, Voice activity detection, \n",
      "Normalization, and Noise reduction are pre -\n",
      "processing steps.   \n",
      "Framing:  Speech signals are quasip eriodic and vary \n",
      "over time. Hence, information and related emotions \n",
      "also fluctuate over time. The segregation of the \n",
      "signals happens in a shorter period to make the \n",
      "speech signal invariant. Twenty milliseconds to thirty \n",
      "milliseconds helps to make the speec h signal \n",
      "invariant, and five milliseconds of overlap of the \n",
      "frames avoid data leakage between the frames .  \n",
      "Windowing:  Windowing on each frame helps to diminish \n",
      "data leakage during Fast Fourier Transformation (FFT) \n",
      "after framing. The Hamming window allows  this step \n",
      "where the window size is N for the frames     , used in \n",
      "equation (1).  \n",
      " \n",
      "                  (   \n",
      "   )       (1) \n",
      "            \n",
      " \n",
      "Voice activity detection (VAD):  An utterance has three \n",
      "portions of speech activities : voiced, unvoiced, and silent. \n",
      "Zero Crossing Rate (ZCR) speech feature helps to detect \n",
      "VAD. ZCR represents the frequency of signal transitions \n",
      "between positive and negative values within a specific \n",
      "frame . Due to high energy, the ZCR value is low for \n",
      "voiced speech and high for unvoiced s peech because of \n",
      "low energy.  \n",
      "Normalization:  Normalization helps to reduce  speaker \n",
      "and recording inconsistency without affecting the features’ \n",
      "strength and enhancing the features’ generalization \n",
      "capability. The Z-normalization method is used more and \n",
      "represented as  \n",
      "       \n",
      "      (2) \n",
      "Where   is the speech signal,   and   are the mean and \n",
      "standard deviation of the data, respectively.  \n",
      "Noise Reduction:  The environmental noise captured while \n",
      "recording a speech signal  affects the recognition rate . \n",
      "Minimum mean square error (MMSE) and log -spectral \n",
      "amplitude MMSE (LogMMSE) reduce the noise from the speech signals. Noise reduction helps to get more accuracy \n",
      "in SER evaluation.  \n",
      "5-4- Feature Extraction  \n",
      "Specific emotions are present in prosodic and spectral \n",
      "features of speech. The extraction of prosodic and spectral \n",
      "features is a vital characteristic of emotion recognition \n",
      "after pre -processing the speech signals.  \n",
      "5-4-1-Prosodic Features  \n",
      "Prosodic features deal with the audio qualities of a speech \n",
      "when connected speeches use sounds as input. The \n",
      "production of the speech deals with the amount of energy, \n",
      "frequency, period, loudness, pitch, and duration. Speech \n",
      "signal com munication depends on intonation, stress, and \n",
      "rhythm, which prosodic features can detect. The prosodic \n",
      "features used in the implementation are : \n",
      "Zero -Crossing Rate (ZCR):  ZCR [17] measures the \n",
      "frequency of signal transitions between positive and \n",
      "negative values in every audio frame  and defined as  \n",
      "    \n",
      "    ∑       [ ]         [      \n",
      "    (3) \n",
      "Where        is the sign function . \n",
      "    [     ]  {        \n",
      "       \n",
      "Fundamental Frequency (Pitch):  Fundamental \n",
      "Frequency (F0) [18] is the minimum freque ncy of the \n",
      "periodic waveform. F0 is the significant parameter to \n",
      "differentiate male speech from female speech. Pitch also \n",
      "determines the voiced and unvoiced portion of the speech \n",
      "signal. This analysis uses pitch parameters like pitch mean \n",
      "value and pitch r ange.  \n",
      "The pitch range determines the number of octaves a speech  \n",
      "sample  can cover, from the lowest to the highest. F0 value \n",
      "varies from 85Hz to 180 HZ for the voiced speech of adult \n",
      "males.  \n",
      " \n",
      " \n",
      "Fig. 4 Representation of Pitch data  \n",
      " \n",
      "Similarly, the F0 value for adult females goes from 165Hz \n",
      "to 255Hz, as shown in Fig 4.  \n",
      "\n",
      " \n",
      "Mohanty, & Cherukuri, Whispered Speech Emotion  Recognition with Gender Detection using BiLSTM and DCNN  \n",
      " \n",
      " \n",
      " \n",
      "156 \n",
      "5-4-2-Spectral features  \n",
      "The representation of spectral features happens by \n",
      "converting the time -domain speech signal into the \n",
      "frequency domain with the help of the Fast Fourier \n",
      "Transform (FFT) which  benefits  to represent the \n",
      "characteristics of the Human Vocal Tract. The  spectral \n",
      "features are  \n",
      "Spectral Centroid (SC):  The Spectral Centroid [19] is the \n",
      "most used speech feature where the positioning of the \n",
      "―center of mass‖ of the audio signal spectr um defines the \n",
      "weight of the spectrum. The center of mass measures the \n",
      "weighted average of the frequency component located in \n",
      "the audio signal, defined as  \n",
      " \n",
      "      ∑            \n",
      "   \n",
      "∑       \n",
      "       (4) \n",
      "Where      represents the magnitude of bin numb er   and \n",
      "      represents the central frequency of the bin.  \n",
      "Chroma STFT:  Chroma STFT [20] is obtained using FFT \n",
      "on speech samples and the resulting spectrums a re a \n",
      "chromatogram in a vertical axis. This feature captures the \n",
      "harmonic feature of the speech s ignals.  \n",
      "Mel-scale Spectrogram:  Mel-scale Spectrogram [ 21] is a \n",
      "spectrogram in which frequencies convert to the Mel scale, \n",
      "which helps to differentiate the range of frequencies. Mel -\n",
      "spectrogram helps to understand emotions in a better way \n",
      "as humans can perceive sound on a logarithmic scale.  \n",
      "Mel Frequency Cepstral Coefficient (MFCC ): MFCC \n",
      "[17] i s SER’s  standard feature extraction technique. The \n",
      "vocal cord s, tongue, and teeth filter the sound and make it \n",
      "unique for each speaker in the Human Speech production \n",
      "system. Mel scale represents MFCC, where the frequency \n",
      "bands are equally spaced and close to the Human Auditory \n",
      "System’s response, as shown in Fig 5.  \n",
      " \n",
      "Fig 5 MFCC speech feature extraction  \n",
      " \n",
      "Shifted Delta Cepstrum (SDC):  Time derivatives apply \n",
      "to the cepstral coefficients obtained from the MFCC and \n",
      "combined with the delta coefficient to get SDC, as shown \n",
      "in Fig 6.       and   are the four parameters used in \n",
      "SDC [ 22].  M denotes the cepstral coefficient for each \n",
      "frame.   indicates added frames.   are the frames that \n",
      "append delta features from the new feature vector .   \n",
      "represents the delta values difference. So, the coefficient \n",
      "vectors represent as   \n",
      "     [                  ]   (5) \n",
      "   are MFCC coefficients and   is the coefficient index. \n",
      "For a given time,   an intermediate calculation is done to \n",
      "obtain the   coefficients.  \n",
      "                                 (6) \n",
      "Finally, the SDC coefficients vectors of   dimensions are \n",
      "obtained as  \n",
      "       [          (                )] (7) \n",
      " \n",
      " \n",
      "Fig. 6 Extraction of SDC speech feature  \n",
      " \n",
      "This speech feature helps to identify genders for a long -\n",
      "range dynamic appearance in speech signals.  \n",
      "Spectral Flux:  Spectral flux [ 20] measures the change in \n",
      "speed of the signal’s power spectrum compared to the \n",
      "previous frame. This feature estimates the speech signal’s \n",
      "power spectrum about  the power spectrum of one frame \n",
      "with others.  \n",
      "5-5- Feature Selection and Dimension Reduction  \n",
      "Feature s election [23] is selecting features from a large set \n",
      "of extracted features to eliminate redundant and unused \n",
      "information and decrease processing time. For this work, \n",
      "the extracted pitch contains lots of information. The global \n",
      "features, like the range of p itch values and mean pitch \n",
      "values, are selected. Removing redundant and unused \n",
      "information, such as additional zeroes, duplicate values, or \n",
      "frames, is part of MFCC feature extraction. The exact \n",
      "process removes the silent speech frames.  \n",
      "Fundamental frequen cy and SDC are two speech features \n",
      "to identify gender. Similarly, Spectral centroid, Chroma \n",
      "STFT, MFCC, and Spectral flux features are used for \n",
      "emotion identification to create a single set of data points \n",
      "from multiple speech features using multifeature fu sion \n",
      "technique [24]. \n",
      "Multifeature Fusion:  The extracted speech features hav e \n",
      "different dimensions, so feature proximity usages  average \n",
      "dimensional spacing between the vectors to denote the \n",
      "proximity between the diverse features  [24]. The average \n",
      "dimensional spacing between the vectors is computed as  \n",
      "          \n",
      "    √∑∑              (8) \n",
      "   =  \n",
      "     √∑∑              (9) \n",
      "\n",
      " \n",
      "Journal of Information Systems and Telecommunication, Vol.12, No.2,  April -June 2024  \n",
      " \n",
      " \n",
      "157 \n",
      "        and     are the mean and variance interval of \n",
      "average dimensions of the feature vector s.    and    are \n",
      "the mean value of   and   type of sound features.     and    \n",
      "are the mean value of   and   type of sound features . Then \n",
      "the subsequent dimensionality reduction [25] method \n",
      "follows once the feature selection process is complete. \n",
      "High data variance is present in the extracted features \n",
      "containing more information. Dimension reduction \n",
      "techniques reduce the dimensions from extracted feature \n",
      "vectors.  \n",
      " \n",
      "5-5-1- Principal Component Analysis  (PCA)  \n",
      "PCA [25] is an approach for redu cing the dimensionality \n",
      "of extensive datasets, transforming them into more \n",
      "compact representations while retaining crucial \n",
      "information . These reductions in the number of variables \n",
      "help to get more accurate results. A reduced dataset makes \n",
      "it easier and fas ter to visualize and analyze the data. \n",
      "Following steps followed to explore PCA .  \n",
      " Standardization  creates a normalized dataset \n",
      "when there is a significant difference in the range \n",
      "of initial variables or a larger range of datasets \n",
      "dominates the smaller range. It can be done by  \n",
      "             \n",
      "                     (10) \n",
      " Covariance matrix computation  helps  to know  \n",
      "how the input  dataset  varies  from  the mean value \n",
      "to each data point.  \n",
      " Eigenvectors and eigenvalues  of the covariance \n",
      "matrix help to identify the principal component.  \n",
      "5-6- Modelling  \n",
      "This implementation uses two deep l earning models. \n",
      "Bidirectional Long Short -Term Memory (BiLSTM) \n",
      "classifies the genders in whispered speech, and Deep \n",
      "Convolutional Neural Network (DCNN) identifies \n",
      "emotions.  \n",
      "BiLSTM  [26] combines  two Recurrent Neural Networks \n",
      "(RNN) that are placed independently and can traverse \n",
      "backward and forward directions at each time step, as \n",
      "shown in Fig 7. There are two ways to deal with data in \n",
      "this model - The first involves processing data from the \n",
      "past to the future , and the second operates in the opposite \n",
      "direction, from future  to past , where two hidden layers of \n",
      "neurons help to preserve the information in both \n",
      "directions. This approach helps to improve the information \n",
      "available in the algorithm.  \n",
      "For B iLSTM, the sequence of inputs is \n",
      "                   for a traditional RNN, which \n",
      "computes the hidden vector sequences \n",
      "                  and results in the output \n",
      "sequences                    for the iteration \n",
      "    to                            (11) \n",
      "                (12) \n",
      "  are the weight matrices;   is the bias vector, and   is \n",
      "the hidden layer function.  \n",
      "DCNN  [27] is a Convolutional Neural Network (CNN) \n",
      "type that helps to identify emotions from whispered \n",
      "speech . The input to the model is the speech data that \n",
      "traverses through many stages, such as the convolution \n",
      "layer, the pooling layer, Activation, Fully Connected, \n",
      "Batch normalization, and dropout layers, as shown in Fig \n",
      "8.  \n",
      " \n",
      " \n",
      " \n",
      "Fig 7. Structure of BiLSTM  \n",
      " \n",
      "To avoid overfitting, Leaky ReLU is used as an activation \n",
      "function in this implementation and evaluated as  \n",
      "{                      \n",
      "                        (13) \n",
      "The fully connected layer is a loss layer, measuring the \n",
      "inconsistency between the desired and actual outputs . Root \n",
      "The Mean Squared Propagation (RMSProp) optimization \n",
      "algorithm enhances the loss function, which varies in \n",
      "vertical directions.   \n",
      " \n",
      "Fig. 8 Structure of Deep Convolution Neural Network  \n",
      "5-7- Emotions  \n",
      "Deep learning techniques are applied to identify the \n",
      "emotions of human speech. Discrete Emotion [3] \n",
      "theory uses seven vital emotions in Human activities.  \n",
      " Sadness:  This is the feeling of dissatisfaction, \n",
      "sorrow, or fruitlessness.  \n",
      " Happiness:  This is the emotional state that elicits \n",
      "satisfaction and pleasure.  \n",
      " Fear:  This feeling triggers a fright response.  \n",
      " Anger:  This emotional state leads to feelings of \n",
      "hostility and frustration.  \n",
      " Surprise:  It is the state of mind that expresses \n",
      "either positive or negative following something \n",
      "unexpected.  \n",
      "\n",
      " \n",
      "Mohanty, & Cherukuri, Whispered Speech Emotion  Recognition with Gender Detection using BiLSTM and DCNN  \n",
      " \n",
      " \n",
      " \n",
      "158 \n",
      " Disgust:  A strong emotion that results in feeling \n",
      "repulsed.  \n",
      " Neutral:  This is the feeling of lack of particular \n",
      "preference.  \n",
      " \n",
      "5-8- Algorithm for Emotion Recognition  \n",
      "The algorithm to detect emotions from the whispered \n",
      "speech is in two parts  \n",
      " Gender Detection  \n",
      " Emotion Identification   \n",
      " \n",
      "Algorithm 1:  Geneder Detection  \n",
      "INPUT: Text  File (Whispered Speech Samples)  \n",
      "OUTPUT: Emotions (Sad, Happy, Fear, Anger, Surprise, \n",
      "Disgust, Neutral)  \n",
      " \n",
      "1: Begin:  \n",
      "2: Read speech  samples from Corpus   \n",
      "3: Pre-processing  the speech  samples:  \n",
      "4: Processed_speech=Pre_processing (Speech_Sample)   \n",
      "5: Extracted  SDC,  Pitch  features:  \n",
      "6: SDC, Pitch=Feature_Extraction(Processed_speech)   \n",
      "7: Multifeature  fusion:  \n",
      "8: multi _feature = multifeatured _fusion(SDC,  Pitch)   \n",
      "9: Dimension  reduction:  \n",
      "10: Dimension _R = Dimension _Reduction(multi _feature)  \n",
      "11: Determine  gender:  \n",
      "12: Gender = BiLSTM  (Dimension _R) \n",
      "13: Placed  in folders:  \n",
      "14: Gender=  (Male,  Female)   \n",
      "15: End:  \n",
      "Algorithm 2 Emotion Identification  \n",
      " \n",
      "1:  Begin:   \n",
      "2:  Speech Samples = (Male,  Female)  \n",
      "3:  Data Augmentation:  \n",
      "4:  Augemented _samples = Data Augmentation  (Speech          \n",
      "Samples)  \n",
      "5:  Pre-processed  the speech  samples:   \n",
      "6:  Processed _speech=Preprocessing  (Augemented _sam \n",
      "ples)  \n",
      "7:  Extracted  Spectral  Centroid,  ChromaSTFT,  MFCC,  \n",
      "Mel spectrogram  features:   \n",
      "8:  Spectral _Centroid,  Chroma _STFT,  MFCC,  \n",
      "Mel_spectrogram = Feature Extraction  (Processed _spe \n",
      "ech)  \n",
      "9:  Multifeature  fusion:   \n",
      "10:  multi _feature = \n",
      "multifeatured _fusion(Spectral _Centroid,  Chroma _STFT,  \n",
      "MFCC,  Mel_spectrogr am) \n",
      "11:  Dimension  reduction:  \n",
      "12:  Dimension_R = Dimension_Reduction(multi_feature)  \n",
      "13:  Emotion  detection:  14:  Emotions = DCNN(Dimension_R)   \n",
      "15:  OUTPUT Emotions  (Sad, Happy, Fear, Anger, \n",
      "Surprise, Disgust, Neutral)  \n",
      "16:  End:  \n",
      "6- Experiment and Result Analysis  \n",
      "The proposed SER has been implemented utilizing Python \n",
      "programming language and fortified by various machine \n",
      "learning libraries and additional supporting libraries . The \n",
      "experiment uses Python (Python 3.6.3rc1) and Librosa \n",
      "(Librosa 0.8.0) for audio processing. Graph plotting uses \n",
      "Seaborn and Matplotlib libraries, which help to analyze the \n",
      "speech data. BiLSTM and DCNN models implement the \n",
      "model using Keras (Keras - 2.6.0), TensorFlow \n",
      "(TensorFlow -2.6.0), and Scikit -learn libraries .  \n",
      "wTIMIT dataset is collected and divided into the train, \n",
      "cross -validation, and test samples. Subsequently, the \n",
      "speech processing takes place during implementation, \n",
      "incorporating data -augmentation techniques like shifting \n",
      "time, changing pitch, and changing speed . The pre -\n",
      "processing of speech content from each sample helps to \n",
      "analyze it and extract its features. The dimensionally \n",
      "reduced extracted features align with the data points in the \n",
      "BiLSTM model. The training dataset makes the model \n",
      "learn the features or data. Epochs continuously learn \n",
      "hidden features when the dataset completes backward and \n",
      "forward iterations. Then the cross -validation dataset is \n",
      "used to estimate the model’s performance on each epoch \n",
      "and prevent the model from being overfit ted. The test \n",
      "dataset helps to evaluate the trained model unbiasedly \n",
      "using performance metrics like precision, recall, f1 score, \n",
      "and confusion Matrix .  \n",
      "The parameters used in BiLSTM models are as in Table 1.  \n",
      "Table 1: Parameters of the BiLSTM model  \n",
      "Layers  Shape of output  Parameters#  \n",
      "Embedding  \n",
      "(Embedding)  (None, 216, 256)  \n",
      " 524544  \n",
      " \n",
      "Bidirectional  \n",
      " (None, 128)  \n",
      " 98816  \n",
      "batch normalization  (None, 128)  \n",
      " 512 \n",
      "Dense (Dense)  (None, 128)  \n",
      " 10512  \n",
      "dropout (Dropout)  (None, 128)  \n",
      " 0 \n",
      "flatten (Flatten)  (None, 128)  \n",
      " 1024  \n",
      "dense1(Dense)  \n",
      " (None, 12)  \n",
      " 0 \n",
      "dense2(Dense)  \n",
      " (None, 12)  \n",
      " 455 \n",
      " \n",
      " \n",
      "Journal of Information Systems and Telecommunication, Vol.12, No.2,  April -June 2024  \n",
      " \n",
      " \n",
      "159 \n",
      "The BiLSTM model helps to segregate the speech samples \n",
      "into male and female. The segregated speech samples \n",
      "move to their respective male and female folders.  \n",
      "Table 2: Parameters of the DCNN model  \n",
      "Layers  Shape of output  Parameters#  \n",
      "conv1d (Conv1D)  \n",
      " (None, 216, 256)  \n",
      " 2304  \n",
      "Activation  \n",
      "(Activation)  (None, 216, 256)  \n",
      " 0 \n",
      "conv1d 1 (Conv1D)  \n",
      " (None, 216, 256)  524544  \n",
      "batch normalization  \n",
      " (None, 216, 256)  1024 \n",
      "activation 1  \n",
      "(Activation)  \n",
      " (None, 216, 256) 0 \n",
      "dropout (Dropout)  (None, 216, 256)  \n",
      " 0 \n",
      "max pooling1d \n",
      "(MaxPooling1D)  \n",
      " (None, 13, 256)  \n",
      " 0 \n",
      "conv1d 2 (Conv1D)  \n",
      " (None, 13, 128)  \n",
      " 262272  \n",
      " \n",
      "activation 2 \n",
      "(Activation)  \n",
      " (None, 13, 128)  \n",
      " 0 \n",
      "conv1d 3 (Conv1D)  \n",
      " (None, 13, 128)  \n",
      " 131200  \n",
      " \n",
      "activation 3 \n",
      "(Activation)  \n",
      " (None, 13, 128)  \n",
      " 0 \n",
      "conv1d 4 (Conv1D)  \n",
      " (None, 13, 128)  \n",
      " 131200  \n",
      " \n",
      "batch normalization  \n",
      " (None, 13, 128)  \n",
      " 512 \n",
      "activation 4 \n",
      "(Activation)  \n",
      " (None, 13, 128)  \n",
      " 0 \n",
      "conv1d 5 (Conv1D)  \n",
      " (None, 13, 128)  \n",
      " 131200  \n",
      " \n",
      "batch normalization \n",
      "1 \n",
      " (None, 13, 128)  \n",
      " 512 \n",
      "activation 5 \n",
      "(Activation)  \n",
      " (None, 13, 128)  \n",
      " 0 \n",
      "dropout 1 (Dropout)  \n",
      " (None, 13, 128)  \n",
      " 0 \n",
      "max pooling1d 1 \n",
      "(MaxPooling1D)  \n",
      " (None, 1, 128)  \n",
      " 0 \n",
      "conv1d 6 (Conv1D)  \n",
      " (None, 1, 64)  \n",
      " 65600  \n",
      "activation 6 \n",
      "(Activation)  \n",
      " (None, 1, 64)  \n",
      " 0 \n",
      "conv1d 7 (Conv1D)  \n",
      " (None, 1, 64)  \n",
      " 32832  \n",
      "activation 7 \n",
      "(Activation)  \n",
      " (None, 1, 64)  \n",
      " 0 \n",
      "flatten (Flatten)  \n",
      " (None, 64)  \n",
      " 0 dense (Dense)  \n",
      " (None, 14)  \n",
      " 910 \n",
      "activation 8 \n",
      "(Activation)  \n",
      " (None, 14)  \n",
      " 0 \n",
      " \n",
      "After separating the speech samples into genders, Chroma \n",
      "STFT, Spectral centroid, Mel-scale spectrogram, and \n",
      "Spectral Flux feature s extraction happen ed. Then the \n",
      "created data points are dimensionally reduced to fit into \n",
      "the DCNN deep learning model to identify emotions. \n",
      "Metrics like precision, recall, F1 score, and the confusion \n",
      "matrix aid in assessing the model's performance.  Table 2 \n",
      "mentions the parameters of the DCNN model.  \n",
      "6-1- Result Analysis  \n",
      "The fusion of Fundamental frequency and SDC \n",
      "detect s the gender of speech samples. The initial \n",
      "stage involves pre -processing the speech samples, \n",
      "followed by feature extraction, which includes SDC \n",
      "and fundamental frequencies. The combination of \n",
      "SDC and Fundamental speech features create a \n",
      "multifeature fusion resulting in a single set of data \n",
      "points. The next step is to reduce the dimensions of \n",
      "data p oints to use them as input to the BiLSTM \n",
      "Model. The BiLSTM model predicts the data in the \n",
      "dataset as male and female, placed in separate \n",
      "folders. Of these, 3342 speech samples are available, \n",
      "and 3294 are correctly classified. So, the accuracy of \n",
      "the model prediction is 99.59%. The precision, recall, \n",
      "and f1 -score values, along with the confusion matrix, \n",
      "are shown in Fig s 9 and 10. The predicted model \n",
      "identifies the female speech samples more accurately \n",
      "than the male.   \n",
      " \n",
      "Fig. 9 Confusion Matrix for Gender detection by BiLSTM.  \n",
      " \n",
      "The graph in Fig 9 shows the gender detection from the \n",
      "speech sample. The X -axis represents the true label, and \n",
      "the Y-axis shows the predicted label.  This graph gives the \n",
      "\n",
      " \n",
      "Mohanty, & Cherukuri, Whispered Speech Emotion  Recognition with Gender Detection using BiLSTM and DCNN  \n",
      " \n",
      " \n",
      " \n",
      "160 \n",
      "count of detected gender spe ech that the Bi -LSTM model \n",
      "correctly and incorrectly detects.  \n",
      " \n",
      "Fig. 10 Accuracy measures of Gender Detection.  \n",
      " \n",
      "2031 female and 1263 male speech are correctly detected, \n",
      "whereas 13 females and 35 males are incorrectly detected. \n",
      "Pitch values identify the emotions after detecting genders \n",
      "from the speech samples.  \n",
      "The extraction of MFCC, Mel -scale spectrogram, Chroma \n",
      "STFT, Spectral Flux, and Spectral Centroid speech \n",
      "features happens from speech samples. Then, all five \n",
      "speech features are fused into a single feature and \n",
      "dimensionally reduced. Finally, the DCNN model is \n",
      "applied to predict emotions.  \n",
      "Individual emotions are detected based on gender as \n",
      "shown in Fig 11. The deviation in the male speech \n",
      "emotions is less than the female . Female fear and female \n",
      "neutral  emotions show divergent results from other \n",
      "emotions. The accuracy of the model is 98.54%.   \n",
      " \n",
      " \n",
      "Fig.11 Emotions based on gender.  \n",
      "6-2- Comparison Analysis  \n",
      "This experiment evaluates the current implementation \n",
      "using the wTIMIT  dataset, resulting in an impressive \n",
      "accuracy of 98.54%. The performance compares with \n",
      "three -layered Long short -term memory Bidirectional \n",
      "RNNs (LSTM BiRNN) that use No -Attention, Feed \n",
      "Forward Attention (FFA), and Improved Feed Forward \n",
      "Attention Mechanism ( IFFA) to evaluate the emotions in \n",
      "the dataset. This model uses 35 hidden states to train the dataset. The models are trained and validated with the help \n",
      "of the wTIMIT dataset [28] and tested with the help of the \n",
      "CHAINS dataset,  as shown in Table 3. This ex periment is \n",
      "a new hypothesis for  emotion detection with gender \n",
      "identification on whispered speech.  Hence, there are fewer \n",
      "references available for similar investigations.  \n",
      "Table 3: Comparison of the Implementation  \n",
      "  Sl# Model Implementation  Accuracy  \n",
      "1 FFA, IFFA, LSTM, Bi_RNN [2 8] 97.6 %  \n",
      "2 Proposed Model  98.54 %  \n",
      "The proposed method improves the performance of \n",
      "emotions after segregating the speech samples into genders.  \n",
      "By separating the approach into two models, this \n",
      "implementation excels in capturing natural and \n",
      "spontaneous expressions of emotions with low latency \n",
      "manner .  \n",
      "7- Conclusion  and Future Work  \n",
      "Identification of gender from whispered speech and \n",
      "recognizing emotions is a complicated task. This \n",
      "implementation initially detects the gender from the \n",
      "speech samples before identifying the emotions. Emotions \n",
      "might vary based on gender in the same situation. Speech \n",
      "features such as Fu ndamental Frequency and SDC help \n",
      "with gender identification. MFCC, Mel -scale spectrogram, \n",
      "Spectral flux, Spectral Centroid, and Chroma STFT play a \n",
      "vital role in detecting emotions. Multifeature fusion helps \n",
      "to combine speech features into a single set of d ata points . \n",
      "The concept verifies with the publicly available dataset \n",
      "wTIMIT with an accuracy of 98.54%. This approach helps \n",
      "to identify nearly inaudible emotions and is used to figure \n",
      "out the strategy. The proposed methodology can be \n",
      "improved using other s peech features, machine learning, \n",
      "and deep learning concepts.  \n",
      " \n",
      "References  \n",
      "[1]  ST Jovicic , and Z  Saric , ―Acoustic analysis of  consonants in \n",
      "whispered speech,‖ Journal of voice , vol 22, no. 3, pp. 263–\n",
      "274, 2008 . \n",
      "[2]  M Kumari , and I  Ali, ―An efficient algorithm for gender \n",
      "detection using voice samples,‖ 2015 Communication, \n",
      "Control and Intelligent Systems (CCIS),  2015 , Mathura, \n",
      "Utter Pradesh,  pp. 221–226, doi: \n",
      "10.1109/CCIntelS.20 15.7437912 . \n",
      "[3] S Motamed , S Setayeshi, A Rabiee , and A Sharifi, ―Speech \n",
      "Emotion Recognition Based on Fusion Method,‖ Journal of \n",
      "Information Systems and Telecommunication (JIST), vol. 3, \n",
      "pp. 50--56, 2017, doi: 10.7508/jist.2017.17.007 .  \n",
      "[4] JS Li, CC Huang , ST Sheu , and MW  Lin, ―Speech emotion \n",
      "recognition and its applications,‖ Proc. of Taiwan Institute of \n",
      "Kansei Conference , 2010  Paris , France , pp. 187–192.  \n",
      "\n",
      " \n",
      "Journal of Information Systems and Telecommunication, Vol.12, No.2,  April -June 2024  \n",
      " \n",
      " \n",
      "161 \n",
      "[5] A Guerrieri , E Braccili , F Sgro , and GN Meldolesi  ―Gender \n",
      "identification in a two -level hierarchical speech emotion \n",
      "recognition system for an Italian Social Robot,‖ Sensors, vol. \n",
      "22, no. 5, pp. 1714, 2022 , doi: 10.3390/s22051714 . \n",
      "[6] M Sarria -Paja, TH Falk, and D  O’Shaughnessy , ―Whispered \n",
      "speaker verification and gender detection using weighted \n",
      "instantaneous frequencies,‖ 2013 IEEE International \n",
      "Conference on Acoustics, Speech and Signal Processing , \n",
      "2013  (May 26 -31), Vancouver , Canada, pp. 7209 –7213 , doi: \n",
      "10.1109/ICASSP.2013.6639062 .  \n",
      "[7] J Deng , S Fruhholz, Z Zhang , and Bojrn Schuller , \n",
      "―Recognizing emotions from whispered speech based on \n",
      "acoustic feature transfer learning,‖ IEEE Access , vol.  5, pp. \n",
      "5235 –5246 , 2017 .  \n",
      "[8] M Cotescu , T Drugman, G Huybrechts, J Lorenzo -Trueba, \n",
      "and A Moinet , ―Voice conversion for whispered speech \n",
      "synthesis,‖  IEEE Signal Processing Letters , vol. 27, pp. 186–\n",
      "190, 2019 , doi: 10.1109/LSP.2019.2961213 .  \n",
      "[9] P Mishra , and R Sharma, ―Gender differentiated \n",
      "convolutional neural networks for speech emotion \n",
      "recognition,‖ 2020 12th International Congress on Ultra -\n",
      "Modern Telecommuni - cations and Control Systems and \n",
      "Workshops (ICUMT),  2020  (October 5 -7), Brno , Czech \n",
      "Republic,  pp. 142–148, doi: \n",
      "10.1109/ICUMT51630.2020.9222412 . \n",
      "[10] Mustaqeem S Kwon, ―Optimal feature selection based \n",
      "speech emotion recognition using two -stream deep \n",
      "convolutional neural network,‖ International Journal of \n",
      "Intelligent Systems , vol. 36, no. 9, pp. 5116 – 5135 , 2021 , \n",
      "doi:  10.1002/int.22505 . \n",
      "[11] J. Ancilin and A. Milton, ―Improved speech emotion \n",
      "recognition with mel frequency magnitude coefficient ,‖ \n",
      "Applied Acoustics , vol. 179, pp. 108046 , 2021 , doi: \n",
      "10.1016/j.apacoust.2021.108046 .  \n",
      "[12] S. Jothimani and K. Premalatha, ―Mff -saug: Multi feature \n",
      "fusion with spectrogram augmentatio n of speech emotion \n",
      "recognition using convolution neural network ,‖ Chaos, \n",
      "Solitons & Fractals , vol. 162, pp. 112512 , 2022 , doi: \n",
      "10.1016/j.chaos.2022.112512 .  \n",
      "[13] B Yalamanchili, SK Samayamantula, and KR Anne, ―Neural \n",
      "network -based blended ensemble learning for speech \n",
      "emotion recognition,‖ Multidimensional Systems and Signal \n",
      "Processing, vol. 33, no. 4, pp. 1323 --1348 , 2022 , doi: \n",
      "10.1007/s11045 -022-00845 -9.  \n",
      "[14] T Feng , R Hebbar , and S Narayanan, ―Trustser: On the \n",
      "trustworthiness of fine -tuning pre -trained speech embeddings \n",
      "for speech emotion recognition,‖ arXiv preprint \n",
      "arXiv:2305.11229, 2023 , doi:  10.48550/arXiv.2305.11229  \n",
      "[15] RV Darekar and M Chavan , S Sharanyaa,  and NR Ranjan , \n",
      "―A hybrid meta -heuristic ensemble based classification \n",
      "technique speech emotion recognition,‖ Advances in \n",
      "Engineering Software, vol. 180, pp. 103412 , 2023 . \n",
      "[16] J Rekimot o, ―Dualvoice: A speech interaction method using \n",
      "whisper -voice as commands,‖ CHI Conference on Human \n",
      "Factors in Computing Systems Extended Abstracts,  pp. 1–6, \n",
      "2022 , doi: 10.1145/3491101.3519700 . \n",
      "[17]H Dolka,  AX VM , and S Juliet,―  Speech  emotion r ecognition  \n",
      "using  ANN on MFCC  features,‖ 2021 3rd international \n",
      "conference on signal processing and communication (ICPSC) , \n",
      "2021, Coimbatore, India, pp. 431–435, doi: \n",
      "10.1109/ICSPC51351.2021.9451810 . [18] MK Reddy and KS Rao, ―Robust pitch extraction method \n",
      "for the hmm -based speech synthesis system,‖ IEEE signal \n",
      "processing letters , vol.  24, no. 8, pp. 1133 –1137 , 2017 , doi: \n",
      "10.1109/LSP.2017. 2712646 .  \n",
      "[19] J Chatterjee, V Mukesh, HH Hsu , G Vyas , and Z Liu , \n",
      "―Speech emotion recognition using cross - correlation and \n",
      "acoustic features,‖ 2018 IEEE 16th Intl Conf on Dependable, \n",
      "Autonomic and Secure Computing, 16th Intl Conf on \n",
      "Pervasive Intelligence and Computing, 4th Intl Conf on Big \n",
      "Data Intelligence and Computing and Cyber Science and \n",
      "Technology Congress , 2018  (August 12-15), Athens, Greece , \n",
      "pp. 243–249, doi: 10.1109/DASC/PiCom/DataCom/CyberSci  \n",
      " Tec.2018.00050 .  \n",
      "[20] S Rajesh and NJ Nalini, ―Musical instrument emotion \n",
      "recognition using deep recurrent neural network,‖ Procedia \n",
      "Computer Science, vol. 167, pp. 16 --25, 2020, doi: \n",
      "10.1016/j.procs.2020.03.178 . \n",
      "[21] M Aly , and NS Alotaibi, ―A novel deep learning model to \n",
      "detect covid -19 based on wavelet features extracted from \n",
      "mel- scale spectrogram of patients’ cough and breathing \n",
      "sounds,‖ Informatics in Medicine Unlocked , vol.  32, pp.  \n",
      "101049 , 2022, doi: 10.1016/j.imu.2022.101049 . \n",
      "[22] Z Qawaqneh,  AA Mallouh,  and BD Barkana, ― Age and \n",
      "gender  classification  from  speech and face images by jointly \n",
      "fine-tuned deep neural networks,‖ Expert Systems with \n",
      "Applications , vol. 85, pp. 76–86, 2017 , doi: \n",
      "10.1016/j.eswa.2017.05.037 .  \n",
      "[23] A Koduru, HB Valiveti and AK Budati, ―Feature extraction \n",
      "algorithms to improve the speech emotion recognition rate,‖ \n",
      "International Journal of Speech Technology , vol. 23, no. 1, \n",
      "pp. 45–55, 2020 , doi: 10.1007/s10772 -020-09672 -4.  \n",
      "[24] S Zhang , and C Li, ―Research on feature fusion speech \n",
      "emotion recognition technology for smart teaching,‖ Mobile \n",
      "Information Systems , vol. 2022 , 2022 , doi:  \n",
      "10.1155/2022/7785929 . \n",
      "[25] GB, Prasanna, SV Bhat, C Naik , and HN Champa, ―An \n",
      "Efficient Method for Handwritten Kannada Digit \n",
      "Recognition based on PCA and SVM Classifier,‖ Journal of \n",
      "Information Systems and Telecommunication (JIST), vol. 3, \n",
      "no. 35, pp. 169 2021, doi: 20.1001.1.23221437.2021.9.35.3.2 . \n",
      "[26] A Graves, N Jaitly, and A Mohamed, ―Hybrid speech \n",
      "recognition wit h deep bidirectional lstm,‖ 2013 IEEE \n",
      "workshop on automatic speech recognition and \n",
      "understanding , 2013, Olomouc, Czech Republic, pp.  273–\n",
      "278, doi: 10.1109/ASRU.2013.6707742 .  \n",
      "[27] N Aloysius and M Geetha, ―A review on deep convolutional \n",
      "neural networks,‖ 2017 international conference on \n",
      "communication and signal processing (ICCSP), 2017, pp. \n",
      "0588 –0592 , IEEE, doi: 10.1109/ICCSP.2017.8286426 .  \n",
      "[28] SBC Gutha, MAB Shaik, T Udayakumar , and AA \n",
      "Saunshikhar , ―Improved feed forward attention mechanism in \n",
      "bidirectional recurrent neural networks for robust sequence \n",
      "classification,‖ 2020 International Conference on Signal \n",
      "Processing and Communications (SPCOM) , 2020 , IISc, \n",
      "Bangalore.  IEEE , pp. 1—5, doi: \n",
      "0.9179606 10.1109/SPCOM50965.202  \n",
      "\n",
      "\n",
      "Reading: d:\\PythonCode\\document_portal\\notebook\\data\\sample.pdf\n",
      " \n",
      " \n",
      " \n",
      "Journal of Information Systems and Telecommunication  \n",
      "Vol.12, No.2,  April -June 2024, 152-161 \n",
      " \n",
      "http://jist.acecr.org  \n",
      "ISSN 2322 -1437  / EISSN:2345 -2773  \n",
      " \n",
      "Whispered Speech Emotion Recognition with Gender Detection using \n",
      "BiLSTM and DCNN  \n",
      "Aniruddha Mohanty1*, Ravindranath C Cherukuri1 \n",
      " \n",
      "1.Department of Computer  science and Engineering, Christ (Deemed to be University), Bangalore, India  \n",
      " \n",
      " \n",
      "Received: 22 Aug 2023 / Revised: 04 Apr 2024 / Accepted: 21 May 2024  \n",
      " \n",
      " \n",
      "Abstract   \n",
      "Emotions are human mental states at a particular instance in time concerning one’s circumstances, mood, and relationships \n",
      "with others . Identifying emotions from the whispered speech is complicated as the conversation might be confidential. The \n",
      "representation of the speech relies on the magnitude of its information. Whispered speech is intelligible, a low -intensi ty \n",
      "signal, and varies from normal speech. Emotion identification is quite tricky from whispered speech. Both prosodic and \n",
      "spectral speech features help to identify emotions. The emotion identification in a whispered speech happens using prosodic \n",
      "speech fea tures such as zero -crossing rate  (ZCR) , pitch, and spectral features that include spectral centroid, chroma STFT, \n",
      "Mel scale spectrogram, Mel -frequency cepstral coefficient (MFCC), Shifted Delta Cepstrum (SDC), and Spectral Flux. \n",
      "There are two parts to the proposed implementation. Bidirectional Long  Short -Term Memory (BiLSTM) helps to identify \n",
      "the gender from the speech sample in the first step with SDC and pitch. The Deep Convolutional Neural Network (DCNN) \n",
      "model helps to identify the emotions in the second  step. This implementation is evaluated using the  wTIMIT data corpus \n",
      "and gives 98.54% accuracy. Emotions have a dynamic effect on genders, so this implementation performs better than \n",
      "traditional approaches. This approach helps to design online learning management systems, different applications for \n",
      "mobile devices, checking cyber -criminal activities, emotion detection for older people, automatic speaker identification and \n",
      "authentication, forensics, and surveillance . \n",
      " \n",
      "Keywords:  Whispered Speech; Emotion Re cognition; Speech Features; Data Corpus; BiLSTM;  DCNN . \n",
      " \n",
      "1- Introduction  \n",
      "Emotions are the humans’ short -lived feeling s that affects \n",
      "thinking, actions, relationships, and social interactions. \n",
      "Emotions express humans’ physiological and emotional \n",
      "states with facial expressions and body language. \n",
      "Whispered speech is a form of speec h that expresses \n",
      "emotions. Whispered speech is a type of communication \n",
      "produced with breath without any noise and excitation of \n",
      "voice. The whispered speech structure changes \n",
      "significantly because of the lack of periodic excitation in \n",
      "the voice. This result s in missing speech and reduced \n",
      "transparency in communication.  \n",
      "The difference between normal and whispered speech is \n",
      "the absence of vocal tract vibrations due to the vocal \n",
      "tract’s physiological blocking. The strength of whispered \n",
      "speech is minute and with out voice compared to phoned \n",
      "speech. The spectral and prosodic features help to detect \n",
      "the whispered speech. Prosodic features of speech vary \n",
      "over time. Spectral features of whispered speech are highly accurate over unvoiced consonants, voiced consonants, \n",
      "and formants in vowels [1]. \n",
      "The impacts on Speech Emotion Recognition ( SER) are \n",
      "due to various acoustic conditions such as compressed, \n",
      "noisy, telephonic conversation s and imitator speeches. \n",
      "Other environmental conditions,  such as stress, rhythm, \n",
      "and intonation, can affect  SER.  Whispered speech is also \n",
      "affected by similar acoustic and environmental conditions. \n",
      "SER depends on acoustic characteristics and gender in \n",
      "some scenarios. Hence gender plays a vital role in SER.  \n",
      "Nowadays, several whispered speech  samples of male and \n",
      "female voices are available. The effectiveness of the SER \n",
      "is more when the implementation is in two parts. The \n",
      "identification of the gender [2] happens initially using \n",
      "Bidirectional Long Short -term Memory (BiLSTM)  with \n",
      "speech features . The next step is detecting emotions using \n",
      "various speech features [3] and Deep Convolutional \n",
      "Neural Network (DCNN).   \n",
      "The structure of the paper includes various sections. \n",
      "Section 2 describes Human Emotions and their \n",
      "Applications. Details of the Related Work are in Section 3. \n",
      "Section 4 describes the System Model, which is the black \n",
      "box view of the implementation. Section 5 is the Model \n",
      " \n",
      "Journal of Information Systems and Telecommunication, Vol.12, No.2,  April -June 2024  \n",
      " \n",
      " \n",
      "153 \n",
      "Design that describes the details of the speech features and \n",
      "the deep learning models used in this implementation. \n",
      "Section 6 gives the Experiment and Result Analysis. \n",
      "Section 7 brief ly discuss  the Conclusion and Future Work.   \n",
      "2- Human  Emotions  and their  Applications  \n",
      "Human emotions are the mental state caused by countless \n",
      "associated views , feeling s, behavioral replies, and the \n",
      "degree of pressure and annoyance. It is often associated \n",
      "with attitude , temperament, behavior , disposition, and \n",
      "creativi ty [4]. Emotion recognition helps to detect a \n",
      "humans ’ emotional mood , which lasts hours and days. \n",
      "Speech conveys emotions such as anger, disgust, fear, \n",
      "happiness, neutrality, sadness, and surprise. The machine \n",
      "automatically detects various emotions from speech with \n",
      "the help of different algorithms. Emotion Recognition \n",
      "helps to : \n",
      " \n",
      "•  Detect customers ’ intentions based on the teleconference. \n",
      "•  Detect cybercrime.  \n",
      "•  Students ’ attention and teachers ’ content adjustment.  \n",
      "•  Disability assistance  \n",
      "•  Customer satisfaction  \n",
      "•  Stress monitoring  \n",
      "•  Social media analysis  \n",
      "•  Suspicious activity  \n",
      "•  Human -machine interaction and so on.   \n",
      "3- Related Work  \n",
      "Over time, numerous studies have detected emotions from \n",
      "whispered speech . The vario us deep learning models \n",
      "detect emotions based on the speech features extracted \n",
      "from the collected whispered speech data corpus . The SER \n",
      "for normal and whispered speech is diverse because of \n",
      "vocal excitation. Emotions vary with many factors; gender \n",
      "is among  the most influential factors  [5]. Identifying \n",
      "gender in the first step improves emotion detection from \n",
      "whispered speech. So, the related work explored is on \n",
      "gender detection and emotion recognition from whispered \n",
      "speech.  \n",
      "MFCC obtained by the Hilbert enve lope approach and \n",
      "weighted instantaneous frequencies (WIFs) obtained by \n",
      "the coherent demodulation help to detect gender in \n",
      "whispered speech  samples [6]. There is an opportunity to \n",
      "explore gender detection in noisy speech conditions using \n",
      "these approaches.  \n",
      "Autoencoder -enabled features  in the transfer learning \n",
      "framework propose to practice  phonated data to identify \n",
      "emotions from Whispered speech [7]. The feature \n",
      "extraction is from the Geneva Whispered Emotion Corpus \n",
      "(GeWEC) and Berlin Emotional Speech Databa se (EMO -\n",
      "DB) data corpus. The acoustic features such as Mel-frequency cepstral coefficient (MFCC), root mean square \n",
      "(RMS), frame energy, zero -crossing -rate (ZCR), pitch \n",
      "frequency (F0), probability of voicing autocorrelation \n",
      "function are the inputs to evaluate the framework. \n",
      "Implementing deep learning concepts on spontaneous data \n",
      "gives more accuracy than the current framework .  \n",
      "Gender is detected to target an anonymous speaker. The \n",
      "Deep Neural Network (DNN) model is used to generalize \n",
      "gender by using the MFCC speech feature. This \n",
      "implementation is applied to the wTIMIT dataset to verify \n",
      "the gender and recognize the speaker  [8]. The DNN model \n",
      "cannot generalize gender; evaluation can happen with \n",
      "other datasets.   \n",
      "MFCC and CNN, with the fully  connected network, detect \n",
      "gender and emotions like anger, disgust, fear, happiness, \n",
      "sadness, surprise, and a neutral state  [9]. The concept \n",
      "verification happens on RAVDESS, CREMA -D, SAVEE, \n",
      "and TESS datasets, having an accuracy of 92.283%, which \n",
      "is better than the tr aditional model.  \n",
      "The final prediction of the implemented model is to learn \n",
      "the mutually spatial -spectral features happens by a two -\n",
      "stream deep convolutional neural network with an iterative \n",
      "neighborhood  component analysis (INCA) and the most \n",
      "discriminatory optimal features  [10]. The concept \n",
      "verification happens on EMO -DB, SAVEE, and \n",
      "RAVDESS emotional speech corpora which perform with \n",
      "95%, 82%, and 85% accuracy rates. Real -time applications \n",
      "with natural and huge data corpora can help to extend this \n",
      "concept to identify emotions.  \n",
      "Mel Frequency Magnitude Coefficient (MFMC) and three \n",
      "spectral features, namely MFCC, log frequency power \n",
      "coefficient, and linear prediction cepstral coefficient, are \n",
      "used with the help of Support Vector Machine (SVM) \n",
      "modeling  [11]. The performance evaluation uses this \n",
      "concept  on Berlin, RAVDESS, SAVEE, EMOVO, and \n",
      "eNTERFACE data corpus . Feature selection, feature \n",
      "fusion, and multiple classification schemes improve the \n",
      "performance of MFMC.  \n",
      "The proposed MFF -SAug research in  which noise removal \n",
      "improves emotion. White Noise Injection, Pitch Tuning \n",
      "techniques, MFCC, ZCR, RMS speech features, and \n",
      "Convolutional Neural Network (CNN) modeling [12] \n",
      "detect emotions. Emotion detection during the interaction \n",
      "between people can extend the MFF -SAug approach .  \n",
      "The proposed Neural Network -based Blended Ensemble \n",
      "Learning (NNBEL) model is composed of a 1 -dimensional \n",
      "Convolution Neural Network (1D -CNN), Long Short -\n",
      "Term Memory (LSTM), and CapsuleNets. LSTM receives \n",
      "input from the Log Mel -spect rogram speech features, \n",
      "while 1D -CNN and CapsuleNets receive input from the \n",
      "MFCC.  Each model’s output is fed to Multi -Layer \n",
      "Perceptron (MLP) and predicts the final emotion s [13]. \n",
      "This model shows 95.3% and 94% accuracy on \n",
      "RAVDESS and IEMOCAP datasets, respectively.  \n",
      " \n",
      "Mohanty, & Cherukuri, Whispered Speech Emotion  Recognition with Gender Detection using BiLSTM and DCNN  \n",
      " \n",
      " \n",
      " \n",
      "154 \n",
      "TrustSER [14] implemented a general framework to \n",
      "determine the SER system's trustworthiness using deep \n",
      "learning techniques. Trustworthiness evaluates privacy \n",
      "(gender in formation, speaker demographics), safety, \n",
      "fairness, sustainability, and emotions (sad, angry, happy, \n",
      "neutral). The architecture of the TrustSET framework uses \n",
      "CNN encoder and Transformer encoder models. \n",
      "Trustworthy profiles under the Federated Learning \n",
      "scenario might improve privacy, fairness, and safety.  \n",
      "A hybrid meta -heuristic ensemble -based classification [15] \n",
      "helps to detect speech emotions. Raw speech samples are \n",
      "filtered using the Butterworth filter; then, spectrogram \n",
      "speech features are extracted from each frame to create a \n",
      "hybrid feature vector. The ensemble -based classification is \n",
      "applied to hybrid feature vectors to classify em otions. The \n",
      "ensemble -based classification contains a Recurrent Neural \n",
      "Network (RNN), a Deep Belief Network (DBN), and an \n",
      "Artificial Neural Network (ANN).  \n",
      "The above -related work shows that it is crucial to identify \n",
      "gender to segregate emotions in a speech. Male and female \n",
      "emotions are different based on situations. Whispered \n",
      "speech is an essential concept of emotion recognition. \n",
      "There is an opportunity to experiment further based on \n",
      "other datasets of whispered speech. So, this is a motivation \n",
      "to work on emot ion recognition in whispered speech, as \n",
      "this is a less explored area of research .  \n",
      "4- System Model  \n",
      "The analysis of e motion recognition from the whispered \n",
      "speech segregates into two parts. One is gender detection, \n",
      "and the second is emotion detection as shown in Fig 1.  \n",
      " \n",
      " \n",
      "Fig. 1 Emotion recognition from whispered speech  \n",
      " \n",
      "As part of gender detection, pre -processing of the speech \n",
      "samples is performed, and then pitch and SDC are \n",
      "extracted as part of prosodic and spectral features, \n",
      "respectively. Both the features are fused to get a single \n",
      "feature set with the help of multifeature fusion. PCA helps \n",
      "to reduce the dimensions of extracted features and is used \n",
      "as input to Bidirectional Long short -term memory \n",
      "(BiLSTM) to classify genders , as shown in Fig 2.  \n",
      " \n",
      " \n",
      "Fig .2 Gender detection from whispered speech  \n",
      " \n",
      "Following  Gender detection, the augmentation of speech \n",
      "data helps to get more realistic data. Then the speech features such as Chromatogram, Zero -Crossing Rate, \n",
      "Spectral Central, MFCC, Spectral Flux , and Mel \n",
      "Spectrog rams are extracted. All the extracted features \n",
      "combine to get a single feature using multifeature fusion. \n",
      "Then the dimensions are reduced to get the optimal data \n",
      "points. The data points are inserted into the Deep \n",
      "Convolutional Neural Network (DCNN) to identify the \n",
      "emotions , as shown in Fig 3.  \n",
      "5- System  Design  \n",
      "The implementation of system design happens in two parts. \n",
      "The first part identifies the male and female gender from \n",
      "the whispered speech samples. Then the determination of \n",
      "different emotions like sadn ess, happiness, fear, anger, \n",
      "surprise, disgust, and neutral are detected in gender -\n",
      "segregated speech samples.  \n",
      " \n",
      "Fig. 3 Emotion Recognition from Male and Female whispered speech.  \n",
      "5-1- Whispered Speech Data Corpus  \n",
      "wTIMIT [16] is a whispered voice data corpus ha ving 450 \n",
      "phonetically balanced sentences with 29 speakers. There \n",
      "are 11,324 utterances with a normal voice , which  can be \n",
      "used for normal and whispered training. This data is \n",
      "available in two parts - train and test divisions. The \n",
      "samples contain an 8 kHz sampling rate with a high -\n",
      "quality  and low-pass filter.  \n",
      "5-2-    Data Augmentation  \n",
      "Data augmentation [17 ] is the method of getting more \n",
      "realistic data from the existing data, which helps to add \n",
      "more training data to the model, reduce overfitting, and \n",
      "increase the model’s generalization ability. Below are the \n",
      "steps to generate synthetic data.  \n",
      " Shifting Time : Shifting time is a simple concept \n",
      "where the audio shifts to the left or right with a \n",
      "haphazard second. If the audio shifts to the left \n",
      "\n",
      " \n",
      "Journal of Information Systems and Telecommunication, Vol.12, No.2,  April -June 2024  \n",
      " \n",
      " \n",
      "155 \n",
      "with z seconds, then the first z seconds are \n",
      "marked as silence. Similarly, if the audio shifts to \n",
      "the right with z second s, the last z seconds are \n",
      "marked as silence.  \n",
      " Changing Pitch: Pitch change is adjusting the \n",
      "pitch randomly without upsetting the speediness \n",
      "of the audio file.  \n",
      " Changing Speed: Speed audio changes by \n",
      "stretching the time series data by a fixed rate.  \n",
      "5-3- Pre-processing  \n",
      "After the data collection and data augmentation, pre -\n",
      "processing [3] is the initial  phase  in training SER. \n",
      "Framing, Windowing, Voice activity detection, \n",
      "Normalization, and Noise reduction are pre -\n",
      "processing steps.   \n",
      "Framing:  Speech signals are quasip eriodic and vary \n",
      "over time. Hence, information and related emotions \n",
      "also fluctuate over time. The segregation of the \n",
      "signals happens in a shorter period to make the \n",
      "speech signal invariant. Twenty milliseconds to thirty \n",
      "milliseconds helps to make the speec h signal \n",
      "invariant, and five milliseconds of overlap of the \n",
      "frames avoid data leakage between the frames .  \n",
      "Windowing:  Windowing on each frame helps to diminish \n",
      "data leakage during Fast Fourier Transformation (FFT) \n",
      "after framing. The Hamming window allows  this step \n",
      "where the window size is N for the frames     , used in \n",
      "equation (1).  \n",
      " \n",
      "                  (   \n",
      "   )       (1) \n",
      "            \n",
      " \n",
      "Voice activity detection (VAD):  An utterance has three \n",
      "portions of speech activities : voiced, unvoiced, and silent. \n",
      "Zero Crossing Rate (ZCR) speech feature helps to detect \n",
      "VAD. ZCR represents the frequency of signal transitions \n",
      "between positive and negative values within a specific \n",
      "frame . Due to high energy, the ZCR value is low for \n",
      "voiced speech and high for unvoiced s peech because of \n",
      "low energy.  \n",
      "Normalization:  Normalization helps to reduce  speaker \n",
      "and recording inconsistency without affecting the features’ \n",
      "strength and enhancing the features’ generalization \n",
      "capability. The Z-normalization method is used more and \n",
      "represented as  \n",
      "       \n",
      "      (2) \n",
      "Where   is the speech signal,   and   are the mean and \n",
      "standard deviation of the data, respectively.  \n",
      "Noise Reduction:  The environmental noise captured while \n",
      "recording a speech signal  affects the recognition rate . \n",
      "Minimum mean square error (MMSE) and log -spectral \n",
      "amplitude MMSE (LogMMSE) reduce the noise from the speech signals. Noise reduction helps to get more accuracy \n",
      "in SER evaluation.  \n",
      "5-4- Feature Extraction  \n",
      "Specific emotions are present in prosodic and spectral \n",
      "features of speech. The extraction of prosodic and spectral \n",
      "features is a vital characteristic of emotion recognition \n",
      "after pre -processing the speech signals.  \n",
      "5-4-1-Prosodic Features  \n",
      "Prosodic features deal with the audio qualities of a speech \n",
      "when connected speeches use sounds as input. The \n",
      "production of the speech deals with the amount of energy, \n",
      "frequency, period, loudness, pitch, and duration. Speech \n",
      "signal com munication depends on intonation, stress, and \n",
      "rhythm, which prosodic features can detect. The prosodic \n",
      "features used in the implementation are : \n",
      "Zero -Crossing Rate (ZCR):  ZCR [17] measures the \n",
      "frequency of signal transitions between positive and \n",
      "negative values in every audio frame  and defined as  \n",
      "    \n",
      "    ∑       [ ]         [      \n",
      "    (3) \n",
      "Where        is the sign function . \n",
      "    [     ]  {        \n",
      "       \n",
      "Fundamental Frequency (Pitch):  Fundamental \n",
      "Frequency (F0) [18] is the minimum freque ncy of the \n",
      "periodic waveform. F0 is the significant parameter to \n",
      "differentiate male speech from female speech. Pitch also \n",
      "determines the voiced and unvoiced portion of the speech \n",
      "signal. This analysis uses pitch parameters like pitch mean \n",
      "value and pitch r ange.  \n",
      "The pitch range determines the number of octaves a speech  \n",
      "sample  can cover, from the lowest to the highest. F0 value \n",
      "varies from 85Hz to 180 HZ for the voiced speech of adult \n",
      "males.  \n",
      " \n",
      " \n",
      "Fig. 4 Representation of Pitch data  \n",
      " \n",
      "Similarly, the F0 value for adult females goes from 165Hz \n",
      "to 255Hz, as shown in Fig 4.  \n",
      "\n",
      " \n",
      "Mohanty, & Cherukuri, Whispered Speech Emotion  Recognition with Gender Detection using BiLSTM and DCNN  \n",
      " \n",
      " \n",
      " \n",
      "156 \n",
      "5-4-2-Spectral features  \n",
      "The representation of spectral features happens by \n",
      "converting the time -domain speech signal into the \n",
      "frequency domain with the help of the Fast Fourier \n",
      "Transform (FFT) which  benefits  to represent the \n",
      "characteristics of the Human Vocal Tract. The  spectral \n",
      "features are  \n",
      "Spectral Centroid (SC):  The Spectral Centroid [19] is the \n",
      "most used speech feature where the positioning of the \n",
      "―center of mass‖ of the audio signal spectr um defines the \n",
      "weight of the spectrum. The center of mass measures the \n",
      "weighted average of the frequency component located in \n",
      "the audio signal, defined as  \n",
      " \n",
      "      ∑            \n",
      "   \n",
      "∑       \n",
      "       (4) \n",
      "Where      represents the magnitude of bin numb er   and \n",
      "      represents the central frequency of the bin.  \n",
      "Chroma STFT:  Chroma STFT [20] is obtained using FFT \n",
      "on speech samples and the resulting spectrums a re a \n",
      "chromatogram in a vertical axis. This feature captures the \n",
      "harmonic feature of the speech s ignals.  \n",
      "Mel-scale Spectrogram:  Mel-scale Spectrogram [ 21] is a \n",
      "spectrogram in which frequencies convert to the Mel scale, \n",
      "which helps to differentiate the range of frequencies. Mel -\n",
      "spectrogram helps to understand emotions in a better way \n",
      "as humans can perceive sound on a logarithmic scale.  \n",
      "Mel Frequency Cepstral Coefficient (MFCC ): MFCC \n",
      "[17] i s SER’s  standard feature extraction technique. The \n",
      "vocal cord s, tongue, and teeth filter the sound and make it \n",
      "unique for each speaker in the Human Speech production \n",
      "system. Mel scale represents MFCC, where the frequency \n",
      "bands are equally spaced and close to the Human Auditory \n",
      "System’s response, as shown in Fig 5.  \n",
      " \n",
      "Fig 5 MFCC speech feature extraction  \n",
      " \n",
      "Shifted Delta Cepstrum (SDC):  Time derivatives apply \n",
      "to the cepstral coefficients obtained from the MFCC and \n",
      "combined with the delta coefficient to get SDC, as shown \n",
      "in Fig 6.       and   are the four parameters used in \n",
      "SDC [ 22].  M denotes the cepstral coefficient for each \n",
      "frame.   indicates added frames.   are the frames that \n",
      "append delta features from the new feature vector .   \n",
      "represents the delta values difference. So, the coefficient \n",
      "vectors represent as   \n",
      "     [                  ]   (5) \n",
      "   are MFCC coefficients and   is the coefficient index. \n",
      "For a given time,   an intermediate calculation is done to \n",
      "obtain the   coefficients.  \n",
      "                                 (6) \n",
      "Finally, the SDC coefficients vectors of   dimensions are \n",
      "obtained as  \n",
      "       [          (                )] (7) \n",
      " \n",
      " \n",
      "Fig. 6 Extraction of SDC speech feature  \n",
      " \n",
      "This speech feature helps to identify genders for a long -\n",
      "range dynamic appearance in speech signals.  \n",
      "Spectral Flux:  Spectral flux [ 20] measures the change in \n",
      "speed of the signal’s power spectrum compared to the \n",
      "previous frame. This feature estimates the speech signal’s \n",
      "power spectrum about  the power spectrum of one frame \n",
      "with others.  \n",
      "5-5- Feature Selection and Dimension Reduction  \n",
      "Feature s election [23] is selecting features from a large set \n",
      "of extracted features to eliminate redundant and unused \n",
      "information and decrease processing time. For this work, \n",
      "the extracted pitch contains lots of information. The global \n",
      "features, like the range of p itch values and mean pitch \n",
      "values, are selected. Removing redundant and unused \n",
      "information, such as additional zeroes, duplicate values, or \n",
      "frames, is part of MFCC feature extraction. The exact \n",
      "process removes the silent speech frames.  \n",
      "Fundamental frequen cy and SDC are two speech features \n",
      "to identify gender. Similarly, Spectral centroid, Chroma \n",
      "STFT, MFCC, and Spectral flux features are used for \n",
      "emotion identification to create a single set of data points \n",
      "from multiple speech features using multifeature fu sion \n",
      "technique [24]. \n",
      "Multifeature Fusion:  The extracted speech features hav e \n",
      "different dimensions, so feature proximity usages  average \n",
      "dimensional spacing between the vectors to denote the \n",
      "proximity between the diverse features  [24]. The average \n",
      "dimensional spacing between the vectors is computed as  \n",
      "          \n",
      "    √∑∑              (8) \n",
      "   =  \n",
      "     √∑∑              (9) \n",
      "\n",
      " \n",
      "Journal of Information Systems and Telecommunication, Vol.12, No.2,  April -June 2024  \n",
      " \n",
      " \n",
      "157 \n",
      "        and     are the mean and variance interval of \n",
      "average dimensions of the feature vector s.    and    are \n",
      "the mean value of   and   type of sound features.     and    \n",
      "are the mean value of   and   type of sound features . Then \n",
      "the subsequent dimensionality reduction [25] method \n",
      "follows once the feature selection process is complete. \n",
      "High data variance is present in the extracted features \n",
      "containing more information. Dimension reduction \n",
      "techniques reduce the dimensions from extracted feature \n",
      "vectors.  \n",
      " \n",
      "5-5-1- Principal Component Analysis  (PCA)  \n",
      "PCA [25] is an approach for redu cing the dimensionality \n",
      "of extensive datasets, transforming them into more \n",
      "compact representations while retaining crucial \n",
      "information . These reductions in the number of variables \n",
      "help to get more accurate results. A reduced dataset makes \n",
      "it easier and fas ter to visualize and analyze the data. \n",
      "Following steps followed to explore PCA .  \n",
      " Standardization  creates a normalized dataset \n",
      "when there is a significant difference in the range \n",
      "of initial variables or a larger range of datasets \n",
      "dominates the smaller range. It can be done by  \n",
      "             \n",
      "                     (10) \n",
      " Covariance matrix computation  helps  to know  \n",
      "how the input  dataset  varies  from  the mean value \n",
      "to each data point.  \n",
      " Eigenvectors and eigenvalues  of the covariance \n",
      "matrix help to identify the principal component.  \n",
      "5-6- Modelling  \n",
      "This implementation uses two deep l earning models. \n",
      "Bidirectional Long Short -Term Memory (BiLSTM) \n",
      "classifies the genders in whispered speech, and Deep \n",
      "Convolutional Neural Network (DCNN) identifies \n",
      "emotions.  \n",
      "BiLSTM  [26] combines  two Recurrent Neural Networks \n",
      "(RNN) that are placed independently and can traverse \n",
      "backward and forward directions at each time step, as \n",
      "shown in Fig 7. There are two ways to deal with data in \n",
      "this model - The first involves processing data from the \n",
      "past to the future , and the second operates in the opposite \n",
      "direction, from future  to past , where two hidden layers of \n",
      "neurons help to preserve the information in both \n",
      "directions. This approach helps to improve the information \n",
      "available in the algorithm.  \n",
      "For B iLSTM, the sequence of inputs is \n",
      "                   for a traditional RNN, which \n",
      "computes the hidden vector sequences \n",
      "                  and results in the output \n",
      "sequences                    for the iteration \n",
      "    to                            (11) \n",
      "                (12) \n",
      "  are the weight matrices;   is the bias vector, and   is \n",
      "the hidden layer function.  \n",
      "DCNN  [27] is a Convolutional Neural Network (CNN) \n",
      "type that helps to identify emotions from whispered \n",
      "speech . The input to the model is the speech data that \n",
      "traverses through many stages, such as the convolution \n",
      "layer, the pooling layer, Activation, Fully Connected, \n",
      "Batch normalization, and dropout layers, as shown in Fig \n",
      "8.  \n",
      " \n",
      " \n",
      " \n",
      "Fig 7. Structure of BiLSTM  \n",
      " \n",
      "To avoid overfitting, Leaky ReLU is used as an activation \n",
      "function in this implementation and evaluated as  \n",
      "{                      \n",
      "                        (13) \n",
      "The fully connected layer is a loss layer, measuring the \n",
      "inconsistency between the desired and actual outputs . Root \n",
      "The Mean Squared Propagation (RMSProp) optimization \n",
      "algorithm enhances the loss function, which varies in \n",
      "vertical directions.   \n",
      " \n",
      "Fig. 8 Structure of Deep Convolution Neural Network  \n",
      "5-7- Emotions  \n",
      "Deep learning techniques are applied to identify the \n",
      "emotions of human speech. Discrete Emotion [3] \n",
      "theory uses seven vital emotions in Human activities.  \n",
      " Sadness:  This is the feeling of dissatisfaction, \n",
      "sorrow, or fruitlessness.  \n",
      " Happiness:  This is the emotional state that elicits \n",
      "satisfaction and pleasure.  \n",
      " Fear:  This feeling triggers a fright response.  \n",
      " Anger:  This emotional state leads to feelings of \n",
      "hostility and frustration.  \n",
      " Surprise:  It is the state of mind that expresses \n",
      "either positive or negative following something \n",
      "unexpected.  \n",
      "\n",
      " \n",
      "Mohanty, & Cherukuri, Whispered Speech Emotion  Recognition with Gender Detection using BiLSTM and DCNN  \n",
      " \n",
      " \n",
      " \n",
      "158 \n",
      " Disgust:  A strong emotion that results in feeling \n",
      "repulsed.  \n",
      " Neutral:  This is the feeling of lack of particular \n",
      "preference.  \n",
      " \n",
      "5-8- Algorithm for Emotion Recognition  \n",
      "The algorithm to detect emotions from the whispered \n",
      "speech is in two parts  \n",
      " Gender Detection  \n",
      " Emotion Identification   \n",
      " \n",
      "Algorithm 1:  Geneder Detection  \n",
      "INPUT: Text  File (Whispered Speech Samples)  \n",
      "OUTPUT: Emotions (Sad, Happy, Fear, Anger, Surprise, \n",
      "Disgust, Neutral)  \n",
      " \n",
      "1: Begin:  \n",
      "2: Read speech  samples from Corpus   \n",
      "3: Pre-processing  the speech  samples:  \n",
      "4: Processed_speech=Pre_processing (Speech_Sample)   \n",
      "5: Extracted  SDC,  Pitch  features:  \n",
      "6: SDC, Pitch=Feature_Extraction(Processed_speech)   \n",
      "7: Multifeature  fusion:  \n",
      "8: multi _feature = multifeatured _fusion(SDC,  Pitch)   \n",
      "9: Dimension  reduction:  \n",
      "10: Dimension _R = Dimension _Reduction(multi _feature)  \n",
      "11: Determine  gender:  \n",
      "12: Gender = BiLSTM  (Dimension _R) \n",
      "13: Placed  in folders:  \n",
      "14: Gender=  (Male,  Female)   \n",
      "15: End:  \n",
      "Algorithm 2 Emotion Identification  \n",
      " \n",
      "1:  Begin:   \n",
      "2:  Speech Samples = (Male,  Female)  \n",
      "3:  Data Augmentation:  \n",
      "4:  Augemented _samples = Data Augmentation  (Speech          \n",
      "Samples)  \n",
      "5:  Pre-processed  the speech  samples:   \n",
      "6:  Processed _speech=Preprocessing  (Augemented _sam \n",
      "ples)  \n",
      "7:  Extracted  Spectral  Centroid,  ChromaSTFT,  MFCC,  \n",
      "Mel spectrogram  features:   \n",
      "8:  Spectral _Centroid,  Chroma _STFT,  MFCC,  \n",
      "Mel_spectrogram = Feature Extraction  (Processed _spe \n",
      "ech)  \n",
      "9:  Multifeature  fusion:   \n",
      "10:  multi _feature = \n",
      "multifeatured _fusion(Spectral _Centroid,  Chroma _STFT,  \n",
      "MFCC,  Mel_spectrogr am) \n",
      "11:  Dimension  reduction:  \n",
      "12:  Dimension_R = Dimension_Reduction(multi_feature)  \n",
      "13:  Emotion  detection:  14:  Emotions = DCNN(Dimension_R)   \n",
      "15:  OUTPUT Emotions  (Sad, Happy, Fear, Anger, \n",
      "Surprise, Disgust, Neutral)  \n",
      "16:  End:  \n",
      "6- Experiment and Result Analysis  \n",
      "The proposed SER has been implemented utilizing Python \n",
      "programming language and fortified by various machine \n",
      "learning libraries and additional supporting libraries . The \n",
      "experiment uses Python (Python 3.6.3rc1) and Librosa \n",
      "(Librosa 0.8.0) for audio processing. Graph plotting uses \n",
      "Seaborn and Matplotlib libraries, which help to analyze the \n",
      "speech data. BiLSTM and DCNN models implement the \n",
      "model using Keras (Keras - 2.6.0), TensorFlow \n",
      "(TensorFlow -2.6.0), and Scikit -learn libraries .  \n",
      "wTIMIT dataset is collected and divided into the train, \n",
      "cross -validation, and test samples. Subsequently, the \n",
      "speech processing takes place during implementation, \n",
      "incorporating data -augmentation techniques like shifting \n",
      "time, changing pitch, and changing speed . The pre -\n",
      "processing of speech content from each sample helps to \n",
      "analyze it and extract its features. The dimensionally \n",
      "reduced extracted features align with the data points in the \n",
      "BiLSTM model. The training dataset makes the model \n",
      "learn the features or data. Epochs continuously learn \n",
      "hidden features when the dataset completes backward and \n",
      "forward iterations. Then the cross -validation dataset is \n",
      "used to estimate the model’s performance on each epoch \n",
      "and prevent the model from being overfit ted. The test \n",
      "dataset helps to evaluate the trained model unbiasedly \n",
      "using performance metrics like precision, recall, f1 score, \n",
      "and confusion Matrix .  \n",
      "The parameters used in BiLSTM models are as in Table 1.  \n",
      "Table 1: Parameters of the BiLSTM model  \n",
      "Layers  Shape of output  Parameters#  \n",
      "Embedding  \n",
      "(Embedding)  (None, 216, 256)  \n",
      " 524544  \n",
      " \n",
      "Bidirectional  \n",
      " (None, 128)  \n",
      " 98816  \n",
      "batch normalization  (None, 128)  \n",
      " 512 \n",
      "Dense (Dense)  (None, 128)  \n",
      " 10512  \n",
      "dropout (Dropout)  (None, 128)  \n",
      " 0 \n",
      "flatten (Flatten)  (None, 128)  \n",
      " 1024  \n",
      "dense1(Dense)  \n",
      " (None, 12)  \n",
      " 0 \n",
      "dense2(Dense)  \n",
      " (None, 12)  \n",
      " 455 \n",
      " \n",
      " \n",
      "Journal of Information Systems and Telecommunication, Vol.12, No.2,  April -June 2024  \n",
      " \n",
      " \n",
      "159 \n",
      "The BiLSTM model helps to segregate the speech samples \n",
      "into male and female. The segregated speech samples \n",
      "move to their respective male and female folders.  \n",
      "Table 2: Parameters of the DCNN model  \n",
      "Layers  Shape of output  Parameters#  \n",
      "conv1d (Conv1D)  \n",
      " (None, 216, 256)  \n",
      " 2304  \n",
      "Activation  \n",
      "(Activation)  (None, 216, 256)  \n",
      " 0 \n",
      "conv1d 1 (Conv1D)  \n",
      " (None, 216, 256)  524544  \n",
      "batch normalization  \n",
      " (None, 216, 256)  1024 \n",
      "activation 1  \n",
      "(Activation)  \n",
      " (None, 216, 256) 0 \n",
      "dropout (Dropout)  (None, 216, 256)  \n",
      " 0 \n",
      "max pooling1d \n",
      "(MaxPooling1D)  \n",
      " (None, 13, 256)  \n",
      " 0 \n",
      "conv1d 2 (Conv1D)  \n",
      " (None, 13, 128)  \n",
      " 262272  \n",
      " \n",
      "activation 2 \n",
      "(Activation)  \n",
      " (None, 13, 128)  \n",
      " 0 \n",
      "conv1d 3 (Conv1D)  \n",
      " (None, 13, 128)  \n",
      " 131200  \n",
      " \n",
      "activation 3 \n",
      "(Activation)  \n",
      " (None, 13, 128)  \n",
      " 0 \n",
      "conv1d 4 (Conv1D)  \n",
      " (None, 13, 128)  \n",
      " 131200  \n",
      " \n",
      "batch normalization  \n",
      " (None, 13, 128)  \n",
      " 512 \n",
      "activation 4 \n",
      "(Activation)  \n",
      " (None, 13, 128)  \n",
      " 0 \n",
      "conv1d 5 (Conv1D)  \n",
      " (None, 13, 128)  \n",
      " 131200  \n",
      " \n",
      "batch normalization \n",
      "1 \n",
      " (None, 13, 128)  \n",
      " 512 \n",
      "activation 5 \n",
      "(Activation)  \n",
      " (None, 13, 128)  \n",
      " 0 \n",
      "dropout 1 (Dropout)  \n",
      " (None, 13, 128)  \n",
      " 0 \n",
      "max pooling1d 1 \n",
      "(MaxPooling1D)  \n",
      " (None, 1, 128)  \n",
      " 0 \n",
      "conv1d 6 (Conv1D)  \n",
      " (None, 1, 64)  \n",
      " 65600  \n",
      "activation 6 \n",
      "(Activation)  \n",
      " (None, 1, 64)  \n",
      " 0 \n",
      "conv1d 7 (Conv1D)  \n",
      " (None, 1, 64)  \n",
      " 32832  \n",
      "activation 7 \n",
      "(Activation)  \n",
      " (None, 1, 64)  \n",
      " 0 \n",
      "flatten (Flatten)  \n",
      " (None, 64)  \n",
      " 0 dense (Dense)  \n",
      " (None, 14)  \n",
      " 910 \n",
      "activation 8 \n",
      "(Activation)  \n",
      " (None, 14)  \n",
      " 0 \n",
      " \n",
      "After separating the speech samples into genders, Chroma \n",
      "STFT, Spectral centroid, Mel-scale spectrogram, and \n",
      "Spectral Flux feature s extraction happen ed. Then the \n",
      "created data points are dimensionally reduced to fit into \n",
      "the DCNN deep learning model to identify emotions. \n",
      "Metrics like precision, recall, F1 score, and the confusion \n",
      "matrix aid in assessing the model's performance.  Table 2 \n",
      "mentions the parameters of the DCNN model.  \n",
      "6-1- Result Analysis  \n",
      "The fusion of Fundamental frequency and SDC \n",
      "detect s the gender of speech samples. The initial \n",
      "stage involves pre -processing the speech samples, \n",
      "followed by feature extraction, which includes SDC \n",
      "and fundamental frequencies. The combination of \n",
      "SDC and Fundamental speech features create a \n",
      "multifeature fusion resulting in a single set of data \n",
      "points. The next step is to reduce the dimensions of \n",
      "data p oints to use them as input to the BiLSTM \n",
      "Model. The BiLSTM model predicts the data in the \n",
      "dataset as male and female, placed in separate \n",
      "folders. Of these, 3342 speech samples are available, \n",
      "and 3294 are correctly classified. So, the accuracy of \n",
      "the model prediction is 99.59%. The precision, recall, \n",
      "and f1 -score values, along with the confusion matrix, \n",
      "are shown in Fig s 9 and 10. The predicted model \n",
      "identifies the female speech samples more accurately \n",
      "than the male.   \n",
      " \n",
      "Fig. 9 Confusion Matrix for Gender detection by BiLSTM.  \n",
      " \n",
      "The graph in Fig 9 shows the gender detection from the \n",
      "speech sample. The X -axis represents the true label, and \n",
      "the Y-axis shows the predicted label.  This graph gives the \n",
      "\n",
      " \n",
      "Mohanty, & Cherukuri, Whispered Speech Emotion  Recognition with Gender Detection using BiLSTM and DCNN  \n",
      " \n",
      " \n",
      " \n",
      "160 \n",
      "count of detected gender spe ech that the Bi -LSTM model \n",
      "correctly and incorrectly detects.  \n",
      " \n",
      "Fig. 10 Accuracy measures of Gender Detection.  \n",
      " \n",
      "2031 female and 1263 male speech are correctly detected, \n",
      "whereas 13 females and 35 males are incorrectly detected. \n",
      "Pitch values identify the emotions after detecting genders \n",
      "from the speech samples.  \n",
      "The extraction of MFCC, Mel -scale spectrogram, Chroma \n",
      "STFT, Spectral Flux, and Spectral Centroid speech \n",
      "features happens from speech samples. Then, all five \n",
      "speech features are fused into a single feature and \n",
      "dimensionally reduced. Finally, the DCNN model is \n",
      "applied to predict emotions.  \n",
      "Individual emotions are detected based on gender as \n",
      "shown in Fig 11. The deviation in the male speech \n",
      "emotions is less than the female . Female fear and female \n",
      "neutral  emotions show divergent results from other \n",
      "emotions. The accuracy of the model is 98.54%.   \n",
      " \n",
      " \n",
      "Fig.11 Emotions based on gender.  \n",
      "6-2- Comparison Analysis  \n",
      "This experiment evaluates the current implementation \n",
      "using the wTIMIT  dataset, resulting in an impressive \n",
      "accuracy of 98.54%. The performance compares with \n",
      "three -layered Long short -term memory Bidirectional \n",
      "RNNs (LSTM BiRNN) that use No -Attention, Feed \n",
      "Forward Attention (FFA), and Improved Feed Forward \n",
      "Attention Mechanism ( IFFA) to evaluate the emotions in \n",
      "the dataset. This model uses 35 hidden states to train the dataset. The models are trained and validated with the help \n",
      "of the wTIMIT dataset [28] and tested with the help of the \n",
      "CHAINS dataset,  as shown in Table 3. This ex periment is \n",
      "a new hypothesis for  emotion detection with gender \n",
      "identification on whispered speech.  Hence, there are fewer \n",
      "references available for similar investigations.  \n",
      "Table 3: Comparison of the Implementation  \n",
      "  Sl# Model Implementation  Accuracy  \n",
      "1 FFA, IFFA, LSTM, Bi_RNN [2 8] 97.6 %  \n",
      "2 Proposed Model  98.54 %  \n",
      "The proposed method improves the performance of \n",
      "emotions after segregating the speech samples into genders.  \n",
      "By separating the approach into two models, this \n",
      "implementation excels in capturing natural and \n",
      "spontaneous expressions of emotions with low latency \n",
      "manner .  \n",
      "7- Conclusion  and Future Work  \n",
      "Identification of gender from whispered speech and \n",
      "recognizing emotions is a complicated task. This \n",
      "implementation initially detects the gender from the \n",
      "speech samples before identifying the emotions. Emotions \n",
      "might vary based on gender in the same situation. Speech \n",
      "features such as Fu ndamental Frequency and SDC help \n",
      "with gender identification. MFCC, Mel -scale spectrogram, \n",
      "Spectral flux, Spectral Centroid, and Chroma STFT play a \n",
      "vital role in detecting emotions. Multifeature fusion helps \n",
      "to combine speech features into a single set of d ata points . \n",
      "The concept verifies with the publicly available dataset \n",
      "wTIMIT with an accuracy of 98.54%. This approach helps \n",
      "to identify nearly inaudible emotions and is used to figure \n",
      "out the strategy. The proposed methodology can be \n",
      "improved using other s peech features, machine learning, \n",
      "and deep learning concepts.  \n",
      " \n",
      "References  \n",
      "[1]  ST Jovicic , and Z  Saric , ―Acoustic analysis of  consonants in \n",
      "whispered speech,‖ Journal of voice , vol 22, no. 3, pp. 263–\n",
      "274, 2008 . \n",
      "[2]  M Kumari , and I  Ali, ―An efficient algorithm for gender \n",
      "detection using voice samples,‖ 2015 Communication, \n",
      "Control and Intelligent Systems (CCIS),  2015 , Mathura, \n",
      "Utter Pradesh,  pp. 221–226, doi: \n",
      "10.1109/CCIntelS.20 15.7437912 . \n",
      "[3] S Motamed , S Setayeshi, A Rabiee , and A Sharifi, ―Speech \n",
      "Emotion Recognition Based on Fusion Method,‖ Journal of \n",
      "Information Systems and Telecommunication (JIST), vol. 3, \n",
      "pp. 50--56, 2017, doi: 10.7508/jist.2017.17.007 .  \n",
      "[4] JS Li, CC Huang , ST Sheu , and MW  Lin, ―Speech emotion \n",
      "recognition and its applications,‖ Proc. of Taiwan Institute of \n",
      "Kansei Conference , 2010  Paris , France , pp. 187–192.  \n",
      "\n",
      " \n",
      "Journal of Information Systems and Telecommunication, Vol.12, No.2,  April -June 2024  \n",
      " \n",
      " \n",
      "161 \n",
      "[5] A Guerrieri , E Braccili , F Sgro , and GN Meldolesi  ―Gender \n",
      "identification in a two -level hierarchical speech emotion \n",
      "recognition system for an Italian Social Robot,‖ Sensors, vol. \n",
      "22, no. 5, pp. 1714, 2022 , doi: 10.3390/s22051714 . \n",
      "[6] M Sarria -Paja, TH Falk, and D  O’Shaughnessy , ―Whispered \n",
      "speaker verification and gender detection using weighted \n",
      "instantaneous frequencies,‖ 2013 IEEE International \n",
      "Conference on Acoustics, Speech and Signal Processing , \n",
      "2013  (May 26 -31), Vancouver , Canada, pp. 7209 –7213 , doi: \n",
      "10.1109/ICASSP.2013.6639062 .  \n",
      "[7] J Deng , S Fruhholz, Z Zhang , and Bojrn Schuller , \n",
      "―Recognizing emotions from whispered speech based on \n",
      "acoustic feature transfer learning,‖ IEEE Access , vol.  5, pp. \n",
      "5235 –5246 , 2017 .  \n",
      "[8] M Cotescu , T Drugman, G Huybrechts, J Lorenzo -Trueba, \n",
      "and A Moinet , ―Voice conversion for whispered speech \n",
      "synthesis,‖  IEEE Signal Processing Letters , vol. 27, pp. 186–\n",
      "190, 2019 , doi: 10.1109/LSP.2019.2961213 .  \n",
      "[9] P Mishra , and R Sharma, ―Gender differentiated \n",
      "convolutional neural networks for speech emotion \n",
      "recognition,‖ 2020 12th International Congress on Ultra -\n",
      "Modern Telecommuni - cations and Control Systems and \n",
      "Workshops (ICUMT),  2020  (October 5 -7), Brno , Czech \n",
      "Republic,  pp. 142–148, doi: \n",
      "10.1109/ICUMT51630.2020.9222412 . \n",
      "[10] Mustaqeem S Kwon, ―Optimal feature selection based \n",
      "speech emotion recognition using two -stream deep \n",
      "convolutional neural network,‖ International Journal of \n",
      "Intelligent Systems , vol. 36, no. 9, pp. 5116 – 5135 , 2021 , \n",
      "doi:  10.1002/int.22505 . \n",
      "[11] J. Ancilin and A. Milton, ―Improved speech emotion \n",
      "recognition with mel frequency magnitude coefficient ,‖ \n",
      "Applied Acoustics , vol. 179, pp. 108046 , 2021 , doi: \n",
      "10.1016/j.apacoust.2021.108046 .  \n",
      "[12] S. Jothimani and K. Premalatha, ―Mff -saug: Multi feature \n",
      "fusion with spectrogram augmentatio n of speech emotion \n",
      "recognition using convolution neural network ,‖ Chaos, \n",
      "Solitons & Fractals , vol. 162, pp. 112512 , 2022 , doi: \n",
      "10.1016/j.chaos.2022.112512 .  \n",
      "[13] B Yalamanchili, SK Samayamantula, and KR Anne, ―Neural \n",
      "network -based blended ensemble learning for speech \n",
      "emotion recognition,‖ Multidimensional Systems and Signal \n",
      "Processing, vol. 33, no. 4, pp. 1323 --1348 , 2022 , doi: \n",
      "10.1007/s11045 -022-00845 -9.  \n",
      "[14] T Feng , R Hebbar , and S Narayanan, ―Trustser: On the \n",
      "trustworthiness of fine -tuning pre -trained speech embeddings \n",
      "for speech emotion recognition,‖ arXiv preprint \n",
      "arXiv:2305.11229, 2023 , doi:  10.48550/arXiv.2305.11229  \n",
      "[15] RV Darekar and M Chavan , S Sharanyaa,  and NR Ranjan , \n",
      "―A hybrid meta -heuristic ensemble based classification \n",
      "technique speech emotion recognition,‖ Advances in \n",
      "Engineering Software, vol. 180, pp. 103412 , 2023 . \n",
      "[16] J Rekimot o, ―Dualvoice: A speech interaction method using \n",
      "whisper -voice as commands,‖ CHI Conference on Human \n",
      "Factors in Computing Systems Extended Abstracts,  pp. 1–6, \n",
      "2022 , doi: 10.1145/3491101.3519700 . \n",
      "[17]H Dolka,  AX VM , and S Juliet,―  Speech  emotion r ecognition  \n",
      "using  ANN on MFCC  features,‖ 2021 3rd international \n",
      "conference on signal processing and communication (ICPSC) , \n",
      "2021, Coimbatore, India, pp. 431–435, doi: \n",
      "10.1109/ICSPC51351.2021.9451810 . [18] MK Reddy and KS Rao, ―Robust pitch extraction method \n",
      "for the hmm -based speech synthesis system,‖ IEEE signal \n",
      "processing letters , vol.  24, no. 8, pp. 1133 –1137 , 2017 , doi: \n",
      "10.1109/LSP.2017. 2712646 .  \n",
      "[19] J Chatterjee, V Mukesh, HH Hsu , G Vyas , and Z Liu , \n",
      "―Speech emotion recognition using cross - correlation and \n",
      "acoustic features,‖ 2018 IEEE 16th Intl Conf on Dependable, \n",
      "Autonomic and Secure Computing, 16th Intl Conf on \n",
      "Pervasive Intelligence and Computing, 4th Intl Conf on Big \n",
      "Data Intelligence and Computing and Cyber Science and \n",
      "Technology Congress , 2018  (August 12-15), Athens, Greece , \n",
      "pp. 243–249, doi: 10.1109/DASC/PiCom/DataCom/CyberSci  \n",
      " Tec.2018.00050 .  \n",
      "[20] S Rajesh and NJ Nalini, ―Musical instrument emotion \n",
      "recognition using deep recurrent neural network,‖ Procedia \n",
      "Computer Science, vol. 167, pp. 16 --25, 2020, doi: \n",
      "10.1016/j.procs.2020.03.178 . \n",
      "[21] M Aly , and NS Alotaibi, ―A novel deep learning model to \n",
      "detect covid -19 based on wavelet features extracted from \n",
      "mel- scale spectrogram of patients’ cough and breathing \n",
      "sounds,‖ Informatics in Medicine Unlocked , vol.  32, pp.  \n",
      "101049 , 2022, doi: 10.1016/j.imu.2022.101049 . \n",
      "[22] Z Qawaqneh,  AA Mallouh,  and BD Barkana, ― Age and \n",
      "gender  classification  from  speech and face images by jointly \n",
      "fine-tuned deep neural networks,‖ Expert Systems with \n",
      "Applications , vol. 85, pp. 76–86, 2017 , doi: \n",
      "10.1016/j.eswa.2017.05.037 .  \n",
      "[23] A Koduru, HB Valiveti and AK Budati, ―Feature extraction \n",
      "algorithms to improve the speech emotion recognition rate,‖ \n",
      "International Journal of Speech Technology , vol. 23, no. 1, \n",
      "pp. 45–55, 2020 , doi: 10.1007/s10772 -020-09672 -4.  \n",
      "[24] S Zhang , and C Li, ―Research on feature fusion speech \n",
      "emotion recognition technology for smart teaching,‖ Mobile \n",
      "Information Systems , vol. 2022 , 2022 , doi:  \n",
      "10.1155/2022/7785929 . \n",
      "[25] GB, Prasanna, SV Bhat, C Naik , and HN Champa, ―An \n",
      "Efficient Method for Handwritten Kannada Digit \n",
      "Recognition based on PCA and SVM Classifier,‖ Journal of \n",
      "Information Systems and Telecommunication (JIST), vol. 3, \n",
      "no. 35, pp. 169 2021, doi: 20.1001.1.23221437.2021.9.35.3.2 . \n",
      "[26] A Graves, N Jaitly, and A Mohamed, ―Hybrid speech \n",
      "recognition wit h deep bidirectional lstm,‖ 2013 IEEE \n",
      "workshop on automatic speech recognition and \n",
      "understanding , 2013, Olomouc, Czech Republic, pp.  273–\n",
      "278, doi: 10.1109/ASRU.2013.6707742 .  \n",
      "[27] N Aloysius and M Geetha, ―A review on deep convolutional \n",
      "neural networks,‖ 2017 international conference on \n",
      "communication and signal processing (ICCSP), 2017, pp. \n",
      "0588 –0592 , IEEE, doi: 10.1109/ICCSP.2017.8286426 .  \n",
      "[28] SBC Gutha, MAB Shaik, T Udayakumar , and AA \n",
      "Saunshikhar , ―Improved feed forward attention mechanism in \n",
      "bidirectional recurrent neural networks for robust sequence \n",
      "classification,‖ 2020 International Conference on Signal \n",
      "Processing and Communications (SPCOM) , 2020 , IISc, \n",
      "Bangalore.  IEEE , pp. 1—5, doi: \n",
      "0.9179606 10.1109/SPCOM50965.202  \n",
      "\n",
      "\n",
      "Reading: d:\\PythonCode\\document_portal\\notebook\\data\\W_An_Outlier_Detection_and_Rectification.pdf\n",
      " \n",
      " \n",
      " \n",
      "Journal of Information Systems and Telecommunication  \n",
      "Vol.12, No.2,  April -June 2024, 152-161 \n",
      " \n",
      "http://jist.acecr.org  \n",
      "ISSN 2322 -1437  / EISSN:2345 -2773  \n",
      " \n",
      "Whispered Speech Emotion Recognition with Gender Detection using \n",
      "BiLSTM and DCNN  \n",
      "Aniruddha Mohanty1*, Ravindranath C Cherukuri1 \n",
      " \n",
      "1.Department of Computer  science and Engineering, Christ (Deemed to be University), Bangalore, India  \n",
      " \n",
      " \n",
      "Received: 22 Aug 2023 / Revised: 04 Apr 2024 / Accepted: 21 May 2024  \n",
      " \n",
      " \n",
      "Abstract   \n",
      "Emotions are human mental states at a particular instance in time concerning one’s circumstances, mood, and relationships \n",
      "with others . Identifying emotions from the whispered speech is complicated as the conversation might be confidential. The \n",
      "representation of the speech relies on the magnitude of its information. Whispered speech is intelligible, a low -intensi ty \n",
      "signal, and varies from normal speech. Emotion identification is quite tricky from whispered speech. Both prosodic and \n",
      "spectral speech features help to identify emotions. The emotion identification in a whispered speech happens using prosodic \n",
      "speech fea tures such as zero -crossing rate  (ZCR) , pitch, and spectral features that include spectral centroid, chroma STFT, \n",
      "Mel scale spectrogram, Mel -frequency cepstral coefficient (MFCC), Shifted Delta Cepstrum (SDC), and Spectral Flux. \n",
      "There are two parts to the proposed implementation. Bidirectional Long  Short -Term Memory (BiLSTM) helps to identify \n",
      "the gender from the speech sample in the first step with SDC and pitch. The Deep Convolutional Neural Network (DCNN) \n",
      "model helps to identify the emotions in the second  step. This implementation is evaluated using the  wTIMIT data corpus \n",
      "and gives 98.54% accuracy. Emotions have a dynamic effect on genders, so this implementation performs better than \n",
      "traditional approaches. This approach helps to design online learning management systems, different applications for \n",
      "mobile devices, checking cyber -criminal activities, emotion detection for older people, automatic speaker identification and \n",
      "authentication, forensics, and surveillance . \n",
      " \n",
      "Keywords:  Whispered Speech; Emotion Re cognition; Speech Features; Data Corpus; BiLSTM;  DCNN . \n",
      " \n",
      "1- Introduction  \n",
      "Emotions are the humans’ short -lived feeling s that affects \n",
      "thinking, actions, relationships, and social interactions. \n",
      "Emotions express humans’ physiological and emotional \n",
      "states with facial expressions and body language. \n",
      "Whispered speech is a form of speec h that expresses \n",
      "emotions. Whispered speech is a type of communication \n",
      "produced with breath without any noise and excitation of \n",
      "voice. The whispered speech structure changes \n",
      "significantly because of the lack of periodic excitation in \n",
      "the voice. This result s in missing speech and reduced \n",
      "transparency in communication.  \n",
      "The difference between normal and whispered speech is \n",
      "the absence of vocal tract vibrations due to the vocal \n",
      "tract’s physiological blocking. The strength of whispered \n",
      "speech is minute and with out voice compared to phoned \n",
      "speech. The spectral and prosodic features help to detect \n",
      "the whispered speech. Prosodic features of speech vary \n",
      "over time. Spectral features of whispered speech are highly accurate over unvoiced consonants, voiced consonants, \n",
      "and formants in vowels [1]. \n",
      "The impacts on Speech Emotion Recognition ( SER) are \n",
      "due to various acoustic conditions such as compressed, \n",
      "noisy, telephonic conversation s and imitator speeches. \n",
      "Other environmental conditions,  such as stress, rhythm, \n",
      "and intonation, can affect  SER.  Whispered speech is also \n",
      "affected by similar acoustic and environmental conditions. \n",
      "SER depends on acoustic characteristics and gender in \n",
      "some scenarios. Hence gender plays a vital role in SER.  \n",
      "Nowadays, several whispered speech  samples of male and \n",
      "female voices are available. The effectiveness of the SER \n",
      "is more when the implementation is in two parts. The \n",
      "identification of the gender [2] happens initially using \n",
      "Bidirectional Long Short -term Memory (BiLSTM)  with \n",
      "speech features . The next step is detecting emotions using \n",
      "various speech features [3] and Deep Convolutional \n",
      "Neural Network (DCNN).   \n",
      "The structure of the paper includes various sections. \n",
      "Section 2 describes Human Emotions and their \n",
      "Applications. Details of the Related Work are in Section 3. \n",
      "Section 4 describes the System Model, which is the black \n",
      "box view of the implementation. Section 5 is the Model \n",
      " \n",
      "Journal of Information Systems and Telecommunication, Vol.12, No.2,  April -June 2024  \n",
      " \n",
      " \n",
      "153 \n",
      "Design that describes the details of the speech features and \n",
      "the deep learning models used in this implementation. \n",
      "Section 6 gives the Experiment and Result Analysis. \n",
      "Section 7 brief ly discuss  the Conclusion and Future Work.   \n",
      "2- Human  Emotions  and their  Applications  \n",
      "Human emotions are the mental state caused by countless \n",
      "associated views , feeling s, behavioral replies, and the \n",
      "degree of pressure and annoyance. It is often associated \n",
      "with attitude , temperament, behavior , disposition, and \n",
      "creativi ty [4]. Emotion recognition helps to detect a \n",
      "humans ’ emotional mood , which lasts hours and days. \n",
      "Speech conveys emotions such as anger, disgust, fear, \n",
      "happiness, neutrality, sadness, and surprise. The machine \n",
      "automatically detects various emotions from speech with \n",
      "the help of different algorithms. Emotion Recognition \n",
      "helps to : \n",
      " \n",
      "•  Detect customers ’ intentions based on the teleconference. \n",
      "•  Detect cybercrime.  \n",
      "•  Students ’ attention and teachers ’ content adjustment.  \n",
      "•  Disability assistance  \n",
      "•  Customer satisfaction  \n",
      "•  Stress monitoring  \n",
      "•  Social media analysis  \n",
      "•  Suspicious activity  \n",
      "•  Human -machine interaction and so on.   \n",
      "3- Related Work  \n",
      "Over time, numerous studies have detected emotions from \n",
      "whispered speech . The vario us deep learning models \n",
      "detect emotions based on the speech features extracted \n",
      "from the collected whispered speech data corpus . The SER \n",
      "for normal and whispered speech is diverse because of \n",
      "vocal excitation. Emotions vary with many factors; gender \n",
      "is among  the most influential factors  [5]. Identifying \n",
      "gender in the first step improves emotion detection from \n",
      "whispered speech. So, the related work explored is on \n",
      "gender detection and emotion recognition from whispered \n",
      "speech.  \n",
      "MFCC obtained by the Hilbert enve lope approach and \n",
      "weighted instantaneous frequencies (WIFs) obtained by \n",
      "the coherent demodulation help to detect gender in \n",
      "whispered speech  samples [6]. There is an opportunity to \n",
      "explore gender detection in noisy speech conditions using \n",
      "these approaches.  \n",
      "Autoencoder -enabled features  in the transfer learning \n",
      "framework propose to practice  phonated data to identify \n",
      "emotions from Whispered speech [7]. The feature \n",
      "extraction is from the Geneva Whispered Emotion Corpus \n",
      "(GeWEC) and Berlin Emotional Speech Databa se (EMO -\n",
      "DB) data corpus. The acoustic features such as Mel-frequency cepstral coefficient (MFCC), root mean square \n",
      "(RMS), frame energy, zero -crossing -rate (ZCR), pitch \n",
      "frequency (F0), probability of voicing autocorrelation \n",
      "function are the inputs to evaluate the framework. \n",
      "Implementing deep learning concepts on spontaneous data \n",
      "gives more accuracy than the current framework .  \n",
      "Gender is detected to target an anonymous speaker. The \n",
      "Deep Neural Network (DNN) model is used to generalize \n",
      "gender by using the MFCC speech feature. This \n",
      "implementation is applied to the wTIMIT dataset to verify \n",
      "the gender and recognize the speaker  [8]. The DNN model \n",
      "cannot generalize gender; evaluation can happen with \n",
      "other datasets.   \n",
      "MFCC and CNN, with the fully  connected network, detect \n",
      "gender and emotions like anger, disgust, fear, happiness, \n",
      "sadness, surprise, and a neutral state  [9]. The concept \n",
      "verification happens on RAVDESS, CREMA -D, SAVEE, \n",
      "and TESS datasets, having an accuracy of 92.283%, which \n",
      "is better than the tr aditional model.  \n",
      "The final prediction of the implemented model is to learn \n",
      "the mutually spatial -spectral features happens by a two -\n",
      "stream deep convolutional neural network with an iterative \n",
      "neighborhood  component analysis (INCA) and the most \n",
      "discriminatory optimal features  [10]. The concept \n",
      "verification happens on EMO -DB, SAVEE, and \n",
      "RAVDESS emotional speech corpora which perform with \n",
      "95%, 82%, and 85% accuracy rates. Real -time applications \n",
      "with natural and huge data corpora can help to extend this \n",
      "concept to identify emotions.  \n",
      "Mel Frequency Magnitude Coefficient (MFMC) and three \n",
      "spectral features, namely MFCC, log frequency power \n",
      "coefficient, and linear prediction cepstral coefficient, are \n",
      "used with the help of Support Vector Machine (SVM) \n",
      "modeling  [11]. The performance evaluation uses this \n",
      "concept  on Berlin, RAVDESS, SAVEE, EMOVO, and \n",
      "eNTERFACE data corpus . Feature selection, feature \n",
      "fusion, and multiple classification schemes improve the \n",
      "performance of MFMC.  \n",
      "The proposed MFF -SAug research in  which noise removal \n",
      "improves emotion. White Noise Injection, Pitch Tuning \n",
      "techniques, MFCC, ZCR, RMS speech features, and \n",
      "Convolutional Neural Network (CNN) modeling [12] \n",
      "detect emotions. Emotion detection during the interaction \n",
      "between people can extend the MFF -SAug approach .  \n",
      "The proposed Neural Network -based Blended Ensemble \n",
      "Learning (NNBEL) model is composed of a 1 -dimensional \n",
      "Convolution Neural Network (1D -CNN), Long Short -\n",
      "Term Memory (LSTM), and CapsuleNets. LSTM receives \n",
      "input from the Log Mel -spect rogram speech features, \n",
      "while 1D -CNN and CapsuleNets receive input from the \n",
      "MFCC.  Each model’s output is fed to Multi -Layer \n",
      "Perceptron (MLP) and predicts the final emotion s [13]. \n",
      "This model shows 95.3% and 94% accuracy on \n",
      "RAVDESS and IEMOCAP datasets, respectively.  \n",
      " \n",
      "Mohanty, & Cherukuri, Whispered Speech Emotion  Recognition with Gender Detection using BiLSTM and DCNN  \n",
      " \n",
      " \n",
      " \n",
      "154 \n",
      "TrustSER [14] implemented a general framework to \n",
      "determine the SER system's trustworthiness using deep \n",
      "learning techniques. Trustworthiness evaluates privacy \n",
      "(gender in formation, speaker demographics), safety, \n",
      "fairness, sustainability, and emotions (sad, angry, happy, \n",
      "neutral). The architecture of the TrustSET framework uses \n",
      "CNN encoder and Transformer encoder models. \n",
      "Trustworthy profiles under the Federated Learning \n",
      "scenario might improve privacy, fairness, and safety.  \n",
      "A hybrid meta -heuristic ensemble -based classification [15] \n",
      "helps to detect speech emotions. Raw speech samples are \n",
      "filtered using the Butterworth filter; then, spectrogram \n",
      "speech features are extracted from each frame to create a \n",
      "hybrid feature vector. The ensemble -based classification is \n",
      "applied to hybrid feature vectors to classify em otions. The \n",
      "ensemble -based classification contains a Recurrent Neural \n",
      "Network (RNN), a Deep Belief Network (DBN), and an \n",
      "Artificial Neural Network (ANN).  \n",
      "The above -related work shows that it is crucial to identify \n",
      "gender to segregate emotions in a speech. Male and female \n",
      "emotions are different based on situations. Whispered \n",
      "speech is an essential concept of emotion recognition. \n",
      "There is an opportunity to experiment further based on \n",
      "other datasets of whispered speech. So, this is a motivation \n",
      "to work on emot ion recognition in whispered speech, as \n",
      "this is a less explored area of research .  \n",
      "4- System Model  \n",
      "The analysis of e motion recognition from the whispered \n",
      "speech segregates into two parts. One is gender detection, \n",
      "and the second is emotion detection as shown in Fig 1.  \n",
      " \n",
      " \n",
      "Fig. 1 Emotion recognition from whispered speech  \n",
      " \n",
      "As part of gender detection, pre -processing of the speech \n",
      "samples is performed, and then pitch and SDC are \n",
      "extracted as part of prosodic and spectral features, \n",
      "respectively. Both the features are fused to get a single \n",
      "feature set with the help of multifeature fusion. PCA helps \n",
      "to reduce the dimensions of extracted features and is used \n",
      "as input to Bidirectional Long short -term memory \n",
      "(BiLSTM) to classify genders , as shown in Fig 2.  \n",
      " \n",
      " \n",
      "Fig .2 Gender detection from whispered speech  \n",
      " \n",
      "Following  Gender detection, the augmentation of speech \n",
      "data helps to get more realistic data. Then the speech features such as Chromatogram, Zero -Crossing Rate, \n",
      "Spectral Central, MFCC, Spectral Flux , and Mel \n",
      "Spectrog rams are extracted. All the extracted features \n",
      "combine to get a single feature using multifeature fusion. \n",
      "Then the dimensions are reduced to get the optimal data \n",
      "points. The data points are inserted into the Deep \n",
      "Convolutional Neural Network (DCNN) to identify the \n",
      "emotions , as shown in Fig 3.  \n",
      "5- System  Design  \n",
      "The implementation of system design happens in two parts. \n",
      "The first part identifies the male and female gender from \n",
      "the whispered speech samples. Then the determination of \n",
      "different emotions like sadn ess, happiness, fear, anger, \n",
      "surprise, disgust, and neutral are detected in gender -\n",
      "segregated speech samples.  \n",
      " \n",
      "Fig. 3 Emotion Recognition from Male and Female whispered speech.  \n",
      "5-1- Whispered Speech Data Corpus  \n",
      "wTIMIT [16] is a whispered voice data corpus ha ving 450 \n",
      "phonetically balanced sentences with 29 speakers. There \n",
      "are 11,324 utterances with a normal voice , which  can be \n",
      "used for normal and whispered training. This data is \n",
      "available in two parts - train and test divisions. The \n",
      "samples contain an 8 kHz sampling rate with a high -\n",
      "quality  and low-pass filter.  \n",
      "5-2-    Data Augmentation  \n",
      "Data augmentation [17 ] is the method of getting more \n",
      "realistic data from the existing data, which helps to add \n",
      "more training data to the model, reduce overfitting, and \n",
      "increase the model’s generalization ability. Below are the \n",
      "steps to generate synthetic data.  \n",
      " Shifting Time : Shifting time is a simple concept \n",
      "where the audio shifts to the left or right with a \n",
      "haphazard second. If the audio shifts to the left \n",
      "\n",
      " \n",
      "Journal of Information Systems and Telecommunication, Vol.12, No.2,  April -June 2024  \n",
      " \n",
      " \n",
      "155 \n",
      "with z seconds, then the first z seconds are \n",
      "marked as silence. Similarly, if the audio shifts to \n",
      "the right with z second s, the last z seconds are \n",
      "marked as silence.  \n",
      " Changing Pitch: Pitch change is adjusting the \n",
      "pitch randomly without upsetting the speediness \n",
      "of the audio file.  \n",
      " Changing Speed: Speed audio changes by \n",
      "stretching the time series data by a fixed rate.  \n",
      "5-3- Pre-processing  \n",
      "After the data collection and data augmentation, pre -\n",
      "processing [3] is the initial  phase  in training SER. \n",
      "Framing, Windowing, Voice activity detection, \n",
      "Normalization, and Noise reduction are pre -\n",
      "processing steps.   \n",
      "Framing:  Speech signals are quasip eriodic and vary \n",
      "over time. Hence, information and related emotions \n",
      "also fluctuate over time. The segregation of the \n",
      "signals happens in a shorter period to make the \n",
      "speech signal invariant. Twenty milliseconds to thirty \n",
      "milliseconds helps to make the speec h signal \n",
      "invariant, and five milliseconds of overlap of the \n",
      "frames avoid data leakage between the frames .  \n",
      "Windowing:  Windowing on each frame helps to diminish \n",
      "data leakage during Fast Fourier Transformation (FFT) \n",
      "after framing. The Hamming window allows  this step \n",
      "where the window size is N for the frames     , used in \n",
      "equation (1).  \n",
      " \n",
      "                  (   \n",
      "   )       (1) \n",
      "            \n",
      " \n",
      "Voice activity detection (VAD):  An utterance has three \n",
      "portions of speech activities : voiced, unvoiced, and silent. \n",
      "Zero Crossing Rate (ZCR) speech feature helps to detect \n",
      "VAD. ZCR represents the frequency of signal transitions \n",
      "between positive and negative values within a specific \n",
      "frame . Due to high energy, the ZCR value is low for \n",
      "voiced speech and high for unvoiced s peech because of \n",
      "low energy.  \n",
      "Normalization:  Normalization helps to reduce  speaker \n",
      "and recording inconsistency without affecting the features’ \n",
      "strength and enhancing the features’ generalization \n",
      "capability. The Z-normalization method is used more and \n",
      "represented as  \n",
      "       \n",
      "      (2) \n",
      "Where   is the speech signal,   and   are the mean and \n",
      "standard deviation of the data, respectively.  \n",
      "Noise Reduction:  The environmental noise captured while \n",
      "recording a speech signal  affects the recognition rate . \n",
      "Minimum mean square error (MMSE) and log -spectral \n",
      "amplitude MMSE (LogMMSE) reduce the noise from the speech signals. Noise reduction helps to get more accuracy \n",
      "in SER evaluation.  \n",
      "5-4- Feature Extraction  \n",
      "Specific emotions are present in prosodic and spectral \n",
      "features of speech. The extraction of prosodic and spectral \n",
      "features is a vital characteristic of emotion recognition \n",
      "after pre -processing the speech signals.  \n",
      "5-4-1-Prosodic Features  \n",
      "Prosodic features deal with the audio qualities of a speech \n",
      "when connected speeches use sounds as input. The \n",
      "production of the speech deals with the amount of energy, \n",
      "frequency, period, loudness, pitch, and duration. Speech \n",
      "signal com munication depends on intonation, stress, and \n",
      "rhythm, which prosodic features can detect. The prosodic \n",
      "features used in the implementation are : \n",
      "Zero -Crossing Rate (ZCR):  ZCR [17] measures the \n",
      "frequency of signal transitions between positive and \n",
      "negative values in every audio frame  and defined as  \n",
      "    \n",
      "    ∑       [ ]         [      \n",
      "    (3) \n",
      "Where        is the sign function . \n",
      "    [     ]  {        \n",
      "       \n",
      "Fundamental Frequency (Pitch):  Fundamental \n",
      "Frequency (F0) [18] is the minimum freque ncy of the \n",
      "periodic waveform. F0 is the significant parameter to \n",
      "differentiate male speech from female speech. Pitch also \n",
      "determines the voiced and unvoiced portion of the speech \n",
      "signal. This analysis uses pitch parameters like pitch mean \n",
      "value and pitch r ange.  \n",
      "The pitch range determines the number of octaves a speech  \n",
      "sample  can cover, from the lowest to the highest. F0 value \n",
      "varies from 85Hz to 180 HZ for the voiced speech of adult \n",
      "males.  \n",
      " \n",
      " \n",
      "Fig. 4 Representation of Pitch data  \n",
      " \n",
      "Similarly, the F0 value for adult females goes from 165Hz \n",
      "to 255Hz, as shown in Fig 4.  \n",
      "\n",
      " \n",
      "Mohanty, & Cherukuri, Whispered Speech Emotion  Recognition with Gender Detection using BiLSTM and DCNN  \n",
      " \n",
      " \n",
      " \n",
      "156 \n",
      "5-4-2-Spectral features  \n",
      "The representation of spectral features happens by \n",
      "converting the time -domain speech signal into the \n",
      "frequency domain with the help of the Fast Fourier \n",
      "Transform (FFT) which  benefits  to represent the \n",
      "characteristics of the Human Vocal Tract. The  spectral \n",
      "features are  \n",
      "Spectral Centroid (SC):  The Spectral Centroid [19] is the \n",
      "most used speech feature where the positioning of the \n",
      "―center of mass‖ of the audio signal spectr um defines the \n",
      "weight of the spectrum. The center of mass measures the \n",
      "weighted average of the frequency component located in \n",
      "the audio signal, defined as  \n",
      " \n",
      "      ∑            \n",
      "   \n",
      "∑       \n",
      "       (4) \n",
      "Where      represents the magnitude of bin numb er   and \n",
      "      represents the central frequency of the bin.  \n",
      "Chroma STFT:  Chroma STFT [20] is obtained using FFT \n",
      "on speech samples and the resulting spectrums a re a \n",
      "chromatogram in a vertical axis. This feature captures the \n",
      "harmonic feature of the speech s ignals.  \n",
      "Mel-scale Spectrogram:  Mel-scale Spectrogram [ 21] is a \n",
      "spectrogram in which frequencies convert to the Mel scale, \n",
      "which helps to differentiate the range of frequencies. Mel -\n",
      "spectrogram helps to understand emotions in a better way \n",
      "as humans can perceive sound on a logarithmic scale.  \n",
      "Mel Frequency Cepstral Coefficient (MFCC ): MFCC \n",
      "[17] i s SER’s  standard feature extraction technique. The \n",
      "vocal cord s, tongue, and teeth filter the sound and make it \n",
      "unique for each speaker in the Human Speech production \n",
      "system. Mel scale represents MFCC, where the frequency \n",
      "bands are equally spaced and close to the Human Auditory \n",
      "System’s response, as shown in Fig 5.  \n",
      " \n",
      "Fig 5 MFCC speech feature extraction  \n",
      " \n",
      "Shifted Delta Cepstrum (SDC):  Time derivatives apply \n",
      "to the cepstral coefficients obtained from the MFCC and \n",
      "combined with the delta coefficient to get SDC, as shown \n",
      "in Fig 6.       and   are the four parameters used in \n",
      "SDC [ 22].  M denotes the cepstral coefficient for each \n",
      "frame.   indicates added frames.   are the frames that \n",
      "append delta features from the new feature vector .   \n",
      "represents the delta values difference. So, the coefficient \n",
      "vectors represent as   \n",
      "     [                  ]   (5) \n",
      "   are MFCC coefficients and   is the coefficient index. \n",
      "For a given time,   an intermediate calculation is done to \n",
      "obtain the   coefficients.  \n",
      "                                 (6) \n",
      "Finally, the SDC coefficients vectors of   dimensions are \n",
      "obtained as  \n",
      "       [          (                )] (7) \n",
      " \n",
      " \n",
      "Fig. 6 Extraction of SDC speech feature  \n",
      " \n",
      "This speech feature helps to identify genders for a long -\n",
      "range dynamic appearance in speech signals.  \n",
      "Spectral Flux:  Spectral flux [ 20] measures the change in \n",
      "speed of the signal’s power spectrum compared to the \n",
      "previous frame. This feature estimates the speech signal’s \n",
      "power spectrum about  the power spectrum of one frame \n",
      "with others.  \n",
      "5-5- Feature Selection and Dimension Reduction  \n",
      "Feature s election [23] is selecting features from a large set \n",
      "of extracted features to eliminate redundant and unused \n",
      "information and decrease processing time. For this work, \n",
      "the extracted pitch contains lots of information. The global \n",
      "features, like the range of p itch values and mean pitch \n",
      "values, are selected. Removing redundant and unused \n",
      "information, such as additional zeroes, duplicate values, or \n",
      "frames, is part of MFCC feature extraction. The exact \n",
      "process removes the silent speech frames.  \n",
      "Fundamental frequen cy and SDC are two speech features \n",
      "to identify gender. Similarly, Spectral centroid, Chroma \n",
      "STFT, MFCC, and Spectral flux features are used for \n",
      "emotion identification to create a single set of data points \n",
      "from multiple speech features using multifeature fu sion \n",
      "technique [24]. \n",
      "Multifeature Fusion:  The extracted speech features hav e \n",
      "different dimensions, so feature proximity usages  average \n",
      "dimensional spacing between the vectors to denote the \n",
      "proximity between the diverse features  [24]. The average \n",
      "dimensional spacing between the vectors is computed as  \n",
      "          \n",
      "    √∑∑              (8) \n",
      "   =  \n",
      "     √∑∑              (9) \n",
      "\n",
      " \n",
      "Journal of Information Systems and Telecommunication, Vol.12, No.2,  April -June 2024  \n",
      " \n",
      " \n",
      "157 \n",
      "        and     are the mean and variance interval of \n",
      "average dimensions of the feature vector s.    and    are \n",
      "the mean value of   and   type of sound features.     and    \n",
      "are the mean value of   and   type of sound features . Then \n",
      "the subsequent dimensionality reduction [25] method \n",
      "follows once the feature selection process is complete. \n",
      "High data variance is present in the extracted features \n",
      "containing more information. Dimension reduction \n",
      "techniques reduce the dimensions from extracted feature \n",
      "vectors.  \n",
      " \n",
      "5-5-1- Principal Component Analysis  (PCA)  \n",
      "PCA [25] is an approach for redu cing the dimensionality \n",
      "of extensive datasets, transforming them into more \n",
      "compact representations while retaining crucial \n",
      "information . These reductions in the number of variables \n",
      "help to get more accurate results. A reduced dataset makes \n",
      "it easier and fas ter to visualize and analyze the data. \n",
      "Following steps followed to explore PCA .  \n",
      " Standardization  creates a normalized dataset \n",
      "when there is a significant difference in the range \n",
      "of initial variables or a larger range of datasets \n",
      "dominates the smaller range. It can be done by  \n",
      "             \n",
      "                     (10) \n",
      " Covariance matrix computation  helps  to know  \n",
      "how the input  dataset  varies  from  the mean value \n",
      "to each data point.  \n",
      " Eigenvectors and eigenvalues  of the covariance \n",
      "matrix help to identify the principal component.  \n",
      "5-6- Modelling  \n",
      "This implementation uses two deep l earning models. \n",
      "Bidirectional Long Short -Term Memory (BiLSTM) \n",
      "classifies the genders in whispered speech, and Deep \n",
      "Convolutional Neural Network (DCNN) identifies \n",
      "emotions.  \n",
      "BiLSTM  [26] combines  two Recurrent Neural Networks \n",
      "(RNN) that are placed independently and can traverse \n",
      "backward and forward directions at each time step, as \n",
      "shown in Fig 7. There are two ways to deal with data in \n",
      "this model - The first involves processing data from the \n",
      "past to the future , and the second operates in the opposite \n",
      "direction, from future  to past , where two hidden layers of \n",
      "neurons help to preserve the information in both \n",
      "directions. This approach helps to improve the information \n",
      "available in the algorithm.  \n",
      "For B iLSTM, the sequence of inputs is \n",
      "                   for a traditional RNN, which \n",
      "computes the hidden vector sequences \n",
      "                  and results in the output \n",
      "sequences                    for the iteration \n",
      "    to                            (11) \n",
      "                (12) \n",
      "  are the weight matrices;   is the bias vector, and   is \n",
      "the hidden layer function.  \n",
      "DCNN  [27] is a Convolutional Neural Network (CNN) \n",
      "type that helps to identify emotions from whispered \n",
      "speech . The input to the model is the speech data that \n",
      "traverses through many stages, such as the convolution \n",
      "layer, the pooling layer, Activation, Fully Connected, \n",
      "Batch normalization, and dropout layers, as shown in Fig \n",
      "8.  \n",
      " \n",
      " \n",
      " \n",
      "Fig 7. Structure of BiLSTM  \n",
      " \n",
      "To avoid overfitting, Leaky ReLU is used as an activation \n",
      "function in this implementation and evaluated as  \n",
      "{                      \n",
      "                        (13) \n",
      "The fully connected layer is a loss layer, measuring the \n",
      "inconsistency between the desired and actual outputs . Root \n",
      "The Mean Squared Propagation (RMSProp) optimization \n",
      "algorithm enhances the loss function, which varies in \n",
      "vertical directions.   \n",
      " \n",
      "Fig. 8 Structure of Deep Convolution Neural Network  \n",
      "5-7- Emotions  \n",
      "Deep learning techniques are applied to identify the \n",
      "emotions of human speech. Discrete Emotion [3] \n",
      "theory uses seven vital emotions in Human activities.  \n",
      " Sadness:  This is the feeling of dissatisfaction, \n",
      "sorrow, or fruitlessness.  \n",
      " Happiness:  This is the emotional state that elicits \n",
      "satisfaction and pleasure.  \n",
      " Fear:  This feeling triggers a fright response.  \n",
      " Anger:  This emotional state leads to feelings of \n",
      "hostility and frustration.  \n",
      " Surprise:  It is the state of mind that expresses \n",
      "either positive or negative following something \n",
      "unexpected.  \n",
      "\n",
      " \n",
      "Mohanty, & Cherukuri, Whispered Speech Emotion  Recognition with Gender Detection using BiLSTM and DCNN  \n",
      " \n",
      " \n",
      " \n",
      "158 \n",
      " Disgust:  A strong emotion that results in feeling \n",
      "repulsed.  \n",
      " Neutral:  This is the feeling of lack of particular \n",
      "preference.  \n",
      " \n",
      "5-8- Algorithm for Emotion Recognition  \n",
      "The algorithm to detect emotions from the whispered \n",
      "speech is in two parts  \n",
      " Gender Detection  \n",
      " Emotion Identification   \n",
      " \n",
      "Algorithm 1:  Geneder Detection  \n",
      "INPUT: Text  File (Whispered Speech Samples)  \n",
      "OUTPUT: Emotions (Sad, Happy, Fear, Anger, Surprise, \n",
      "Disgust, Neutral)  \n",
      " \n",
      "1: Begin:  \n",
      "2: Read speech  samples from Corpus   \n",
      "3: Pre-processing  the speech  samples:  \n",
      "4: Processed_speech=Pre_processing (Speech_Sample)   \n",
      "5: Extracted  SDC,  Pitch  features:  \n",
      "6: SDC, Pitch=Feature_Extraction(Processed_speech)   \n",
      "7: Multifeature  fusion:  \n",
      "8: multi _feature = multifeatured _fusion(SDC,  Pitch)   \n",
      "9: Dimension  reduction:  \n",
      "10: Dimension _R = Dimension _Reduction(multi _feature)  \n",
      "11: Determine  gender:  \n",
      "12: Gender = BiLSTM  (Dimension _R) \n",
      "13: Placed  in folders:  \n",
      "14: Gender=  (Male,  Female)   \n",
      "15: End:  \n",
      "Algorithm 2 Emotion Identification  \n",
      " \n",
      "1:  Begin:   \n",
      "2:  Speech Samples = (Male,  Female)  \n",
      "3:  Data Augmentation:  \n",
      "4:  Augemented _samples = Data Augmentation  (Speech          \n",
      "Samples)  \n",
      "5:  Pre-processed  the speech  samples:   \n",
      "6:  Processed _speech=Preprocessing  (Augemented _sam \n",
      "ples)  \n",
      "7:  Extracted  Spectral  Centroid,  ChromaSTFT,  MFCC,  \n",
      "Mel spectrogram  features:   \n",
      "8:  Spectral _Centroid,  Chroma _STFT,  MFCC,  \n",
      "Mel_spectrogram = Feature Extraction  (Processed _spe \n",
      "ech)  \n",
      "9:  Multifeature  fusion:   \n",
      "10:  multi _feature = \n",
      "multifeatured _fusion(Spectral _Centroid,  Chroma _STFT,  \n",
      "MFCC,  Mel_spectrogr am) \n",
      "11:  Dimension  reduction:  \n",
      "12:  Dimension_R = Dimension_Reduction(multi_feature)  \n",
      "13:  Emotion  detection:  14:  Emotions = DCNN(Dimension_R)   \n",
      "15:  OUTPUT Emotions  (Sad, Happy, Fear, Anger, \n",
      "Surprise, Disgust, Neutral)  \n",
      "16:  End:  \n",
      "6- Experiment and Result Analysis  \n",
      "The proposed SER has been implemented utilizing Python \n",
      "programming language and fortified by various machine \n",
      "learning libraries and additional supporting libraries . The \n",
      "experiment uses Python (Python 3.6.3rc1) and Librosa \n",
      "(Librosa 0.8.0) for audio processing. Graph plotting uses \n",
      "Seaborn and Matplotlib libraries, which help to analyze the \n",
      "speech data. BiLSTM and DCNN models implement the \n",
      "model using Keras (Keras - 2.6.0), TensorFlow \n",
      "(TensorFlow -2.6.0), and Scikit -learn libraries .  \n",
      "wTIMIT dataset is collected and divided into the train, \n",
      "cross -validation, and test samples. Subsequently, the \n",
      "speech processing takes place during implementation, \n",
      "incorporating data -augmentation techniques like shifting \n",
      "time, changing pitch, and changing speed . The pre -\n",
      "processing of speech content from each sample helps to \n",
      "analyze it and extract its features. The dimensionally \n",
      "reduced extracted features align with the data points in the \n",
      "BiLSTM model. The training dataset makes the model \n",
      "learn the features or data. Epochs continuously learn \n",
      "hidden features when the dataset completes backward and \n",
      "forward iterations. Then the cross -validation dataset is \n",
      "used to estimate the model’s performance on each epoch \n",
      "and prevent the model from being overfit ted. The test \n",
      "dataset helps to evaluate the trained model unbiasedly \n",
      "using performance metrics like precision, recall, f1 score, \n",
      "and confusion Matrix .  \n",
      "The parameters used in BiLSTM models are as in Table 1.  \n",
      "Table 1: Parameters of the BiLSTM model  \n",
      "Layers  Shape of output  Parameters#  \n",
      "Embedding  \n",
      "(Embedding)  (None, 216, 256)  \n",
      " 524544  \n",
      " \n",
      "Bidirectional  \n",
      " (None, 128)  \n",
      " 98816  \n",
      "batch normalization  (None, 128)  \n",
      " 512 \n",
      "Dense (Dense)  (None, 128)  \n",
      " 10512  \n",
      "dropout (Dropout)  (None, 128)  \n",
      " 0 \n",
      "flatten (Flatten)  (None, 128)  \n",
      " 1024  \n",
      "dense1(Dense)  \n",
      " (None, 12)  \n",
      " 0 \n",
      "dense2(Dense)  \n",
      " (None, 12)  \n",
      " 455 \n",
      " \n",
      " \n",
      "Journal of Information Systems and Telecommunication, Vol.12, No.2,  April -June 2024  \n",
      " \n",
      " \n",
      "159 \n",
      "The BiLSTM model helps to segregate the speech samples \n",
      "into male and female. The segregated speech samples \n",
      "move to their respective male and female folders.  \n",
      "Table 2: Parameters of the DCNN model  \n",
      "Layers  Shape of output  Parameters#  \n",
      "conv1d (Conv1D)  \n",
      " (None, 216, 256)  \n",
      " 2304  \n",
      "Activation  \n",
      "(Activation)  (None, 216, 256)  \n",
      " 0 \n",
      "conv1d 1 (Conv1D)  \n",
      " (None, 216, 256)  524544  \n",
      "batch normalization  \n",
      " (None, 216, 256)  1024 \n",
      "activation 1  \n",
      "(Activation)  \n",
      " (None, 216, 256) 0 \n",
      "dropout (Dropout)  (None, 216, 256)  \n",
      " 0 \n",
      "max pooling1d \n",
      "(MaxPooling1D)  \n",
      " (None, 13, 256)  \n",
      " 0 \n",
      "conv1d 2 (Conv1D)  \n",
      " (None, 13, 128)  \n",
      " 262272  \n",
      " \n",
      "activation 2 \n",
      "(Activation)  \n",
      " (None, 13, 128)  \n",
      " 0 \n",
      "conv1d 3 (Conv1D)  \n",
      " (None, 13, 128)  \n",
      " 131200  \n",
      " \n",
      "activation 3 \n",
      "(Activation)  \n",
      " (None, 13, 128)  \n",
      " 0 \n",
      "conv1d 4 (Conv1D)  \n",
      " (None, 13, 128)  \n",
      " 131200  \n",
      " \n",
      "batch normalization  \n",
      " (None, 13, 128)  \n",
      " 512 \n",
      "activation 4 \n",
      "(Activation)  \n",
      " (None, 13, 128)  \n",
      " 0 \n",
      "conv1d 5 (Conv1D)  \n",
      " (None, 13, 128)  \n",
      " 131200  \n",
      " \n",
      "batch normalization \n",
      "1 \n",
      " (None, 13, 128)  \n",
      " 512 \n",
      "activation 5 \n",
      "(Activation)  \n",
      " (None, 13, 128)  \n",
      " 0 \n",
      "dropout 1 (Dropout)  \n",
      " (None, 13, 128)  \n",
      " 0 \n",
      "max pooling1d 1 \n",
      "(MaxPooling1D)  \n",
      " (None, 1, 128)  \n",
      " 0 \n",
      "conv1d 6 (Conv1D)  \n",
      " (None, 1, 64)  \n",
      " 65600  \n",
      "activation 6 \n",
      "(Activation)  \n",
      " (None, 1, 64)  \n",
      " 0 \n",
      "conv1d 7 (Conv1D)  \n",
      " (None, 1, 64)  \n",
      " 32832  \n",
      "activation 7 \n",
      "(Activation)  \n",
      " (None, 1, 64)  \n",
      " 0 \n",
      "flatten (Flatten)  \n",
      " (None, 64)  \n",
      " 0 dense (Dense)  \n",
      " (None, 14)  \n",
      " 910 \n",
      "activation 8 \n",
      "(Activation)  \n",
      " (None, 14)  \n",
      " 0 \n",
      " \n",
      "After separating the speech samples into genders, Chroma \n",
      "STFT, Spectral centroid, Mel-scale spectrogram, and \n",
      "Spectral Flux feature s extraction happen ed. Then the \n",
      "created data points are dimensionally reduced to fit into \n",
      "the DCNN deep learning model to identify emotions. \n",
      "Metrics like precision, recall, F1 score, and the confusion \n",
      "matrix aid in assessing the model's performance.  Table 2 \n",
      "mentions the parameters of the DCNN model.  \n",
      "6-1- Result Analysis  \n",
      "The fusion of Fundamental frequency and SDC \n",
      "detect s the gender of speech samples. The initial \n",
      "stage involves pre -processing the speech samples, \n",
      "followed by feature extraction, which includes SDC \n",
      "and fundamental frequencies. The combination of \n",
      "SDC and Fundamental speech features create a \n",
      "multifeature fusion resulting in a single set of data \n",
      "points. The next step is to reduce the dimensions of \n",
      "data p oints to use them as input to the BiLSTM \n",
      "Model. The BiLSTM model predicts the data in the \n",
      "dataset as male and female, placed in separate \n",
      "folders. Of these, 3342 speech samples are available, \n",
      "and 3294 are correctly classified. So, the accuracy of \n",
      "the model prediction is 99.59%. The precision, recall, \n",
      "and f1 -score values, along with the confusion matrix, \n",
      "are shown in Fig s 9 and 10. The predicted model \n",
      "identifies the female speech samples more accurately \n",
      "than the male.   \n",
      " \n",
      "Fig. 9 Confusion Matrix for Gender detection by BiLSTM.  \n",
      " \n",
      "The graph in Fig 9 shows the gender detection from the \n",
      "speech sample. The X -axis represents the true label, and \n",
      "the Y-axis shows the predicted label.  This graph gives the \n",
      "\n",
      " \n",
      "Mohanty, & Cherukuri, Whispered Speech Emotion  Recognition with Gender Detection using BiLSTM and DCNN  \n",
      " \n",
      " \n",
      " \n",
      "160 \n",
      "count of detected gender spe ech that the Bi -LSTM model \n",
      "correctly and incorrectly detects.  \n",
      " \n",
      "Fig. 10 Accuracy measures of Gender Detection.  \n",
      " \n",
      "2031 female and 1263 male speech are correctly detected, \n",
      "whereas 13 females and 35 males are incorrectly detected. \n",
      "Pitch values identify the emotions after detecting genders \n",
      "from the speech samples.  \n",
      "The extraction of MFCC, Mel -scale spectrogram, Chroma \n",
      "STFT, Spectral Flux, and Spectral Centroid speech \n",
      "features happens from speech samples. Then, all five \n",
      "speech features are fused into a single feature and \n",
      "dimensionally reduced. Finally, the DCNN model is \n",
      "applied to predict emotions.  \n",
      "Individual emotions are detected based on gender as \n",
      "shown in Fig 11. The deviation in the male speech \n",
      "emotions is less than the female . Female fear and female \n",
      "neutral  emotions show divergent results from other \n",
      "emotions. The accuracy of the model is 98.54%.   \n",
      " \n",
      " \n",
      "Fig.11 Emotions based on gender.  \n",
      "6-2- Comparison Analysis  \n",
      "This experiment evaluates the current implementation \n",
      "using the wTIMIT  dataset, resulting in an impressive \n",
      "accuracy of 98.54%. The performance compares with \n",
      "three -layered Long short -term memory Bidirectional \n",
      "RNNs (LSTM BiRNN) that use No -Attention, Feed \n",
      "Forward Attention (FFA), and Improved Feed Forward \n",
      "Attention Mechanism ( IFFA) to evaluate the emotions in \n",
      "the dataset. This model uses 35 hidden states to train the dataset. The models are trained and validated with the help \n",
      "of the wTIMIT dataset [28] and tested with the help of the \n",
      "CHAINS dataset,  as shown in Table 3. This ex periment is \n",
      "a new hypothesis for  emotion detection with gender \n",
      "identification on whispered speech.  Hence, there are fewer \n",
      "references available for similar investigations.  \n",
      "Table 3: Comparison of the Implementation  \n",
      "  Sl# Model Implementation  Accuracy  \n",
      "1 FFA, IFFA, LSTM, Bi_RNN [2 8] 97.6 %  \n",
      "2 Proposed Model  98.54 %  \n",
      "The proposed method improves the performance of \n",
      "emotions after segregating the speech samples into genders.  \n",
      "By separating the approach into two models, this \n",
      "implementation excels in capturing natural and \n",
      "spontaneous expressions of emotions with low latency \n",
      "manner .  \n",
      "7- Conclusion  and Future Work  \n",
      "Identification of gender from whispered speech and \n",
      "recognizing emotions is a complicated task. This \n",
      "implementation initially detects the gender from the \n",
      "speech samples before identifying the emotions. Emotions \n",
      "might vary based on gender in the same situation. Speech \n",
      "features such as Fu ndamental Frequency and SDC help \n",
      "with gender identification. MFCC, Mel -scale spectrogram, \n",
      "Spectral flux, Spectral Centroid, and Chroma STFT play a \n",
      "vital role in detecting emotions. Multifeature fusion helps \n",
      "to combine speech features into a single set of d ata points . \n",
      "The concept verifies with the publicly available dataset \n",
      "wTIMIT with an accuracy of 98.54%. This approach helps \n",
      "to identify nearly inaudible emotions and is used to figure \n",
      "out the strategy. The proposed methodology can be \n",
      "improved using other s peech features, machine learning, \n",
      "and deep learning concepts.  \n",
      " \n",
      "References  \n",
      "[1]  ST Jovicic , and Z  Saric , ―Acoustic analysis of  consonants in \n",
      "whispered speech,‖ Journal of voice , vol 22, no. 3, pp. 263–\n",
      "274, 2008 . \n",
      "[2]  M Kumari , and I  Ali, ―An efficient algorithm for gender \n",
      "detection using voice samples,‖ 2015 Communication, \n",
      "Control and Intelligent Systems (CCIS),  2015 , Mathura, \n",
      "Utter Pradesh,  pp. 221–226, doi: \n",
      "10.1109/CCIntelS.20 15.7437912 . \n",
      "[3] S Motamed , S Setayeshi, A Rabiee , and A Sharifi, ―Speech \n",
      "Emotion Recognition Based on Fusion Method,‖ Journal of \n",
      "Information Systems and Telecommunication (JIST), vol. 3, \n",
      "pp. 50--56, 2017, doi: 10.7508/jist.2017.17.007 .  \n",
      "[4] JS Li, CC Huang , ST Sheu , and MW  Lin, ―Speech emotion \n",
      "recognition and its applications,‖ Proc. of Taiwan Institute of \n",
      "Kansei Conference , 2010  Paris , France , pp. 187–192.  \n",
      "\n",
      " \n",
      "Journal of Information Systems and Telecommunication, Vol.12, No.2,  April -June 2024  \n",
      " \n",
      " \n",
      "161 \n",
      "[5] A Guerrieri , E Braccili , F Sgro , and GN Meldolesi  ―Gender \n",
      "identification in a two -level hierarchical speech emotion \n",
      "recognition system for an Italian Social Robot,‖ Sensors, vol. \n",
      "22, no. 5, pp. 1714, 2022 , doi: 10.3390/s22051714 . \n",
      "[6] M Sarria -Paja, TH Falk, and D  O’Shaughnessy , ―Whispered \n",
      "speaker verification and gender detection using weighted \n",
      "instantaneous frequencies,‖ 2013 IEEE International \n",
      "Conference on Acoustics, Speech and Signal Processing , \n",
      "2013  (May 26 -31), Vancouver , Canada, pp. 7209 –7213 , doi: \n",
      "10.1109/ICASSP.2013.6639062 .  \n",
      "[7] J Deng , S Fruhholz, Z Zhang , and Bojrn Schuller , \n",
      "―Recognizing emotions from whispered speech based on \n",
      "acoustic feature transfer learning,‖ IEEE Access , vol.  5, pp. \n",
      "5235 –5246 , 2017 .  \n",
      "[8] M Cotescu , T Drugman, G Huybrechts, J Lorenzo -Trueba, \n",
      "and A Moinet , ―Voice conversion for whispered speech \n",
      "synthesis,‖  IEEE Signal Processing Letters , vol. 27, pp. 186–\n",
      "190, 2019 , doi: 10.1109/LSP.2019.2961213 .  \n",
      "[9] P Mishra , and R Sharma, ―Gender differentiated \n",
      "convolutional neural networks for speech emotion \n",
      "recognition,‖ 2020 12th International Congress on Ultra -\n",
      "Modern Telecommuni - cations and Control Systems and \n",
      "Workshops (ICUMT),  2020  (October 5 -7), Brno , Czech \n",
      "Republic,  pp. 142–148, doi: \n",
      "10.1109/ICUMT51630.2020.9222412 . \n",
      "[10] Mustaqeem S Kwon, ―Optimal feature selection based \n",
      "speech emotion recognition using two -stream deep \n",
      "convolutional neural network,‖ International Journal of \n",
      "Intelligent Systems , vol. 36, no. 9, pp. 5116 – 5135 , 2021 , \n",
      "doi:  10.1002/int.22505 . \n",
      "[11] J. Ancilin and A. Milton, ―Improved speech emotion \n",
      "recognition with mel frequency magnitude coefficient ,‖ \n",
      "Applied Acoustics , vol. 179, pp. 108046 , 2021 , doi: \n",
      "10.1016/j.apacoust.2021.108046 .  \n",
      "[12] S. Jothimani and K. Premalatha, ―Mff -saug: Multi feature \n",
      "fusion with spectrogram augmentatio n of speech emotion \n",
      "recognition using convolution neural network ,‖ Chaos, \n",
      "Solitons & Fractals , vol. 162, pp. 112512 , 2022 , doi: \n",
      "10.1016/j.chaos.2022.112512 .  \n",
      "[13] B Yalamanchili, SK Samayamantula, and KR Anne, ―Neural \n",
      "network -based blended ensemble learning for speech \n",
      "emotion recognition,‖ Multidimensional Systems and Signal \n",
      "Processing, vol. 33, no. 4, pp. 1323 --1348 , 2022 , doi: \n",
      "10.1007/s11045 -022-00845 -9.  \n",
      "[14] T Feng , R Hebbar , and S Narayanan, ―Trustser: On the \n",
      "trustworthiness of fine -tuning pre -trained speech embeddings \n",
      "for speech emotion recognition,‖ arXiv preprint \n",
      "arXiv:2305.11229, 2023 , doi:  10.48550/arXiv.2305.11229  \n",
      "[15] RV Darekar and M Chavan , S Sharanyaa,  and NR Ranjan , \n",
      "―A hybrid meta -heuristic ensemble based classification \n",
      "technique speech emotion recognition,‖ Advances in \n",
      "Engineering Software, vol. 180, pp. 103412 , 2023 . \n",
      "[16] J Rekimot o, ―Dualvoice: A speech interaction method using \n",
      "whisper -voice as commands,‖ CHI Conference on Human \n",
      "Factors in Computing Systems Extended Abstracts,  pp. 1–6, \n",
      "2022 , doi: 10.1145/3491101.3519700 . \n",
      "[17]H Dolka,  AX VM , and S Juliet,―  Speech  emotion r ecognition  \n",
      "using  ANN on MFCC  features,‖ 2021 3rd international \n",
      "conference on signal processing and communication (ICPSC) , \n",
      "2021, Coimbatore, India, pp. 431–435, doi: \n",
      "10.1109/ICSPC51351.2021.9451810 . [18] MK Reddy and KS Rao, ―Robust pitch extraction method \n",
      "for the hmm -based speech synthesis system,‖ IEEE signal \n",
      "processing letters , vol.  24, no. 8, pp. 1133 –1137 , 2017 , doi: \n",
      "10.1109/LSP.2017. 2712646 .  \n",
      "[19] J Chatterjee, V Mukesh, HH Hsu , G Vyas , and Z Liu , \n",
      "―Speech emotion recognition using cross - correlation and \n",
      "acoustic features,‖ 2018 IEEE 16th Intl Conf on Dependable, \n",
      "Autonomic and Secure Computing, 16th Intl Conf on \n",
      "Pervasive Intelligence and Computing, 4th Intl Conf on Big \n",
      "Data Intelligence and Computing and Cyber Science and \n",
      "Technology Congress , 2018  (August 12-15), Athens, Greece , \n",
      "pp. 243–249, doi: 10.1109/DASC/PiCom/DataCom/CyberSci  \n",
      " Tec.2018.00050 .  \n",
      "[20] S Rajesh and NJ Nalini, ―Musical instrument emotion \n",
      "recognition using deep recurrent neural network,‖ Procedia \n",
      "Computer Science, vol. 167, pp. 16 --25, 2020, doi: \n",
      "10.1016/j.procs.2020.03.178 . \n",
      "[21] M Aly , and NS Alotaibi, ―A novel deep learning model to \n",
      "detect covid -19 based on wavelet features extracted from \n",
      "mel- scale spectrogram of patients’ cough and breathing \n",
      "sounds,‖ Informatics in Medicine Unlocked , vol.  32, pp.  \n",
      "101049 , 2022, doi: 10.1016/j.imu.2022.101049 . \n",
      "[22] Z Qawaqneh,  AA Mallouh,  and BD Barkana, ― Age and \n",
      "gender  classification  from  speech and face images by jointly \n",
      "fine-tuned deep neural networks,‖ Expert Systems with \n",
      "Applications , vol. 85, pp. 76–86, 2017 , doi: \n",
      "10.1016/j.eswa.2017.05.037 .  \n",
      "[23] A Koduru, HB Valiveti and AK Budati, ―Feature extraction \n",
      "algorithms to improve the speech emotion recognition rate,‖ \n",
      "International Journal of Speech Technology , vol. 23, no. 1, \n",
      "pp. 45–55, 2020 , doi: 10.1007/s10772 -020-09672 -4.  \n",
      "[24] S Zhang , and C Li, ―Research on feature fusion speech \n",
      "emotion recognition technology for smart teaching,‖ Mobile \n",
      "Information Systems , vol. 2022 , 2022 , doi:  \n",
      "10.1155/2022/7785929 . \n",
      "[25] GB, Prasanna, SV Bhat, C Naik , and HN Champa, ―An \n",
      "Efficient Method for Handwritten Kannada Digit \n",
      "Recognition based on PCA and SVM Classifier,‖ Journal of \n",
      "Information Systems and Telecommunication (JIST), vol. 3, \n",
      "no. 35, pp. 169 2021, doi: 20.1001.1.23221437.2021.9.35.3.2 . \n",
      "[26] A Graves, N Jaitly, and A Mohamed, ―Hybrid speech \n",
      "recognition wit h deep bidirectional lstm,‖ 2013 IEEE \n",
      "workshop on automatic speech recognition and \n",
      "understanding , 2013, Olomouc, Czech Republic, pp.  273–\n",
      "278, doi: 10.1109/ASRU.2013.6707742 .  \n",
      "[27] N Aloysius and M Geetha, ―A review on deep convolutional \n",
      "neural networks,‖ 2017 international conference on \n",
      "communication and signal processing (ICCSP), 2017, pp. \n",
      "0588 –0592 , IEEE, doi: 10.1109/ICCSP.2017.8286426 .  \n",
      "[28] SBC Gutha, MAB Shaik, T Udayakumar , and AA \n",
      "Saunshikhar , ―Improved feed forward attention mechanism in \n",
      "bidirectional recurrent neural networks for robust sequence \n",
      "classification,‖ 2020 International Conference on Signal \n",
      "Processing and Communications (SPCOM) , 2020 , IISc, \n",
      "Bangalore.  IEEE , pp. 1—5, doi: \n",
      "0.9179606 10.1109/SPCOM50965.202  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "\n",
    "pdf_texts = {}\n",
    "\n",
    "for file_name in pdf_path:\n",
    "    reader = PdfReader(file_name)\n",
    "    print(f\"\\nReading: {file_name}\")\n",
    "    text = \"\"\n",
    "    for page in reader.pages:\n",
    "        page_text = page.extract_text()\n",
    "        if page_text:\n",
    "            text += page_text + \"\\n\"\n",
    "\n",
    "    pdf_texts[file_name] = text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9d7204",
   "metadata": {},
   "outputs": [],
   "source": [
    "Article_1=pdf_texts[\"d:\\\\PythonCode\\\\document_portal\\\\notebook\\\\data\\\\Article.pdf\"]\n",
    "Article_2=pdf_texts[\"d:\\\\PythonCode\\\\document_portal\\\\notebook\\\\data\\\\CancerTumorDetectionusingGeneticMutatedDataandMachineLearningModels.pdf\"]\n",
    "Article_3=pdf_texts[\"d:\\\\PythonCode\\\\document_portal\\\\notebook\\\\data\\\\Improvement_of_Speech_Emotion_Recognition_by_Deep_Convolutional_Neural_Network.pdf\"]\n",
    "Article_4=pdf_texts[\"d:\\\\PythonCode\\\\document_portal\\\\notebook\\\\data\\\\Improving_Speaker_Gender_Detection_by_Combining_Pitch_and_SDC__1_.pdf\"]\n",
    "Article_5=pdf_texts[\"d:\\\\PythonCode\\\\document_portal\\\\notebook\\\\data\\\\Paper_94-Sentiment_Analysis_on_Banking_Feedback_and_News_Data.pdf\"]\n",
    "Article_6=pdf_texts[\"d:\\\\PythonCode\\\\document_portal\\\\notebook\\\\data\\\\Prospect-of-Low-Power-Sensor-Network-Technology-in-Disaster-Management-for-Sustainable-Future.pdf\"]\n",
    "Article_7=pdf_texts[\"d:\\\\PythonCode\\\\document_portal\\\\notebook\\\\data\\\\W_An_Outlier_Detection_and_Rectification.pdf\"]\n",
    "Article_8=pdf_texts[\"d:\\\\PythonCode\\\\document_portal\\\\notebook\\\\data\\\\ijcsit2012030339.pdf\"]\n",
    "Article_9=pdf_texts[\"d:\\\\PythonCode\\\\document_portal\\\\notebook\\\\data\\\\mohanty-2020-ijca-920840.pdf\"]\n",
    "Article_10=pdf_texts[\"d:\\\\PythonCode\\\\document_portal\\\\notebook\\\\data\\\\sample.pdf\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27b6a61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
